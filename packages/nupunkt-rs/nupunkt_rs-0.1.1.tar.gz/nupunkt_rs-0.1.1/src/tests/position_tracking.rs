/// Tests for position tracking in tokenization
use crate::core::SentenceTokenizer;
use crate::models::create_default_tokenizer;

#[test]
fn test_duplicate_tokens_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Text with duplicate words that would fail with .find()
    let text = "The cat sat on the mat. The cat jumped off the mat.";

    let spans = tokenizer.tokenize_spans(text);

    // Should get two sentences with correct spans
    assert_eq!(spans.len(), 2);

    // First sentence: "The cat sat on the mat."
    assert_eq!(&text[spans[0].0..spans[0].1], "The cat sat on the mat.");

    // Second sentence: "The cat jumped off the mat."
    assert_eq!(&text[spans[1].0..spans[1].1], "The cat jumped off the mat.");
}

#[test]
fn test_unicode_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Text with multi-byte Unicode characters
    let text = "Hello ä¸–ç•Œ! This is a test. Unicode cafÃ© works.";

    let spans = tokenizer.tokenize_spans(text);

    // Should handle multi-byte characters correctly
    assert_eq!(spans.len(), 3);

    // Verify each span extracts the correct text
    assert_eq!(&text[spans[0].0..spans[0].1], "Hello ä¸–ç•Œ!");
    assert_eq!(&text[spans[1].0..spans[1].1], "This is a test.");
    assert_eq!(&text[spans[2].0..spans[2].1], "Unicode cafÃ© works.");
}

#[test]
fn test_repeated_pattern_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Text with many repeated patterns
    let text = "Test test test. Test test. Test.";

    let spans = tokenizer.tokenize_spans(text);

    assert_eq!(spans.len(), 3);
    assert_eq!(&text[spans[0].0..spans[0].1], "Test test test.");
    assert_eq!(&text[spans[1].0..spans[1].1], "Test test.");
    assert_eq!(&text[spans[2].0..spans[2].1], "Test.");
}

#[test]
fn test_emoji_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Text with emojis (multi-byte)
    let text = "Hello! ðŸ˜Š How are you? I'm great! ðŸŽ‰";

    let spans = tokenizer.tokenize_spans(text);

    // Each span should correctly handle emoji byte positions
    for span in &spans {
        // This should not panic - the spans should be valid byte positions
        let _ = &text[span.0..span.1];
    }

    // Verify we get expected sentences
    assert!(spans.len() >= 2);
}

#[test]
fn test_newline_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Text with newlines
    let text = "First sentence.\nSecond sentence.\n\nThird sentence after blank line.";

    let spans = tokenizer.tokenize_spans(text);

    assert_eq!(spans.len(), 3);
    assert_eq!(&text[spans[0].0..spans[0].1], "First sentence.");
    assert_eq!(&text[spans[1].0..spans[1].1], "Second sentence.");
    assert_eq!(
        &text[spans[2].0..spans[2].1],
        "Third sentence after blank line."
    );
}

#[test]
fn test_complex_duplicate_position_tracking() {
    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Complex text with many duplicates
    let text =
        "Dr. Smith and Dr. Jones met Dr. Brown. Dr. Smith said hello to Dr. Jones and Dr. Brown.";

    let spans = tokenizer.tokenize_spans(text);

    // Should handle all the duplicate "Dr." tokens correctly
    assert_eq!(spans.len(), 2);

    // Verify the spans are correct
    assert_eq!(
        &text[spans[0].0..spans[0].1],
        "Dr. Smith and Dr. Jones met Dr. Brown."
    );
    assert_eq!(
        &text[spans[1].0..spans[1].1],
        "Dr. Smith said hello to Dr. Jones and Dr. Brown."
    );
}

#[test]
fn test_position_tracking_performance() {
    use std::time::Instant;

    let tokenizer = create_default_tokenizer().expect("Failed to load default model");

    // Create a large text with many duplicates
    let sentence = "The quick brown fox jumps over the lazy dog. ";
    let large_text = sentence.repeat(1000); // 1000 repetitions

    let start = Instant::now();
    let spans = tokenizer.tokenize_spans(&large_text);
    let duration = start.elapsed();

    // Should be fast even with many duplicates
    assert!(
        duration.as_millis() < 100,
        "Tokenization took too long: {:?}",
        duration
    );

    // Should have found all 1000 sentences
    assert_eq!(spans.len(), 1000);

    // Spot check a few spans
    assert_eq!(&large_text[spans[0].0..spans[0].1], sentence.trim());
    assert_eq!(&large_text[spans[500].0..spans[500].1], sentence.trim());
    assert_eq!(&large_text[spans[999].0..spans[999].1], sentence.trim());
}
