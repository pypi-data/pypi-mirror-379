[project]
name = "graphrag-eval"
version = "5.0.2"
description = "For assessing question answering systems' final answers and intermediate steps, against a given set of questions, reference answers and steps."
authors = [
  { name = "Philip Ganchev", email = "philip.ganchev@graphwise.ai" },
  { name = "Aleksis Datseris", email = "aleksis.datseris@graphwise.ai" },
  { name = "Neli Hateva", email = "neli.hateva@graphwise.ai" },
]
readme = "README.md"
license = "Apache-2.0"
requires-python = ">=3.12,<3.13"

[project.urls]
repository = "https://github.com/Ontotext-AD/graphrag-eval"

[tool.poetry.dependencies]
openai = { version = "^1.97.0", optional = true }
langevals = { version = "0.1.*", optional = true }
langevals-ragas = { version = "^0.1.12", optional = true }

[tool.poetry.extras]
openai = ["openai", "langevals", "langevals-ragas"]

[tool.poetry.group.openai.dependencies]
openai = "^1.97.0"
langevals = "0.1.*"
langevals-ragas = "^0.1.12"

[tool.poetry.group.openai]
optional = true

[tool.poetry.group.test.dependencies]
pytest = "<9,>=8"
pytest-cov = "<7,>=6"
jsonlines = "4.0.0"
pyyaml = "^6.0.2"

[tool.poetry.group.test]
optional = true

[project.scripts]
answer-correctness = "graphrag_eval.answer_correctness:main"

[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"
