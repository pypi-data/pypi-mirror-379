[project]
name = "graphrag-eval"
version = "5.0.0"
description = "For assessing question answering systems' final answers and intermediate steps, against a given set of questions, reference answers and steps."
authors = [
    {name = "Neli Hateva", email = "neli.hateva@graphwise.ai"},
    {name = "Philip Ganchev", email = "philip.ganchev@graphwise.ai"}
]
readme = "README.md"
license = "Apache-2.0"
requires-python = ">=3.12,<3.13"

[project.urls]
repository = "https://github.com/Ontotext-AD/graphrag-eval"

[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry.group.test.dependencies]
pytest = "<9,>=8"
pytest-cov = "<7,>=6"
jsonlines = "4.0.0"
pyyaml = "^6.0.2"

[tool.poetry.group.test]
optional = true

[tool.poetry.group.openai.dependencies]
openai = "^1.97.0"
langevals = "0.1.*"
langevals-ragas = "^0.1.12"

[tool.poetry.group.openai]
optional = true

[project.scripts]
answer-correctness = "graphrag_eval.answer_correctness:main"
