"""Network resilience and adaptive retry logic for batch operations."""

import asyncio
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple
import httpx

from .batch_models import NetworkConditions
from .exceptions import NetworkError, RateLimitError


class NetworkCondition(Enum):
    """Network condition states."""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    CRITICAL = "critical"


@dataclass
class RetryConfig:
    """Configuration for retry behavior."""
    max_retries: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    jitter: bool = True
    backoff_multiplier: float = 1.0
    
    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay for a given attempt."""
        delay = self.base_delay * (self.exponential_base ** (attempt - 1))
        delay = min(delay * self.backoff_multiplier, self.max_delay)
        
        if self.jitter:
            # Add random jitter (Â±25%)
            import random
            jitter_factor = random.uniform(0.75, 1.25)
            delay *= jitter_factor
        
        return delay


@dataclass
class NetworkMetrics:
    """Network performance metrics."""
    latency_samples: List[float] = field(default_factory=list)
    error_count: int = 0
    success_count: int = 0
    total_requests: int = 0
    last_error_time: Optional[datetime] = None
    consecutive_errors: int = 0
    
    @property
    def average_latency(self) -> float:
        """Calculate average latency."""
        if not self.latency_samples:
            return 0.0
        return sum(self.latency_samples) / len(self.latency_samples)
    
    @property
    def error_rate(self) -> float:
        """Calculate error rate."""
        if self.total_requests == 0:
            return 0.0
        return self.error_count / self.total_requests
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate."""
        if self.total_requests == 0:
            return 0.0
        return self.success_count / self.total_requests
    
    def add_success(self, latency: float):
        """Record a successful request."""
        self.success_count += 1
        self.total_requests += 1
        self.latency_samples.append(latency)
        self.consecutive_errors = 0
        
        # Keep only recent latency samples (last 100)
        if len(self.latency_samples) > 100:
            self.latency_samples = self.latency_samples[-100:]
    
    def add_error(self):
        """Record a failed request."""
        self.error_count += 1
        self.total_requests += 1
        self.consecutive_errors += 1
        self.last_error_time = datetime.now()
    
    def get_network_condition(self) -> NetworkCondition:
        """Determine current network condition based on metrics."""
        if self.total_requests < 5:
            return NetworkCondition.GOOD  # Default for insufficient data
        
        avg_latency = self.average_latency
        error_rate = self.error_rate
        
        # Determine condition based on latency and error rate
        if avg_latency < 100 and error_rate < 0.01:
            return NetworkCondition.EXCELLENT
        elif avg_latency < 200 and error_rate < 0.05:
            return NetworkCondition.GOOD
        elif avg_latency < 500 and error_rate < 0.15:
            return NetworkCondition.FAIR
        elif avg_latency < 1000 and error_rate < 0.30:
            return NetworkCondition.POOR
        else:
            return NetworkCondition.CRITICAL


class AdaptiveRetryManager:
    """Manages adaptive retry logic based on network conditions."""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize the adaptive retry manager.
        
        Args:
            logger: Optional logger for retry events
        """
        self.logger = logger or logging.getLogger(__name__)
        self.network_metrics = NetworkMetrics()
        self.retry_configs: Dict[NetworkCondition, RetryConfig] = {
            NetworkCondition.EXCELLENT: RetryConfig(
                max_retries=2,
                base_delay=0.5,
                max_delay=10.0,
                exponential_base=1.5
            ),
            NetworkCondition.GOOD: RetryConfig(
                max_retries=3,
                base_delay=1.0,
                max_delay=30.0,
                exponential_base=2.0
            ),
            NetworkCondition.FAIR: RetryConfig(
                max_retries=4,
                base_delay=2.0,
                max_delay=60.0,
                exponential_base=2.0,
                backoff_multiplier=1.5
            ),
            NetworkCondition.POOR: RetryConfig(
                max_retries=5,
                base_delay=5.0,
                max_delay=120.0,
                exponential_base=2.5,
                backoff_multiplier=2.0
            ),
            NetworkCondition.CRITICAL: RetryConfig(
                max_retries=3,
                base_delay=10.0,
                max_delay=300.0,
                exponential_base=3.0,
                backoff_multiplier=3.0
            )
        }
    
    async def execute_with_retry(
        self,
        operation: Callable,
        *args,
        operation_name: str = "operation",
        **kwargs
    ) -> Any:
        """
        Execute an operation with adaptive retry logic.
        
        Args:
            operation: Async function to execute
            *args: Arguments for the operation
            operation_name: Name for logging purposes
            **kwargs: Keyword arguments for the operation
            
        Returns:
            Result of the operation
            
        Raises:
            Exception: If all retry attempts fail
        """
        condition = self.network_metrics.get_network_condition()
        config = self.retry_configs[condition]
        
        last_exception = None
        
        for attempt in range(1, config.max_retries + 1):
            try:
                start_time = time.time()
                result = await operation(*args, **kwargs)
                
                # Record successful operation
                latency = (time.time() - start_time) * 1000  # Convert to ms
                self.network_metrics.add_success(latency)
                
                if attempt > 1:
                    self.logger.info(
                        f"{operation_name} succeeded on attempt {attempt} "
                        f"(latency: {latency:.1f}ms)"
                    )
                
                return result
                
            except Exception as e:
                last_exception = e
                self.network_metrics.add_error()
                
                # Check if we should retry this exception
                if not self._should_retry_exception(e):
                    self.logger.warning(
                        f"{operation_name} failed with non-retryable error: {e}"
                    )
                    raise e
                
                if attempt < config.max_retries:
                    delay = config.calculate_delay(attempt)
                    self.logger.warning(
                        f"{operation_name} failed on attempt {attempt}/{config.max_retries}: {e}. "
                        f"Retrying in {delay:.1f}s (network condition: {condition.value})"
                    )
                    await asyncio.sleep(delay)
                else:
                    self.logger.error(
                        f"{operation_name} failed after {config.max_retries} attempts: {e}"
                    )
        
        # All retries exhausted
        raise last_exception
    
    def _should_retry_exception(self, exception: Exception) -> bool:
        """
        Determine if an exception should trigger a retry.
        
        Args:
            exception: The exception to evaluate
            
        Returns:
            True if the exception should be retried
        """
        # Network-related errors that should be retried
        retryable_exceptions = (
            httpx.TimeoutException,
            httpx.NetworkError,
            httpx.ConnectError,
            httpx.ReadError,
            NetworkError
        )
        
        if isinstance(exception, retryable_exceptions):
            return True
        
        # HTTP status errors that should be retried
        if isinstance(exception, httpx.HTTPStatusError):
            status_code = exception.response.status_code
            # Retry on server errors and rate limits
            if status_code >= 500 or status_code == 429:
                return True
        
        # Rate limit errors should be retried
        if isinstance(exception, RateLimitError):
            return True
        
        # For testing purposes, also retry generic Exception if it's not explicitly non-retryable
        # In production, you might want to be more restrictive
        if isinstance(exception, Exception) and not isinstance(exception, (
            httpx.HTTPStatusError,  # Already handled above
            ValueError,
            TypeError,
            AttributeError
        )):
            return True
        
        return False
    
    def get_current_network_conditions(self) -> NetworkConditions:
        """
        Get current network conditions for batch optimization.
        
        Returns:
            NetworkConditions object with current metrics
        """
        return NetworkConditions(
            latency_ms=self.network_metrics.average_latency,
            bandwidth_mbps=self._estimate_bandwidth(),
            error_rate=self.network_metrics.error_rate
        )
    
    def _estimate_bandwidth(self) -> float:
        """
        Estimate bandwidth based on network condition.
        This is a simplified estimation - in practice, you might want
        to measure actual throughput.
        
        Returns:
            Estimated bandwidth in Mbps
        """
        condition = self.network_metrics.get_network_condition()
        bandwidth_estimates = {
            NetworkCondition.EXCELLENT: 100.0,
            NetworkCondition.GOOD: 50.0,
            NetworkCondition.FAIR: 20.0,
            NetworkCondition.POOR: 5.0,
            NetworkCondition.CRITICAL: 1.0
        }
        return bandwidth_estimates[condition]
    
    def get_retry_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about retry behavior and network conditions.
        
        Returns:
            Dictionary with retry statistics
        """
        condition = self.network_metrics.get_network_condition()
        config = self.retry_configs[condition]
        
        return {
            'network_condition': condition.value,
            'total_requests': self.network_metrics.total_requests,
            'success_count': self.network_metrics.success_count,
            'error_count': self.network_metrics.error_count,
            'success_rate': self.network_metrics.success_rate,
            'error_rate': self.network_metrics.error_rate,
            'average_latency_ms': self.network_metrics.average_latency,
            'consecutive_errors': self.network_metrics.consecutive_errors,
            'current_retry_config': {
                'max_retries': config.max_retries,
                'base_delay': config.base_delay,
                'max_delay': config.max_delay,
                'exponential_base': config.exponential_base
            }
        }
    
    def reset_metrics(self):
        """Reset network metrics (useful for testing or long-running processes)."""
        self.network_metrics = NetworkMetrics()


class CircuitBreakerManager:
    """Enhanced circuit breaker with network condition awareness."""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 60,
        half_open_max_calls: int = 3
    ):
        """
        Initialize circuit breaker manager.
        
        Args:
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Seconds to wait before attempting recovery
            half_open_max_calls: Max calls to allow in half-open state
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.half_open_max_calls = half_open_max_calls
        
        self._failure_count = 0
        self._success_count = 0
        self._last_failure_time: Optional[datetime] = None
        self._state = 'closed'  # closed, open, half-open
        self._half_open_calls = 0
        
        self.logger = logging.getLogger(__name__)
    
    async def call(self, operation: Callable, *args, **kwargs) -> Any:
        """
        Execute operation through circuit breaker.
        
        Args:
            operation: Async function to call
            *args: Function arguments
            **kwargs: Function keyword arguments
            
        Returns:
            Operation result
            
        Raises:
            Exception: If circuit is open or operation fails
        """
        if self._state == 'open':
            if self._should_attempt_reset():
                self._state = 'half-open'
                self._half_open_calls = 0
                self.logger.info("Circuit breaker transitioning to half-open state")
            else:
                raise Exception(
                    f"Circuit breaker is open. "
                    f"Will retry after {self._time_until_retry():.1f} seconds"
                )
        
        if self._state == 'half-open' and self._half_open_calls >= self.half_open_max_calls:
            # If we've already made max calls in half-open, don't allow more
            raise Exception("Circuit breaker half-open call limit exceeded")
        
        try:
            if self._state == 'half-open':
                self._half_open_calls += 1
            
            result = await operation(*args, **kwargs)
            self._on_success()
            return result
            
        except Exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to attempt reset."""
        if self._last_failure_time is None:
            return True
        
        return (
            datetime.now() - self._last_failure_time
        ).total_seconds() >= self.recovery_timeout
    
    def _time_until_retry(self) -> float:
        """Calculate time until next retry attempt."""
        if self._last_failure_time is None:
            return 0.0
        
        elapsed = (datetime.now() - self._last_failure_time).total_seconds()
        return max(0.0, self.recovery_timeout - elapsed)
    
    def _on_success(self):
        """Handle successful operation."""
        if self._state == 'half-open':
            self._success_count += 1
            if self._success_count >= self.half_open_max_calls:
                self._state = 'closed'
                self._failure_count = 0
                self._success_count = 0
                self._half_open_calls = 0
                self.logger.info("Circuit breaker closed after successful recovery")
        elif self._state == 'closed':
            self._failure_count = 0
    
    def _on_failure(self):
        """Handle failed operation."""
        self._failure_count += 1
        self._last_failure_time = datetime.now()
        
        if self._state == 'half-open':
            self._state = 'open'
            self.logger.warning("Circuit breaker opened due to failure in half-open state")
        elif self._failure_count >= self.failure_threshold:
            self._state = 'open'
            self.logger.warning(
                f"Circuit breaker opened after {self._failure_count} failures"
            )
    
    @property
    def state(self) -> str:
        """Get current circuit breaker state."""
        return self._state
    
    @property
    def failure_count(self) -> int:
        """Get current failure count."""
        return self._failure_count
    
    def get_status(self) -> Dict[str, Any]:
        """Get detailed circuit breaker status."""
        return {
            'state': self._state,
            'failure_count': self._failure_count,
            'success_count': self._success_count,
            'failure_threshold': self.failure_threshold,
            'recovery_timeout': self.recovery_timeout,
            'time_until_retry': self._time_until_retry() if self._state == 'open' else 0,
            'half_open_calls': self._half_open_calls if self._state == 'half-open' else 0,
            'half_open_max_calls': self.half_open_max_calls
        }


class NetworkResilienceManager:
    """Comprehensive network resilience management."""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize network resilience manager.
        
        Args:
            logger: Optional logger for resilience events
        """
        self.logger = logger or logging.getLogger(__name__)
        self.retry_manager = AdaptiveRetryManager(logger)
        self.circuit_breakers: Dict[str, CircuitBreakerManager] = {}
        self.default_circuit_breaker = CircuitBreakerManager()
    
    def get_circuit_breaker(self, name: str) -> CircuitBreakerManager:
        """
        Get or create a circuit breaker for a specific service/endpoint.
        
        Args:
            name: Name of the service/endpoint
            
        Returns:
            Circuit breaker instance
        """
        if name not in self.circuit_breakers:
            self.circuit_breakers[name] = CircuitBreakerManager()
        return self.circuit_breakers[name]
    
    async def execute_with_resilience(
        self,
        operation: Callable,
        *args,
        operation_name: str = "operation",
        circuit_breaker_name: Optional[str] = None,
        **kwargs
    ) -> Any:
        """
        Execute operation with full resilience (retry + circuit breaker).
        
        Args:
            operation: Async function to execute
            *args: Arguments for the operation
            operation_name: Name for logging purposes
            circuit_breaker_name: Optional circuit breaker name
            **kwargs: Keyword arguments for the operation
            
        Returns:
            Result of the operation
            
        Raises:
            Exception: If operation fails after all resilience measures
        """
        circuit_breaker = (
            self.get_circuit_breaker(circuit_breaker_name)
            if circuit_breaker_name
            else self.default_circuit_breaker
        )
        
        async def resilient_operation():
            return await circuit_breaker.call(operation, *args, **kwargs)
        
        return await self.retry_manager.execute_with_retry(
            resilient_operation,
            operation_name=operation_name
        )
    
    def get_network_conditions(self) -> NetworkConditions:
        """Get current network conditions."""
        return self.retry_manager.get_current_network_conditions()
    
    def get_resilience_status(self) -> Dict[str, Any]:
        """
        Get comprehensive resilience status.
        
        Returns:
            Dictionary with resilience status information
        """
        status = {
            'retry_statistics': self.retry_manager.get_retry_statistics(),
            'circuit_breakers': {}
        }
        
        # Add default circuit breaker status
        status['circuit_breakers']['default'] = self.default_circuit_breaker.get_status()
        
        # Add named circuit breaker statuses
        for name, cb in self.circuit_breakers.items():
            status['circuit_breakers'][name] = cb.get_status()
        
        return status
    
    def reset_all_metrics(self):
        """Reset all metrics and circuit breakers."""
        self.retry_manager.reset_metrics()
        self.default_circuit_breaker = CircuitBreakerManager()
        self.circuit_breakers.clear()