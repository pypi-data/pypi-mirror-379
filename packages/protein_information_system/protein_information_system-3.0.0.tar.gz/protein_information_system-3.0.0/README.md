[![PyPI - Version](https://img.shields.io/pypi/v/protein-information-system)](https://pypi.org/project/protein-information-system/)
[![Documentation Status](https://readthedocs.org/projects/protein-information-system/badge/?version=latest)](https://protein-information-system.readthedocs.io/en/latest/?badge=latest)
![Linting Status](https://github.com/CBBIO/protein-information-system/actions/workflows/test-lint.yml/badge.svg?branch=main)
[![codecov](https://codecov.io/gh/CBBIO/protein-information-system/branch/main/graph/badge.svg)](https://codecov.io/gh/CBBIO/protein-information-system)

# **Protein Information System (PIS)**

**Protein Information System (PIS)** is an integrated biological information system focused on extracting, processing, and managing protein-related data. PIS consolidates data from **UniProt**, **PDB**, and **GOA**, enabling the efficient retrieval and organization of protein sequences, structures, and functional annotations.

The primary goal of PIS is to provide a robust framework for large-scale protein data extraction, facilitating downstream functional analysis and annotation transfer. The system is designed for **high-performance computing (HPC) environments**, ensuring scalability and efficiency.


## 📈 **Current State of the Project**

### **FANTASIA: Functional Annotation Toolkit**


> 🧠 **FANTASIA** was built on top of the Protein Information System (PIS) as an advanced tool for **functional protein annotation** using embeddings generated by protein language models.
>
> [🔗 FANTASIA Repository](https://github.com/CBBIO/FANTASIA)
>
> The pipeline supports high-performance computing (HPC) environments and integrates tools such as ProtT5, ESM, and CD-HIT. These models can be extended or replaced with new variants **without modifying the core software structure**, simply by adding the new model to the PIS. This design enables scalable, modular, and reproducible GO term annotation from FASTA sequence files.


### **Protocol for Large-Scale Metamorphism and Multifunctionality Search**

> 🔍 In addition, a systematic protocol has been developed for the **large-scale identification of structural metamorphisms** and **protein multifunctionality**.
>
> [🔗 Metamorphic and multifunctionality Search Repository](https://github.com/CBBIO/metamorphic_multifunctional_search)
> 
> This protocol leverages the full capabilities of PIS to uncover non-obvious relationships between structure and function. **Structural metamorphisms** are detected by filtering large-scale structural alignments between proteins with high sequence identity, identifying divergent conformations. **Multifunctionality** is addressed through a semantic analysis of GO annotations, computing a functional distance metric to determine the two most divergent terms within each GO category per protein.

---

## **Prerequisites**

- Python 3.10
- RabbitMQ
- PostgreSQL with pgVector extension installed.
- PSQL client 16

---

## **Setup Instructions**

### 1. Install Docker
Ensure Docker is installed on your system. If it’s not, you can download it from [here](https://docs.docker.com/get-docker/).

### 2. Starting Required Services

Ensure PostgreSQL and RabbitMQ services are running.

```bash
docker run -d --name pgvectorsql \
    -e POSTGRES_USER=usuario \
    -e POSTGRES_PASSWORD=clave \
    -e POSTGRES_DB=BioData \
    -p 5432:5432 \
    pgvector/pgvector:pg16 
```


### 4. (Optional) Connect to the Database

You can use **pgAdmin 4**, a graphical interface for managing and interacting with PostgreSQL databases, or any other SQL client.

### 5. Set Up RabbitMQ

Start a RabbitMQ container using the command below:

```bash
docker run -d --name rabbitmq \
    -p 15672:15672 \
    -p 5672:5672 \
    rabbitmq:management
```

### 6. (Optional) Manage RabbitMQ

Once RabbitMQ is running, you can access its management interface at [RabbitMQ Management Interface](http://localhost:15672/#/queues).

---

## **Get started:**

To execute the full extraction process, install dependencies and run from project root:

```bash
pis
```

This command will trigger the complete workflow, starting from the initial data preprocessing stages and continuing through to the final data organization and storage.

## **Customizing the Workflow:**

You can customize the sequence of tasks executed by modifying `main.py` or adjusting the relevant parameters in the `config.yaml` file. This allows you to tailor the extraction process to meet specific research needs or to experiment with different data processing configurations.

