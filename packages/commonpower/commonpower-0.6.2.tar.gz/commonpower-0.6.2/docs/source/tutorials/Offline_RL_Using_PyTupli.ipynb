{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# CommonPower imports\n",
    "from commonpower.control.controllers import *\n",
    "# from commonpower.control.wrappers import *\n",
    "from commonpower.control.pytupli_interface import *\n",
    "from commonpower.control.configs.algorithms import *\n",
    "from commonpower.control.runners import DeploymentRunner, SingleAgentTrainer\n",
    "from commonpower.control.wrappers import *\n",
    "from commonpower.control.safety_layer.penalties import *\n",
    "from commonpower.control.safety_layer.safety_layers import *\n",
    "from commonpower.control.logging_utils.loggers import *\n",
    "from commonpower.core import ModelHistory, System\n",
    "from commonpower.data_forecasting.base import DataProvider, Forecaster\n",
    "from commonpower.data_forecasting.data_sources import *\n",
    "from commonpower.data_forecasting.forecasters import *\n",
    "from commonpower.modeling.param_initialization import *\n",
    "from commonpower.models.buses import *\n",
    "from commonpower.models.components import *\n",
    "from commonpower.models.powerflow import *\n",
    "from commonpower.utils.helpers import get_adjusted_cost\n",
    "\n",
    "# PyTupli imports\n",
    "from pytupli.storage import TupliAPIClient, FileStorage\n",
    "from pytupli.schema import *\n",
    "from pytupli.dataset import TupliDataset\n",
    "\n",
    "# D3rlpy imports\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import CQLConfig\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "# SB3 imports\n",
    "from stable_baselines3 import SAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff649cc",
   "metadata": {},
   "source": [
    "## Offline RL in CommonPower using PyTupli\n",
    "\n",
    "This notebook introduces how to use the Python tool PyTupli to create reproducible benchmarks based on CommonPower and record offline data for these benchmarks using our built-in controllers. These datasets can then be used to train an RL agent using offline RL (e.g., using the d3rlpy library). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcde79",
   "metadata": {},
   "source": [
    "### Background: PyTupli\n",
    "[PyTupli](https://github.com/TUMcps/pytupli) is a tool for seamless hosting of databases for collaborative offline RL projects. It provides a Docker container for setting up your own server such that you can store serialized benchmarks and corresponding RL tuples (state, action, next state, reward), as well as related artifacts such as trained controllers, hyperparameters, or CSV files containing time series data. Furthermore, PyTupli offers an advanced user management for fine-grained access control. Beyond its application in offline RL, it can also be used to simply share serialized benchmarks and the necessary time series data in a secure fashion, which is highly relevant in areas with privacy concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3e6b1",
   "metadata": {},
   "source": [
    "### Part I: Creating a Benchmark\n",
    "\n",
    "We will first show how to create a serializable benchmark from a `System` in CommonPower that can then be uploaded to a storage instantiated with PyTupli. The main challenge here consists in serializing the time series data used in CommonPower to simulate static loads, PV production, outdoor temperature, etc. PyTupli allows uploading such artifacts separately to avoid duplicate storage if they are referenced by multiple benchmarks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7ed1e",
   "metadata": {},
   "source": [
    "We will first create a `TupliStorage` instance. We have two options: local file storage or storage on a hosted server. If you are using the API, follow the instructions in the [README of PyTupli](https://github.com/TUMcps/pytupli) to start the application and log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FLAG = 'file'  # \"api\" or \"file\"\n",
    "# which storag to use\n",
    "if STORAGE_FLAG == 'api':\n",
    "    storage = TupliAPIClient()\n",
    "elif STORAGE_FLAG == 'file':\n",
    "    storage = FileStorage()\n",
    "else:\n",
    "    raise ValueError(f\"Unknown storage flag: {STORAGE_FLAG}. Has to be 'api' or 'file'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fb0ae",
   "metadata": {},
   "source": [
    "Our example system that we want to store as a benchmark is a single-family home with a PV array, a battery energy storage system (BESS), and, optionally, a heatpump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario(\n",
    "    forecaster: Forecaster,\n",
    "    controller: BaseController,\n",
    "    use_heat_pump: bool\n",
    "):\n",
    "    current_path = Path().absolute()\n",
    "    data_path = current_path / 'data'\n",
    "    date_format = \"%Y-%m-%d %H:%M:00\"\n",
    "\n",
    "    # create system\n",
    "    train_scenario = BuildingManagementSystemScenario(\n",
    "        data_path=data_path,\n",
    "        date_format=date_format,\n",
    "        forecaster=forecaster,\n",
    "        use_heat_pump=use_heat_pump,\n",
    "    )\n",
    "\n",
    "    sys = train_scenario.get_system()\n",
    "    \n",
    "    # add top level node to controller\n",
    "    controller.add_entity(sys.nodes[0])\n",
    "    return sys\n",
    "\n",
    "\n",
    "class BuildingManagementSystemScenario:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecaster: Forecaster,\n",
    "        data_path: Path,\n",
    "        date_format: str,\n",
    "        use_heat_pump: bool = False,\n",
    "        price_buying: float = 0.37,\n",
    "        price_selling: float = 0.08,  # numbers for 2024: https://www.finanztip.de/photovoltaik/einspeiseverguetung/\n",
    "    ):\n",
    "        self.use_heat_pump = use_heat_pump\n",
    "        self.price_buying = price_buying\n",
    "        self.price_selling = price_selling\n",
    "        self.data_path = data_path.resolve()\n",
    "        self.date_format = date_format\n",
    "        self.forecaster = forecaster\n",
    "        self.forecast_frequency = forecaster.frequency\n",
    "        self.forecast_horizon = forecaster.horizon\n",
    "        self.sys = self.create_system()\n",
    "    \n",
    "    def get_system(self) -> System:\n",
    "        return self.sys\n",
    "\n",
    "    def create_system(self) -> System:\n",
    "        return self._create_system()\n",
    "\n",
    "    def _create_system(self):\n",
    "        self.define_data_sources()\n",
    "        self.load_p_dp = DataProvider(self.p_load_ds, self.forecaster)  # [kW]\n",
    "        self.load_q_dp = DataProvider(self.q_load_ds, self.forecaster)  # [kVA]\n",
    "        self.price_dp = DataProvider(self.buying_price_ds, self.forecaster)  # [€]\n",
    "        self.selling_price_dp = DataProvider(self.selling_price_ds, self.forecaster)  # [€]\n",
    "        self.pv_dp = DataProvider(self.pv_ds, self.forecaster)  # [kW]\n",
    "        self.hp_dp = DataProvider(self.heat_pump_ds, self.forecaster)\n",
    "        # Let's first create an instance of the RTPricedBus with lower and upper bounds for its variables\n",
    "        n1 = RTPricedBus(\"MultiFamilyHouse\", {'p': (-50, 50), 'q': (-50, 50), 'v': (0.95, 1.05), 'd': (-15, 15)})\n",
    "        # Then, we add the previously defined data providers for the buying and selling price of electricity.\n",
    "        n1.add_data_provider(self.price_dp).add_data_provider(self.selling_price_dp)\n",
    "\n",
    "        # external grid\n",
    "        m1 = ExternalGrid(\"ExternalGrid\")\n",
    "\n",
    "        # photovoltaic with generation data\n",
    "        r1 = RenewableGen(\"PV1\").add_data_provider(self.pv_dp)\n",
    "\n",
    "        # static load with data source\n",
    "        d1 = Load(\"Load1\").add_data_provider(self.load_p_dp).add_data_provider(self.load_q_dp)\n",
    "\n",
    "        # battery storage\n",
    "        capacity = 5  # kWh\n",
    "        # Since it has proven beneficial for training to use constant initializers, we use this:\n",
    "        ess_initializer = ConstantInitializer(0.5 * capacity)\n",
    "        hp_initializer = ConstantInitializer(21)\n",
    "\n",
    "        e1 = ESSLinear(\n",
    "            \"ESS1\",\n",
    "            {\n",
    "                'p': (-1.5, 1.5),  # active power limits\n",
    "                'q': (0, 0),  # reactive power limits\n",
    "                'soc': (0.1 * capacity, 0.9 * capacity),  # soc limits\n",
    "                \"soc_init\": ess_initializer,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        h1 = HeatPumpWithoutStorageButCOP(\n",
    "            \"HeatPump\",\n",
    "            {\n",
    "                'p': [0, 5],  # kW\n",
    "                'T_indoor_setpoint': 21,  # Celsius\n",
    "                'T_indoor': [16, 26],  # Celsius\n",
    "                'T_indoor_init': hp_initializer,  # Celsius\n",
    "                'T_ret_FH': [10, 100],  # Celsius\n",
    "                'T_ret_FH_init': ConstantInitializer(25.0),  # Celsius\n",
    "                'H_FH': 1.1,  # kW/K\n",
    "                'H_out': 0.26,  # kW/K\n",
    "                'tau_building': 240,  # h\n",
    "                'Cw_FH': 1.1625,  # kWh/K\n",
    "                'c': 1.0,  # weighting factor (comfort factor) for cost function; multiplied with temperature deviation\n",
    "            },\n",
    "        ).add_data_provider(self.hp_dp)\n",
    "\n",
    "        # add components to the household\n",
    "        n1.add_node(d1).add_node(r1).add_node(e1)\n",
    "        if self.use_heat_pump:\n",
    "            n1.add_node(h1)\n",
    "\n",
    "        # create the system and add top-level busses\n",
    "        return System(power_flow_model=PowerBalanceModel()).add_node(n1).add_node(m1)\n",
    "\n",
    "    def define_data_sources(self):\n",
    "        # Data source (ds) for active power(p) of a household\n",
    "        self.p_load_ds = CSVDataSource(\n",
    "            self.data_path / 'data_ICLR' /'ICLR_load.csv', datetime_format=self.date_format, resample=self.forecast_frequency\n",
    "        )\n",
    "\n",
    "        # We neglect reactive power (q) during this tutorial\n",
    "        self.q_load_ds = ConstantDataSource(\n",
    "            {\"q\": 0.0}, date_range=self.p_load_ds.get_date_range(), frequency=self.forecast_frequency\n",
    "        )\n",
    "\n",
    "        self.buying_price_ds = ConstantDataSource(\n",
    "            {\"psib\": self.price_buying},\n",
    "            date_range=self.p_load_ds.get_date_range(),\n",
    "            frequency=self.forecast_frequency,\n",
    "        )\n",
    "\n",
    "        # Data source for selling prices of electricity\n",
    "        self.selling_price_ds = ConstantDataSource(\n",
    "            {\"psis\": self.price_selling},\n",
    "            date_range=self.buying_price_ds.get_date_range(),\n",
    "            frequency=self.forecast_frequency,\n",
    "        )\n",
    "\n",
    "        # Data source for PV generation\n",
    "        self.pv_ds = CSVDataSource(\n",
    "            self.data_path / 'data_ICLR' / 'ICLR_pv.csv', datetime_format=self.date_format, resample=self.forecast_frequency\n",
    "        ).apply_to_column(\"p\", lambda x: -x)\n",
    "\n",
    "        # Data sources for heat pump: outdoor temperature and coefficient of performance\n",
    "        # taken from When2Heat dataset: https://data.open-power-system-data.org/when2heat/\n",
    "        self.heat_pump_ds = CSVDataSource(\n",
    "            self.data_path / 'open-power-data' / 'DE_Temperature_and_COP2016_PV_Open_Power_Load_BDEW.csv',\n",
    "            datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "            rename_dict={\"time\": \"t\", \"outside_temp\": \"T_outside\", \"COP\": \"COP\"},\n",
    "            auto_drop=True,\n",
    "            delimiter=\";\",\n",
    "            resample=self.forecast_frequency,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2f382",
   "metadata": {},
   "source": [
    "For storing the benchmark, we will attach an optimal controller to the system. \n",
    "This controller can be replaced later by any other controller type, but is required to instantiate the gym environment in CommonPower.\n",
    "We will create two system, one with and one without a heatpump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = OptimalControllerPyTupli(\"opt_ctrl\", obs_handler=ObservationHandler(num_forecasts=6),)  # We have to use this controller type to ensure compatibility with PyTupli\n",
    "controller_hp = OptimalControllerPyTupli(\"opt_ctrl_hp\", obs_handler=ObservationHandler(num_forecasts=6),)\n",
    "global_controller = OptimalController(\"global_ctrl\")  # The global controller will balance the power flow in the system\n",
    "global_controller_hp = OptimalController(\"global_ctrl_hp\")\n",
    "\n",
    "# We use a persistence forecaster that provides forecasts for the next 6 hours based on historic data\n",
    "forecast_length = 6\n",
    "forecaster = PersistenceForecaster(\n",
    "    frequency=timedelta(hours=1), horizon=timedelta(hours=forecast_length), look_back=timedelta(hours=24)\n",
    "    )\n",
    "sys = create_scenario(forecaster=forecaster, use_heat_pump=False, controller=controller)\n",
    "sys_hp = create_scenario(forecaster=forecaster, use_heat_pump=True, controller=controller_hp)\n",
    "\n",
    "# We need the \"global controller\" to balance the power flow in the system\n",
    "global_controller.add_system(sys)\n",
    "global_controller_hp.add_system(sys_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6065361",
   "metadata": {},
   "source": [
    "We now have to initialize the system before we can create a gymnasium environment from it. We use a custom `Wrapper` that realizes the interface to PyTupli. Its main task consists of implementing \n",
    "the correct serialization and deserialization of time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# We have to fix a few parameters before we can initialize the system\n",
    "horizon = timedelta(hours=forecast_length)\n",
    "episode_length = 3 * 24  # three days\n",
    "tau = timedelta(hours=1)  # time step size\n",
    "solver = get_default_solver()\n",
    "sys.initialize(horizon=horizon, episode_horizon=timedelta(episode_length), tau=tau, solver=solver)\n",
    "sys_hp.initialize(horizon=horizon, episode_horizon=timedelta(episode_length), tau=tau, solver=solver)\n",
    "sys.reset(at_time=sys.sample_start_date())\n",
    "sys_hp.reset(at_time=sys_hp.sample_start_date())\n",
    "# instantiate the environment wrappers: One for handling deployment in CommonPower and one for interfacing PyTupli\n",
    "wrapper_stack = WrapperStack().add(DeploymentWrapper).add(CommonPowerTupliEnvWrapper, storage=storage, rl_tuple_cls=CommonPowerRLTuple)\n",
    "sys_env = sys.create_env_func(episode_length=episode_length, wrapper=wrapper_stack.get_stack())\n",
    "sys_hp_env = sys_hp.create_env_func(episode_length=episode_length, wrapper=wrapper_stack.get_stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cef593",
   "metadata": {},
   "source": [
    "Now, we can upload the environments to our storage. We will start with the more simple system that only has a BESS and a PV array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ee459",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_env.store(name=\"building_ess_pv\", description=\"Building with ESS and PV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cbc17",
   "metadata": {},
   "source": [
    "Let us have a look at the stored benchmarks and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0981f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.list_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.list_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa2fba",
   "metadata": {},
   "source": [
    "The two stored artifacts are the time series data for the static load and the PV power production. Next, we will serialize the benchmark with the heatpump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_hp_env.store(name=\"building_ess_pv_hp\", description=\"Building with ESS, PV and Heatpump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654b234",
   "metadata": {},
   "source": [
    "Let's look at the number of stored artifacts again. We should have 3 artifacts now, as the second environment uses the same time series data for \n",
    "the static load and the PV production as the first one. The additional data is for the outdoor temperature and the coefficient of performance (COP), \n",
    "which are stored in one CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5888ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.list_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45dc873",
   "metadata": {},
   "source": [
    "### Part 2: Creating Experience Datasets and Storing them with PyTupli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b537da1",
   "metadata": {},
   "source": [
    "#### Recording Interactions with an Optimal Controller\n",
    "We will now use CommonPower to simulate the building management system with a built-in optimal controller.\n",
    "All interactions should be saved as experience tuples (state, action, next state, reward) associated to the benchmark we created. This can be achieved by using our customized TupliEnvWrapper. We will pass some metadata to the episodes for later filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(EpisodeMetadataCallback):\n",
    "    def __init__(self, is_expert: bool = False):\n",
    "        super().__init__()\n",
    "        # we will compute the cumulative reward for an episode\n",
    "        self.cum_reward = 0\n",
    "        # Furthermore, we want to store the fact that the episode was an expert episode\n",
    "        self.is_expert = is_expert\n",
    "    def reset(self):\n",
    "        # we will compute the cumulative reward for an episode\n",
    "        self.cum_reward = 0\n",
    "    def __call__(self, tuple):\n",
    "        self.cum_reward += tuple.reward[0]\n",
    "        return {\"cum_eps_reward\": self.cum_reward, \"is_expert\": self.is_expert}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9ccee",
   "metadata": {},
   "source": [
    "We show how to load the benchmark from storage, although this would not have been necessary here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b51bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_sys_env = CommonPowerTupliEnvWrapper.load(\n",
    "    storage=storage, \n",
    "    benchmark_id=sys_env.id, \n",
    "    rl_tuple_cls=CommonPowerRLTuple, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fb076",
   "metadata": {},
   "source": [
    "Next, we instantiate a `DeploymentRunner` and pass our customized `TupliEnvWrapper` to record interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf50acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we extract the raw system data from the loaded environment\n",
    "loaded_sys = loaded_sys_env.get_system()\n",
    "horizon_loaded = loaded_sys.horizon\n",
    "# Then, we instantiate the runner and fix its start date\n",
    "runner_loaded = DeploymentRunner(\n",
    "    sys=loaded_sys,\n",
    "    horizon=horizon_loaded,\n",
    "    normalize_actions=False,  # IMPORTANT!\n",
    "    wrapper=WrapperStack().add(\n",
    "        CommonPowerTupliEnvWrapper, \n",
    "        storage=storage, \n",
    "        rl_tuple_cls=CommonPowerRLTuple, \n",
    "        benchmark_id=sys_env.id,\n",
    "        metadata_callback = MyCallback(is_expert=True)\n",
    "        ).get_stack()\n",
    ") \n",
    "runner_loaded.set_start_time(datetime.datetime(2016, 1, 2, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fb7c5",
   "metadata": {},
   "source": [
    "Running the simulation takes some time. You can adjust the number of steps to experiment with smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_loaded.run(n_steps=182*24)  # run for the first half year of 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0f7df",
   "metadata": {},
   "source": [
    "For later comparison, we will compute the costs obtained with the optimal controller on the second half of the year 2016.\n",
    "We will not record this data with PyTupli, thus we don't have to pass the respective wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loaded = ModelHistory([loaded_sys])  # create a history with the loaded system\n",
    "runner_loaded = DeploymentRunner(\n",
    "    sys=loaded_sys,\n",
    "    history=history_loaded,\n",
    "    horizon=horizon_loaded,\n",
    "    normalize_actions=False,  # IMPORTANT\n",
    ") \n",
    "runner_loaded.set_start_time(datetime.datetime(2016, 1, 2, 0, 0, 0) + timedelta(days=182))\n",
    "runner_loaded.run(n_steps=182*24)  # run for the second half year of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total cost\n",
    "total_cost_mpc = sum(get_adjusted_cost(history_loaded, loaded_sys))\n",
    "print(\"Total cost obtained with MPC:\", total_cost_mpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf453c",
   "metadata": {},
   "source": [
    "#### Recording Experience from RL Training\n",
    "Another option for creating experience datasets is to record transitions during online training of an RL agent. We will show an example of this procedure using the SAC implementation of StableBaselines3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f01045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will re-load the system to create a new instance\n",
    "loaded_sys_env = CommonPowerTupliEnvWrapper.load(\n",
    "    storage=storage, \n",
    "    benchmark_id=sys_env.id, \n",
    "    rl_tuple_cls=CommonPowerRLTuple, \n",
    "    )\n",
    "loaded_sys_rl = loaded_sys_env.get_system()\n",
    "# First, we need to instantiate an RL agent\n",
    "agent1 = RLControllerSB3(\n",
    "    name='agent1', \n",
    "    obs_handler=ObservationHandler(num_forecasts=6),\n",
    "    safety_layer=ActionProjectionSafetyLayer(penalty=DistanceDependingPenalty(penalty_factor=0.001)),\n",
    ")\n",
    "# Then, we can replace the optimal controller in the system with this agent\n",
    "agent1.add_entity(loaded_sys_rl.nodes[0])  # add the top-level node of the system to the controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e45255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we configure the algorithm\n",
    "sac_config = SB3MetaConfig(\n",
    "    total_steps=182*24,  # same number of environment interactions as optimal controller\n",
    "    seed=5,\n",
    "    algorithm=SAC,\n",
    "    penalty_factor=0.001,\n",
    "    algorithm_config=SB3SACConfig(\n",
    "        train_freq=1,\n",
    "        learning_rate=0.0008,\n",
    "        batch_size=12,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set up logger\n",
    "log_dir = './test_run/'\n",
    "logger = TensorboardLogger(log_dir='./test_run/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3c66a",
   "metadata": {},
   "source": [
    "The RL agent outputs random actions toward the beginning of the training, which is why we will set the `is_expert` parameter for the episode metadata to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18babffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where the model should be saved\n",
    "model_path = \"./saved_models/sac_model\"\n",
    "\n",
    "# Add SingleAgentWrapper for interfacing SB3 and CommonPowerTupliEnvWrapper for interfacing PyTupli\n",
    "wrapper_stack_sac = WrapperStack().add(SingleAgentWrapper).add(\n",
    "    CommonPowerTupliEnvWrapper,         \n",
    "    storage=storage, \n",
    "    rl_tuple_cls=CommonPowerRLTuple, \n",
    "    benchmark_id=sys_env.id,\n",
    "    metadata_callback=MyCallback(is_expert=False)\n",
    "    )\n",
    "# Now, we create a single-agent runner to train the agent\n",
    "runner_sac = SingleAgentTrainer(\n",
    "    sys=loaded_sys_rl, \n",
    "    wrapper=wrapper_stack_sac.get_stack(), \n",
    "    alg_config=sac_config, \n",
    "    horizon=horizon,\n",
    "    episode_length=24,\n",
    "    logger=logger,\n",
    "    save_path=model_path, \n",
    "    seed=sac_config.seed,\n",
    "    limited_date_range=[datetime.datetime(2016, 1, 2, 0, 0, 0), datetime.datetime(2016, 1, 2, 0, 0, 0) + timedelta(days=182)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6709cd",
   "metadata": {},
   "source": [
    "Finally, we can run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_sac.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb879275",
   "metadata": {},
   "source": [
    "### Part 3: Downloading Datasets for Offline RL Training\n",
    "Given our benchmark id, we can now use PyTupli to download all associated experience data. Furthermore, we can add filters to curate the obtained datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first download all data associated with the given benchmark.\n",
    "full_dataset = TupliDataset(storage=storage).with_benchmark_filter(\n",
    "    FilterEQ(key='id', value=sys_env.id)\n",
    ")\n",
    "full_dataset.load()\n",
    "# Convert to format specified by d3rlpy\n",
    "obs, act, rew, terminal, truncated = full_dataset.convert_to_tensors()\n",
    "# Create d3rlpy dataset\n",
    "full_d3rlpy_dataset = MDPDataset(\n",
    "    observations=obs, actions=act, rewards=rew, terminals=terminal, timeouts=truncated\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1bee5",
   "metadata": {},
   "source": [
    "Let us also create one dataset that uses only the expert data from the MPC controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_dataset = TupliDataset(storage=storage).with_benchmark_filter(\n",
    "    FilterEQ(key='id', value=sys_env.id)\n",
    "    ).with_episode_filter(\n",
    "        FilterEQ(key='metadata.is_expert', value=True)\n",
    "    )\n",
    "mpc_dataset.load()\n",
    "# Convert to format specified by d3rlpy\n",
    "obs, act, rew, terminal, truncated = mpc_dataset.convert_to_tensors()\n",
    "# Create d3rlpy dataset\n",
    "mpc_d3rlpy_dataset = MDPDataset(\n",
    "    observations=obs, actions=act, rewards=rew, terminals=terminal, timeouts=truncated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the d3rlpy dataset as a pickle file for later usage \n",
    "with open(\"./saved_models/full_d3rlpy_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_d3rlpy_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a72dd",
   "metadata": {},
   "source": [
    "### Offline RL Training using D3rlpy\n",
    "We will now use the CQL implementation by d3rlpy to train an RL agent for the given problem on the full dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm for offline training: CQL from d3rlpy\n",
    "d3rlpy.seed(1)  # for reproducibility\n",
    "algo = CQLConfig(batch_size=64, alpha_threshold=2.0, conservative_weight=5.0, soft_q_backup=True).create(device='cpu')\n",
    "# train\n",
    "algo.fit(dataset=full_d3rlpy_dataset, n_steps=10000, n_steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.save_model(\"./saved_models/cql_agent.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240d833",
   "metadata": {},
   "source": [
    "We can also train an agent on the reduced expert dataset. To save you some time, we have pre-trained this agent and will load it during deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18a02e",
   "metadata": {},
   "source": [
    "### Part 5: Comparing Controller Performance\n",
    "Next, we replace the optimal controller in our environment with one of the trained agents and simulate the system for the second half of the year 2016. We can then compare the costs obtained with the offline RL agents with those of the MPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first have to create a new controller that will load the trained model during deployment\n",
    "config = D3RLPyMetaConfig(\n",
    "    seed=42,\n",
    "    algorithm=d3rlpy.algos.CQLConfig,\n",
    "    algorithm_config=D3RLPyCQLConfig(),  # specify the hidden layer size\n",
    "    penalty_factor=0.0,\n",
    "    mdp_dataset_path=\"./saved_models/full_d3rlpy_dataset.pkl\"  # Updated to use pickle file\n",
    ")\n",
    "trained_agent = RLControllerD3RL(\n",
    "    name=\"cql_agent\", \n",
    "    safety_layer=ActionProjectionSafetyLayer(penalty=DistanceDependingPenalty(penalty_factor=0.0)), \n",
    "    pretrained_policy_path=\"./saved_models/cql_agent.pt\",\n",
    "    obs_handler=ObservationHandler(num_forecasts=6),\n",
    ")\n",
    "trained_agent.add_entity(loaded_sys.nodes[0])  # add the top-level node of the system to the controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efade4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cql = ModelHistory([loaded_sys])  # create a history with the loaded system\n",
    "runner_cql = DeploymentRunner(\n",
    "    alg_config=config,\n",
    "    sys=loaded_sys,\n",
    "    history=history_cql,\n",
    "    horizon=horizon_loaded,\n",
    "    normalize_actions=False,  # IMPORTANT\n",
    "    wrapper=SingleAgentWrapper\n",
    ") \n",
    "runner_cql.set_start_time(datetime.datetime(2016, 1, 2, 0, 0, 0) + timedelta(days=182))\n",
    "runner_cql.run(n_steps=182*24)  # run for the second half year of 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2622d",
   "metadata": {},
   "source": [
    "Now, we do the same with the offline RL agent trained purely on expert data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent_expert = RLControllerD3RL(\n",
    "    name=\"cql_agent_expert\", \n",
    "    safety_layer=ActionProjectionSafetyLayer(penalty=DistanceDependingPenalty(penalty_factor=0.0)), \n",
    "    pretrained_policy_path=\"./saved_models/cql_agent_expert.pt\",\n",
    "    obs_handler=ObservationHandler(num_forecasts=6),\n",
    ")\n",
    "trained_agent_expert.add_entity(loaded_sys.nodes[0])  # add the top-level node of the system to the controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d438f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cql_expert = ModelHistory([loaded_sys])  # create a history with the loaded system\n",
    "runner_cql = DeploymentRunner(\n",
    "    alg_config=config,\n",
    "    sys=loaded_sys,\n",
    "    history=history_cql_expert,\n",
    "    horizon=horizon_loaded,\n",
    "    normalize_actions=False,  # IMPORTANT\n",
    "    wrapper=SingleAgentWrapper\n",
    ") \n",
    "runner_cql.set_start_time(datetime.datetime(2016, 1, 2, 0, 0, 0) + timedelta(days=182))\n",
    "runner_cql.run(n_steps=182*24)  # run for the second half year of 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc20a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total cost\n",
    "total_cost_cql = sum(get_adjusted_cost(history_cql, loaded_sys))\n",
    "total_cost_cql_expert = sum(get_adjusted_cost(history_cql_expert, loaded_sys))\n",
    "print(\"Total cost obtained with CQL:\", total_cost_cql)\n",
    "print(\"Total cost obtained with CQL trained on expert data:\", total_cost_cql_expert)\n",
    "print(\"Total cost obtained with MPC:\", total_cost_mpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c564b6d",
   "metadata": {},
   "source": [
    "You can see that the CQL agent trained only on expert data performs better than the one trained on the full dataset which has double the size. However, it is still significantly worse than the optimal controller. To improve performance, one could consider larger datasets, longer training, or hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07206d",
   "metadata": {},
   "source": [
    "### Further Ideas an Clean-up\n",
    "Within this notebook, we only introduce the basic functionalities of PyTupli, like storing and retrieving benchmarks and datasets. We encourage you to discover its other capabilities, like advanced filtering and user management for large collaborative projects. For some inspiration, feel free to check out the [paper](https://arxiv.org/abs/2505.16754)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356fef9",
   "metadata": {},
   "source": [
    "As a last step, we delete the benchmark and associated episodes and artifacts from our storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31310a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_hp_env.delete(delete_episodes=True, delete_artifacts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytupli_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
