{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010c5955",
   "metadata": {},
   "source": [
    "# MARL training and deployment\n",
    "\n",
    "We use CommonPower to create a simulation of a power system where one node (corresponding to a multi-family household) is controlled by the RL agent. Since RL does not naturally allow considering constraints, such as a minimum state of charge of a battery, we have implemented a safety layer that is wrapped around the RL agent. It extracts all necessary constraints from the power system model and checks whether a control action suggested by the agent is safe. If necessary, the safety layer adjust the action, before passing it on to the simulation. The agent then receives a feedback informing it about the adjustment of its action.\n",
    "\n",
    "In this notebook, you will learn how to \n",
    "- use CommonPower to import a network topology from [pandapower](http://www.pandapower.org/)\n",
    "- add components like energy storage systems to a network\n",
    "- set up a decentralized control scheme with multiple RL agents\n",
    "- assign nodes to the agents\n",
    "- train the RL agents\n",
    "- monitor the training process using Tensorboard\n",
    "\n",
    "##### Prerequisites\n",
    "1. Install the requirements in `Readme.md`\n",
    "2. Optional (if you want to track the learning processes using Weights&Biases): Sign up for the [academic version of Weights&Biases](https://wandb.ai/site/research).\n",
    "3. Catch-up on basic knowledge on [Deep Reinforcement Learning (DRL)](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) and  Multi-Agent DRL (PPO) with [MAPPO](https://github.com/TUMcps/on-policy) (we use our own fork).\n",
    "4. Be familiar with [Tensorboard](https://www.tensorflow.org/tensorboard/get_started), a tool to track training of any machine learning project, and (optionally) [Weights&Biases](https://docs.wandb.ai/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "from numpy.random import uniform, choice\n",
    "from commonpower.models.components import *\n",
    "from commonpower.models.buses import *\n",
    "from commonpower.models.lines import *\n",
    "from commonpower.models.powerflow import *\n",
    "from commonpower.extensions.factories import Factory, Sampler\n",
    "from commonpower.data_forecasting import *\n",
    "from commonpower.modeling.param_initialization import RangeInitializer\n",
    "from commonpower.utils.helpers import get_adjusted_cost\n",
    "from commonpower.control.controllers import *\n",
    "from commonpower.control.observation_handling import ObservationHandler\n",
    "from commonpower.control.safety_layer.safety_layers import *\n",
    "from commonpower.control.safety_layer.penalties import *\n",
    "from commonpower.control.logging_utils.loggers import *\n",
    "from commonpower.control.runners import *\n",
    "from commonpower.control.wrappers import *\n",
    "from commonpower.control.configs.algorithms import *\n",
    "from commonpower.control.util import predicted_cost_callback\n",
    "\n",
    "from commonpower.extensions.network_import import PandaPowerImporter\n",
    "import pandapower.networks as pn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85578bf1",
   "metadata": {},
   "source": [
    "## System set-up\n",
    "First, we have to define the energy system the RL agents should interact with. We will use a small network that we import from pandapower to get the network topology and characteristics (line admittances etc.). Then, we will add components to the load buses of this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of control steps\n",
    "horizon = timedelta(hours=24)\n",
    "frequency = timedelta(minutes=60)\n",
    "\n",
    "# path to data profiles\n",
    "current_path = pathlib.Path().absolute()\n",
    "data_path = current_path / 'data'\n",
    "data_path = data_path.resolve()\n",
    "\n",
    "ds1a = CSVDataSource(\n",
    "    data_path / '1-LV-rural2--1-sw' / 'LoadProfile.csv',\n",
    "    delimiter=\";\",\n",
    "    datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "    rename_dict={\"time\": \"t\", \"H0-A_pload\": \"p\", \"H0-A_qload\": \"q\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency).apply_to_column(\"p\", lambda x: 10 * x).apply_to_column(\"q\", lambda x: 0.0)\n",
    "\n",
    "ds1b = CSVDataSource(\n",
    "    data_path / '1-LV-rural2--1-sw' / 'LoadProfile.csv',\n",
    "    delimiter=\";\",\n",
    "    datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "    rename_dict={\"time\": \"t\", \"H0-A_pload\": \"p\", \"H0-A_qload\": \"q\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency\n",
    ").apply_to_column(\"p\", lambda x: 10 * x).shift_time_series(timedelta(hours=24)).apply_to_column(\"q\", lambda x: 0.0)\n",
    "\n",
    "ds1c = CSVDataSource(\n",
    "    data_path / '1-LV-rural2--1-sw' / 'LoadProfile.csv',\n",
    "    delimiter=\";\",\n",
    "    datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "    rename_dict={\"time\": \"t\", \"H0-B_pload\": \"p\", \"H0-B_qload\": \"q\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency).apply_to_column(\"p\", lambda x: 10 * x).apply_to_column(\"q\", lambda x: 0.0)\n",
    "\n",
    "ds2 = CSVDataSource(\n",
    "    data_path / 'spot_prices_dk.csv',\n",
    "    delimiter=\";\",\n",
    "    decimal=\",\",\n",
    "    datetime_format=\"%Y-%m-%d %H:%M\",\n",
    "    rename_dict={\"HourUTC\": \"t\", \"SpotPriceEUR\": \"psi\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency).apply_to_column(\"psi\", lambda x: x / 1000)  # prices are EUR/MWh\n",
    "\n",
    "ds3a = CSVDataSource(\n",
    "    data_path / '1-LV-rural2--1-sw' / 'RESProfile.csv',\n",
    "    delimiter=\";\",\n",
    "    datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "    rename_dict={\"time\": \"t\", \"PV3\": \"p\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency).apply_to_column(\"p\", lambda x: -10 * x)\n",
    "\n",
    "ds3b = CSVDataSource(\n",
    "    data_path / '1-LV-rural2--1-sw' / 'RESProfile.csv',\n",
    "    delimiter=\";\",\n",
    "    datetime_format=\"%d.%m.%Y %H:%M\",\n",
    "    rename_dict={\"time\": \"t\", \"PV7\": \"p\"},\n",
    "    auto_drop=True,\n",
    "    resample=frequency).apply_to_column(\"p\", lambda x: -10 * x)\n",
    "\n",
    "dp1a = DataProvider(ds1a, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "dp1b = DataProvider(ds1b, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "dp1c = DataProvider(ds1c, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "dp2 = DataProvider(ds2, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "dp3a = DataProvider(ds3a, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "dp3b = DataProvider(ds3b, NoisyForecaster(frequency=frequency, horizon=horizon))\n",
    "\n",
    "# We are using DC powerflow \n",
    "power_flow_mode = DCPowerFlowModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1bd33c",
   "metadata": {},
   "source": [
    "We use a Factory so we do not have to manually add components to each load bus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedad911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual system (decentralized control)\n",
    "rand_seed = 888\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "ind_factory = Factory()\n",
    "\n",
    "ind_factory.set_bus_template(\n",
    "    RTPricedBusLinear,\n",
    "    meta_config={\n",
    "        \"p\": Sampler(uniform, low=[-1e3, 1e3], high=[-1e3, 1e3]),\n",
    "        \"q\": Sampler(uniform, low=[-1e3, 1e3], high=[-1e3, 1e3]),\n",
    "        \"v\": Sampler(uniform, low=[0.9, 1.1], high=[0.9, 1.1]),\n",
    "        \"d\": Sampler(uniform, low=[-15, 15], high=[-15, 15])\n",
    "    },\n",
    "    data_providers=[dp2]\n",
    ")\n",
    "\n",
    "# add components to factory\n",
    "# Load: base load of the household (corresponds to fridge, dishwasher, washing machine, etc.)\n",
    "ind_factory.add_component_template(Load, probability=1., data_providers=[Sampler(choice, a=[dp1a, dp1b, dp1c])])\n",
    "\n",
    "# RenewableGen: renewable generation (e.g., PV)\n",
    "ind_factory.add_component_template(\n",
    "    RenewableGen,\n",
    "    probability=.5,\n",
    "    meta_config={\n",
    "        \"p\": Sampler(uniform, low=[-7, 0], high=[-7, 0]),\n",
    "        \"q\": Sampler(uniform, low=[0, 0], high=[0, 0])\n",
    "    },\n",
    "    data_providers=[Sampler(choice, a=[dp3a, dp3b])]\n",
    ")\n",
    "\n",
    "# ESSLinear: energy storage system (e.g., battery) with highly simplified dynamics  (to reduce computation time)\n",
    "ess_capa = 10  # kwh\n",
    "ind_factory.add_component_template(\n",
    "    ESSLinear,\n",
    "    probability=0.5,\n",
    "    meta_config={\n",
    "        'rho': 0.1,\n",
    "        'p': Sampler(uniform, low=[-5, 5], high=[-5, 5]),\n",
    "        'q': Sampler(uniform, low=[0, 0], high=[0, 0]),\n",
    "        'etac': 0.95,\n",
    "        'etad': 0.95,\n",
    "        'etas': 0.99,\n",
    "        'soc': Sampler(uniform, low=[0.2 * ess_capa, 0.8 * ess_capa], high=[0.2 * ess_capa, 0.8 * ess_capa]),\n",
    "        \"soc_init\": [RangeInitializer, {\"lb\": Sampler(uniform, low=0.3 * ess_capa, high=0.3 * ess_capa),\n",
    "                                        \"ub\": Sampler(uniform, low=0.5 * ess_capa, high=0.5 * ess_capa)}]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7194e9",
   "metadata": {},
   "source": [
    "Now we load the network topology from pandapower and create a power system using the factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b62475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize system\n",
    "net = pn.create_kerber_landnetz_kabel_2()\n",
    "ind_sys = PandaPowerImporter().import_net(\n",
    "    net=net,\n",
    "    power_flow_model=power_flow_mode,\n",
    "    node_factory=ind_factory,\n",
    "    restrict_factory_to=\"loadbus\"\n",
    ")\n",
    "\n",
    "n999 = ExternalGrid(\"ExternalGrid\")\n",
    "\n",
    "# set node 1 (main_busbar) as external grid connection, i.e. trading node to import unbalanced quantities of power\n",
    "ind_sys.add_node(n999, at_index=1)\n",
    "\n",
    "# update the respective lines\n",
    "ind_sys.lines[0].src = n999\n",
    "ind_sys.lines[24].src = n999\n",
    "\n",
    "# remove line from ext to main busbar\n",
    "ind_sys.lines.pop(-1)\n",
    "# remove node 0\n",
    "ind_sys.nodes.pop(0)\n",
    "# Show the system set-up\n",
    "ind_sys.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af0348",
   "metadata": {},
   "source": [
    "## MARL Training\n",
    "Now that we have the system set-up, we can add controllers to all buses that have controllable components. In this case, this means all buses with a battery storage system (ESSLinear). \n",
    "\n",
    "There are many hyperparameters for MARL training. The most important ones are the `num_env_steps`, which determines the length of the training, and the `episode_length`, which determines how many days of data we collect before updating the policies. This should be a multiple of the `horizon` parameter. We use pydantic classes to handle the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e60a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic class\n",
    "mappo_config = MAPPOBaseConfig(\n",
    "    algorithm_name='mappo',\n",
    "    seed=1,\n",
    "    num_env_steps=1200 * int(horizon.total_seconds() // 3600),\n",
    "    episode_length=3 * int(horizon.total_seconds() // 3600),\n",
    "    penalty_factor=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde81ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add individual RL controllers\n",
    "for i in range(len(ind_sys.nodes)):\n",
    "    # will also add a controller to households which do not have inputs (e.g., households with only a Load component),\n",
    "    # but these are disregarded when the system is initialized\n",
    "    _ = RLControllerMA(\n",
    "        name=f\"agent{i}\",\n",
    "        obs_handler=ObservationHandler(num_forecasts=6),\n",
    "        cost_callback=predicted_cost_callback,\n",
    "        safety_layer=ActionProjectionSafetyLayer(\n",
    "            penalty=DistanceDependingPenalty(penalty_factor=mappo_config.penalty_factor)\n",
    "        )\n",
    "    ).add_entity(ind_sys.nodes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a5695",
   "metadata": {},
   "source": [
    "For logging the training, you can use tensorboard or Weights&Biases, as described above. When you use W&B you need to change the `entity_name` to the name of your W&B team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging using Tensorboard\n",
    "logger = MARLTensorboardLogger(\n",
    "    log_dir=\"./test_run/\",\n",
    "    callback=MARLBaseCallback\n",
    ")\n",
    "# logging using WandB\n",
    "#logger = MARLWandBLogger(\n",
    "#    log_dir=\"./test_run/\",\n",
    "#    entity_name=\"srl4ps\",  # change to your team name!\n",
    "#    project_name=\"commonpower\",\n",
    "#    alg_config=mappo_config,\n",
    "#    callback=MARLWandBCallback\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cda4e1",
   "metadata": {},
   "source": [
    "The actual training will happen in the next cell. WARNING: It will take several hours until the training fully converges. If you just want to get an idea of what the training process would look like, reduce the `num_env_steps` in the `mappo_config` above. You can also skip the training and go directly to the benchmarking, in which case you will use agents that we have pre-trained for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf923bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = MAPPOTrainer(\n",
    "    sys=ind_sys,\n",
    "    global_controller=OptimalController('global'),\n",
    "    wrapper=MultiAgentWrapper,\n",
    "    alg_config=mappo_config,\n",
    "    seed=mappo_config.seed,\n",
    "    logger=logger\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897f1ae",
   "metadata": {},
   "source": [
    "### Training visualization\n",
    "If you used the TensorBoardLogger, you can plot the training metrics using the notebook magic of tensorboard. The metrics are sorted by agent. The most interesting metrics for us are the __average_episode_rewards__, the __ep_penalty_mean__, and the __value_loss__. Think about what these charts tell you and discuss it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad957e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b77cb",
   "metadata": {},
   "source": [
    "## Benchmarking MARL and decentralized optimal control\n",
    "We want to benchmark our trained agents against decentralized optimal control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90057e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for deployment\n",
    "n_deployment_steps = 48\n",
    "eval_seed = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea4a2d",
   "metadata": {},
   "source": [
    "### Decentralized optimal control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d740720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimal controllers (will overwrite the RL controllers)\n",
    "for i in range(len(ind_sys.nodes)):\n",
    "    # will also add a controller to households which do not have inputs (e.g., households with only a Load component), \n",
    "    # but these are disregarded when the system is initialized\n",
    "    _ = OptimalController(f\"agent{i}\").add_entity(ind_sys.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sys_history = ModelHistory([ind_sys])\n",
    "runner = DeploymentRunner(sys=ind_sys, global_controller=OptimalController(\"global\"),\n",
    "                          seed=eval_seed, history=ind_sys_history, continuous_control=True)\n",
    "runner.run(n_steps=n_deployment_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950a093",
   "metadata": {},
   "source": [
    "### MARL deployment\n",
    "After training the agents, we will showcase how to load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve information which agent controlled which nodes\n",
    "top_level_nodes = []\n",
    "for ctrl in ind_sys.controllers.values():\n",
    "    top_level_nodes.append(ctrl.top_level_nodes)\n",
    "    \n",
    "# deployment of trained agents\n",
    "load_path = \"./saved_models/test_model/\"  # In case you used W&B for logging, the models will be saved in \"./test_run/models/og1te5k4\", where you have to replace the last part with the respective run ID. \n",
    "agents = []\n",
    "for i in range(len(ind_sys.controllers)):\n",
    "    agent_i = RLControllerMA(\n",
    "        name=f\"agent{i}\",\n",
    "        obs_handler=ObservationHandler(num_forecasts=6),\n",
    "        safety_layer=ActionProjectionSafetyLayer(\n",
    "            penalty=DistanceDependingPenalty(penalty_factor=mappo_config.penalty_factor)\n",
    "        ),\n",
    "        pretrained_policy_path=load_path+f\"/agent{i}\"\n",
    "    ).add_entity(top_level_nodes[i][0])\n",
    "    agents.append(agent_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b57dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sys_history_marl = ModelHistory([ind_sys])\n",
    "runner = DeploymentRunner(sys=ind_sys, global_controller=OptimalController(\"global\"), alg_config=mappo_config,\n",
    "                          wrapper=MultiAgentWrapper, history=ind_sys_history_marl, seed=eval_seed, continuous_control=True)\n",
    "runner.run(n_steps=n_deployment_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ae887",
   "metadata": {},
   "source": [
    "### Comparison of total cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7547333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare controllers by tracking the realized cost until the last timestep. \n",
    "# The cost of the last timestep is the accumulated cost of the projected horizon. \n",
    "# Since the projection is computed by the system's \"internal\" solver, which is by definition optimal wrt. to the system's cost function, this represents the \"best case\" cost (subject to the forecaster).\n",
    "# This makes sure that costs realized in the future, e.g. by discharing batteries, is considered in the comparison.\n",
    "decentralized_cost = get_adjusted_cost(ind_sys_history, ind_sys)\n",
    "decentralized_cost_rl = get_adjusted_cost(ind_sys_history_marl, ind_sys)\n",
    "print(f\"decentralized_cost: {sum(decentralized_cost)}\")\n",
    "print(f\"decentralized_cost_rl: {sum(decentralized_cost_rl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e28990",
   "metadata": {},
   "source": [
    "### Comparison of controllers for one day\n",
    "Next, we will show an example of the difference in behavior of an RL controller and an optimal controller for a given day and one household. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # day to use\n",
    "start = datetime(2016, 8, 29, 0, 0)\n",
    "end = datetime(2016, 8, 29, 23, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_history = ind_sys_history.filter_for_entities(ind_sys.nodes[4]).filter_for_time_period(start,end).filter_for_element_names([\"psi\"]).filter_for_time_index().history\n",
    "prices = [t[1]['n4.psi'] for t in price_history]\n",
    "time_stamps = [t[0] for t in price_history]\n",
    "cost_history = ind_sys_history.filter_for_entities(ind_sys.nodes[4]).filter_for_time_period(start,end).filter_for_element_names([\"cost\"]).filter_for_time_index().history\n",
    "costs = [t[1]['n4.cost'] for t in cost_history]\n",
    "soc_history = ind_sys_history.filter_for_entities(ind_sys.nodes[4].nodes[2]).filter_for_time_period(start,end).filter_for_element_names([\"soc\"]).filter_for_time_index().history\n",
    "soc = [t[1][\"n4.el42.soc\"] for t in soc_history]\n",
    "soc_history_rl = ind_sys_history_marl.filter_for_entities(ind_sys.nodes[4].nodes[2]).filter_for_time_period(start,end).filter_for_element_names([\"soc\"]).filter_for_time_index().history\n",
    "soc_rl = [t[1][\"n4.el42.soc\"] for t in soc_history_rl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1364b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def make_results_plot(time_stamps, prices, soc_oc, soc_rl):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(time_stamps, prices, color='blue', label='Spot market prices')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(time_stamps, soc_oc, color='orange', label='SoC Optimal Controller')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.plot(time_stamps, soc_rl, color='green', label='SoC RL Controller')\n",
    "    ax3.tick_params(axis='y', labelcolor='green')\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    fig.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "make_results_plot(time_stamps, prices, soc, soc_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a85e5e",
   "metadata": {},
   "source": [
    "As you can see, the optimal controller is far more aggressive than the RL controller. This is probably due to the safety constraints, since the agent learned not to get close to the limits of the battery. It gives a hint on why the optimal controller performs better in the overall cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e80d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
