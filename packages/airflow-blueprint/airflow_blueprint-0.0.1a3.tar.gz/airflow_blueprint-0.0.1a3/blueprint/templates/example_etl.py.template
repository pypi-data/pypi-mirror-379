"""Example ETL Blueprint template."""

from datetime import datetime, timedelta, timezone
from typing import List

from blueprint import BaseModel, Blueprint, Field


class ExampleETLConfig(BaseModel):
    """Configuration for the Example ETL blueprint."""
    source_table: str = Field(description="Source table to read from")
    destination_table: str = Field(description="Destination table to write to")

    # Optional parameters with defaults
    batch_size: int = Field(default=1000, description="Number of records to process at once")

    # DAG configuration
    dag_id: str = Field(default="example_etl", description="Unique identifier for the DAG")
    schedule: str = Field(default="@daily", description="DAG schedule interval")
    retries: int = Field(default=2, description="Number of retry attempts on task failure")
    catchup: bool = Field(default=False, description="Whether to backfill missed runs")
    tags: List[str] = Field(
        default=["example", "etl", "blueprint"],
        description="DAG tags for organization"
    )


class ExampleETL(Blueprint[ExampleETLConfig]):
    """Example ETL blueprint for demonstration purposes."""

    def render(self, config: ExampleETLConfig):
        """Create an ETL DAG from configuration."""
        from airflow import DAG
        from airflow.decorators import task

        default_args = {
            "owner": "data-team",
            "retries": config.retries,
            "retry_delay": timedelta(minutes=5),
            "email_on_failure": False,
        }

        dag = DAG(
            dag_id=config.dag_id,
            default_args=default_args,
            description=f"ETL from {config.source_table} to {config.destination_table}",
            schedule=config.schedule,
            start_date=datetime(2024, 1, 1, tzinfo=timezone.utc),
            catchup=config.catchup,
            tags=config.tags,
        )

        with dag:
            @task
            def extract():
                """Extract data from source."""
                print(f"Extracting from {config.source_table}")
                print(f"Using batch size: {config.batch_size}")
                # Your extraction logic here
                return {"records": 100}

            @task
            def transform(data: dict):
                """Transform the data."""
                print(f"Transforming {data['records']} records")
                # Your transformation logic here
                return {"transformed_records": data["records"]}

            @task
            def load(data: dict):
                """Load data to destination."""
                print(f"Loading {data['transformed_records']} records to {config.destination_table}")
                # Your loading logic here

            # Define task dependencies
            data = extract()
            transformed = transform(data)
            load(transformed)

        return dag
