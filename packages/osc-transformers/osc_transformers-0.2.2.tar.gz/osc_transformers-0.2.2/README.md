<div align="center">

# OSC-Transformers

**ğŸš€ åŸºäºé…ç½®æ–‡ä»¶çš„æ¨¡å—åŒ– Transformer æ¨¡å‹æ„å»ºæ¡†æ¶**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

*çµæ´»ã€é«˜æ•ˆã€å¯æ‰©å±•çš„ Transformer æ¨¡å‹æ„å»ºå·¥å…·*

</div>

## âœ¨ ç‰¹æ€§

- ğŸ”§ **é…ç½®é©±åŠ¨**: é€šè¿‡ç®€å•é…ç½®æ–‡ä»¶æ„å»º Transformer æ¨¡å‹
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**: æ”¯æŒè‡ªå®šä¹‰æ³¨å†Œå„ç±»ç»„ä»¶
- âš¡ **é«˜æ€§èƒ½**: æ”¯æŒ CUDA Graph å’Œ Paged Attention
- ğŸ¯ **æ˜“äºä½¿ç”¨**: æä¾› Builder æ¨¡å¼å’Œé…ç½®æ–‡ä»¶ä¸¤ç§æ–¹å¼

## ğŸ› ï¸ æ”¯æŒç»„ä»¶

| ç»„ä»¶ç±»å‹ | å†…ç½®å®ç° |
|---------|---------|
| æ³¨æ„åŠ›æœºåˆ¶ | `PagedAttention` |
| å‰é¦ˆç½‘ç»œ | `SwiGLU` |
| å½’ä¸€åŒ– | `RMSNorm` |
| åµŒå…¥å±‚ | `VocabEmbedding` |
| è¾“å‡ºå¤´ | `LMHead` |

## ğŸ“¦ å®‰è£…

```bash
pip install osc-transformers
```

ç¯å¢ƒè¦æ±‚ï¼šPython >= 3.10, PyTorch >= 2.8.0

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»º `model.cfg`:
```toml
[model]
@architecture = "TransformerDecoder"
num_layers = 12
max_length = 2048

[model.attention]
@attention = "PagedAttention"
in_dim = 768
num_heads = 12

[model.embedding]
@embedding = "VocabEmbedding"
num_embeddings = 30000
embedding_dim = 768
```

åŠ è½½æ¨¡å‹ï¼š
```python
from osc_transformers import TransformerDecoder
model = TransformerDecoder.from_config("model.cfg")
```


## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License