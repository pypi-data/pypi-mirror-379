#!/usr/bin/env python3
"""
Production Scale Test with Real Amazon Dataset

This test validates performance with the actual 2.2M row Amazon UK dataset
to ensure production readiness.
"""

import time
import pandas as pd
import sys
import os
import psutil
from typing import Dict, Any

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def get_memory_usage():
    """Get current memory usage in MB."""
    process = psutil.Process()
    return process.memory_info().rss / 1024 / 1024

def load_amazon_dataset(sample_size: int = None) -> pd.DataFrame:
    """Load Amazon UK dataset with optional sampling."""
    
    dataset_path = "Performance/amz_uk_processed_data.csv"
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset not found: {dataset_path}")
        print("üìÅ Available files in Performance/:")
        if os.path.exists("Performance"):
            for file in os.listdir("Performance"):
                print(f"   - {file}")
        return None
    
    print(f"üìä Loading Amazon UK dataset...")
    
    try:
        if sample_size:
            # Load with sampling for memory efficiency
            df = pd.read_csv(dataset_path, nrows=sample_size)
            print(f"‚úÖ Loaded {len(df):,} rows (sampled from dataset)")
        else:
            df = pd.read_csv(dataset_path)
            print(f"‚úÖ Loaded full dataset: {len(df):,} rows")
        
        print(f"   Columns: {', '.join(df.columns.tolist())}")
        print(f"   Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error loading dataset: {e}")
        return None

def create_realistic_patterns_for_amazon_data(df: pd.DataFrame) -> Dict[str, str]:
    """Create realistic business patterns based on Amazon dataset structure."""
    
    # Analyze dataset structure
    print(f"üîç Analyzing dataset for realistic patterns...")
    print(f"   Dataset shape: {df.shape}")
    print(f"   Sample data:")
    print(df.head().to_string())
    
    # Create patterns based on actual Amazon data patterns
    patterns = {}
    
    # Check if we have categorical columns suitable for pattern matching
    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    print(f"   Categorical columns: {categorical_columns}")
    print(f"   Numeric columns: {numeric_columns}")
    
    # Create synthetic categorical column for pattern testing if needed
    if 'category' not in categorical_columns:
        print(f"   üîß Creating synthetic 'category' column for pattern testing...")
        # Create categories based on price ranges or other numeric data
        if 'price' in df.columns:
            df['category'] = pd.cut(df['price'], 
                                   bins=[-float('inf'), 10, 25, 50, float('inf')], 
                                   labels=['A', 'B', 'C', 'D'])
        elif len(numeric_columns) > 0:
            # Use first numeric column
            col = numeric_columns[0]
            df['category'] = pd.cut(df[col], 
                                   bins=5, 
                                   labels=['A', 'B', 'C', 'D', 'E'])
        else:
            # Create based on row index
            df['category'] = ['A', 'B', 'C', 'D', 'E'][df.index % 5]
    
    # Business-realistic patterns for e-commerce data
    patterns = {
        'price_trend_analysis': 'A+ B+',  # Low prices followed by high prices
        'customer_journey': 'A (B|C)+ D*',  # Browse -> consideration -> purchase
        'seasonal_pattern': 'A{2,5} B* C+',  # Seasonal buying patterns
        'product_lifecycle': 'A+ B{1,3} C* D?',  # Launch -> growth -> mature -> decline
        'promotion_effect': '(A|B)+ C{2,10}',  # Promotion triggers buying
    }
    
    return patterns

def test_production_scale_performance(max_rows: int = 100000):
    """Test production scale performance with real Amazon data."""
    
    print("üöÄ Production Scale Performance Test")
    print("=" * 50)
    print(f"Testing with Amazon UK dataset (up to {max_rows:,} rows)")
    print()
    
    # Load dataset
    df = load_amazon_dataset()
    if df is None:
        print("‚ùå Cannot proceed without dataset")
        return
    
    # Get realistic patterns
    patterns = create_realistic_patterns_for_amazon_data(df)
    
    # Test with increasing sample sizes
    test_sizes = [10000, 25000, 50000, 100000, 200000]
    if len(df) < max_rows:
        test_sizes = [size for size in test_sizes if size <= len(df)]
        test_sizes.append(len(df))  # Test with full dataset
    
    from src.executor.match_recognize import match_recognize
    
    results = []
    
    for size in test_sizes:
        print(f"\nüìä Testing with {size:,} rows")
        print("-" * 30)
        
        # Sample data
        test_df = df.head(size).copy()
        
        for pattern_name, pattern in patterns.items():
            print(f"   üîç {pattern_name}: {pattern}... ", end="", flush=True)
            
            try:
                # Build query
                query = f"""
                SELECT *
                FROM data
                MATCH_RECOGNIZE (
                    ORDER BY ROWNUM
                    MEASURES 
                        FIRST(A.ROWNUM) as start_row,
                        LAST(D.ROWNUM) as end_row,
                        COUNT(*) as match_length
                    PATTERN ({pattern})
                    DEFINE
                        A AS category = 'A',
                        B AS category = 'B',
                        C AS category = 'C',
                        D AS category = 'D',
                        E AS category = 'E'
                )
                """
                
                # Add ROWNUM for ordering if not present
                if 'ROWNUM' not in test_df.columns:
                    test_df['ROWNUM'] = range(len(test_df))
                
                # Measure memory and time
                memory_before = get_memory_usage()
                start_time = time.time()
                
                result = match_recognize(query, test_df)
                
                end_time = time.time()
                memory_after = get_memory_usage()
                
                execution_time = end_time - start_time
                memory_used = memory_after - memory_before
                matches = len(result)
                throughput = size / execution_time
                
                # Status
                if execution_time < 1.0:
                    status = "üöÄ"
                elif execution_time < 5.0:
                    status = "‚úÖ"
                elif execution_time < 15.0:
                    status = "‚ö†Ô∏è"
                else:
                    status = "üö®"
                
                print(f"{status} {execution_time:.3f}s | {matches:,} matches | {throughput:,.0f} rows/sec")
                
                results.append({
                    'size': size,
                    'pattern': pattern_name,
                    'time': execution_time,
                    'matches': matches,
                    'throughput': throughput,
                    'memory_mb': memory_used
                })
                
                # Stop if too slow
                if execution_time > 30:
                    print(f"      ‚èπÔ∏è  Pattern too slow for larger datasets")
                    break
                    
            except Exception as e:
                print(f"‚ùå ERROR: {str(e)}")
    
    # Summary analysis
    print(f"\nüèÜ PRODUCTION SCALE SUMMARY")
    print("=" * 35)
    
    if results:
        # Best performers
        best_throughput = max(results, key=lambda x: x['throughput'])
        print(f"üöÄ Best performance: {best_throughput['throughput']:,.0f} rows/sec")
        print(f"   Pattern: {best_throughput['pattern']}")
        print(f"   Dataset size: {best_throughput['size']:,} rows")
        
        # Memory efficiency
        avg_memory = sum(r['memory_mb'] for r in results) / len(results)
        print(f"üìä Average memory usage: {avg_memory:.1f} MB per query")
        
        # Production readiness assessment
        production_ready_patterns = [r for r in results if r['throughput'] > 5000 and r['time'] < 10]
        print(f"‚úÖ Production-ready patterns: {len(production_ready_patterns)}/{len(results)}")
        
        if len(production_ready_patterns) == len(results):
            print(f"üéâ ALL PATTERNS ARE PRODUCTION READY!")
        elif len(production_ready_patterns) > len(results) * 0.8:
            print(f"üéØ Most patterns are production ready")
        else:
            print(f"‚ö†Ô∏è  Some patterns may need optimization")
    
    return results

def test_full_dataset_capability():
    """Test capability to handle the full 2.2M row dataset."""
    
    print(f"\nüéØ Full Dataset Capability Test")
    print("=" * 40)
    
    # Load full dataset info
    dataset_path = "Performance/amz_uk_processed_data.csv"
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset not found, skipping full dataset test")
        return
    
    # Get dataset size without loading it fully
    with open(dataset_path, 'r') as f:
        line_count = sum(1 for line in f) - 1  # Subtract header
    
    print(f"üìä Full dataset: {line_count:,} rows")
    
    # Estimate performance based on smaller tests
    # Test with 10% sample
    sample_size = max(10000, line_count // 10)
    
    print(f"üî¨ Testing with {sample_size:,} row sample...")
    
    df = load_amazon_dataset(sample_size)
    if df is None:
        return
    
    patterns = create_realistic_patterns_for_amazon_data(df)
    
    from src.executor.match_recognize import match_recognize
    
    # Test the fastest pattern on larger sample
    fastest_pattern = 'A+ B+'  # We know this is optimized
    
    query = f"""
    SELECT *
    FROM data
    MATCH_RECOGNIZE (
        ORDER BY ROWNUM
        MEASURES 
            FIRST(A.ROWNUM) as start_row,
            LAST(B.ROWNUM) as end_row
        PATTERN ({fastest_pattern})
        DEFINE
            A AS category = 'A',
            B AS category = 'B'
    )
    """
    
    if 'ROWNUM' not in df.columns:
        df['ROWNUM'] = range(len(df))
    
    start_time = time.time()
    result = match_recognize(query, df)
    end_time = time.time()
    
    execution_time = end_time - start_time
    throughput = sample_size / execution_time
    
    print(f"‚úÖ Sample performance: {throughput:,.0f} rows/sec")
    
    # Estimate full dataset time
    estimated_full_time = line_count / throughput
    
    print(f"üìà Estimated full dataset processing time: {estimated_full_time:.1f} seconds")
    
    if estimated_full_time < 60:
        print(f"üöÄ EXCELLENT: Full dataset processing under 1 minute")
    elif estimated_full_time < 300:
        print(f"‚úÖ GOOD: Full dataset processing under 5 minutes")
    elif estimated_full_time < 900:
        print(f"‚ö†Ô∏è  ACCEPTABLE: Full dataset processing under 15 minutes")
    else:
        print(f"üö® SLOW: Full dataset processing over 15 minutes")

def main():
    """Run production scale tests."""
    
    print("üéØ Production Scale Validation")
    print("=" * 40)
    print("Testing with real Amazon UK dataset (2.2M rows)")
    print()
    
    try:
        # Test production scale performance
        results = test_production_scale_performance()
        
        # Test full dataset capability
        test_full_dataset_capability()
        
        print(f"\nüéä PRODUCTION VALIDATION COMPLETE!")
        
    except Exception as e:
        print(f"‚ùå Production test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
