"""
Production-ready pattern tokenizer for SQL:2016 row pattern matching.

This module implements comprehensive pattern tokenization with full support for:
- Complex PERMUTE patterns with nested structures
- Advanced quantifiers (greedy, reluctant, possessive)
- Pattern exclusions with proper nesting
- Alternation patterns with priority handling
- Anchor patterns (start/end) with validation
- Comprehensive error handling and validation
- Performance optimization for large patterns

Features:
- Thread-safe tokenization with proper validation
- Advanced syntax error reporting with context
- Memory-efficient processing for large patterns
- Comprehensive SQL:2016 compliance
- Production-grade error handling and recovery

Author: Pattern Matching Engine Team
Version: 3.0.0
"""

import re
import logging
import threading
from enum import Enum
from dataclasses import dataclass, field
from typing import List, Optional, Set, Tuple, Dict, Any, Union, Iterator
from contextlib import contextmanager

from src.utils.logging_config import get_logger, PerformanceTimer
from src.pattern.permute_handler import PermuteHandler

# Module logger with enhanced configuration
logger = get_logger(__name__)

# Constants for production-ready behavior
MAX_PATTERN_LENGTH = 50000        # Prevent extremely long patterns
MAX_NESTING_DEPTH = 100          # Prevent infinite recursion
MAX_PERMUTE_VARIABLES = 50       # Limit PERMUTE complexity
CACHE_SIZE_LIMIT = 1000          # LRU cache for tokenization results

# Thread-local storage for tokenization state
_tokenizer_state = threading.local()

@contextmanager
def _tokenization_context(pattern: str):
    """Context manager for thread-safe tokenization state."""
    old_state = getattr(_tokenizer_state, 'state', None)
    _tokenizer_state.state = {
        'pattern': pattern,
        'position': 0,
        'depth': 0,
        'errors': []
    }
    try:
        yield _tokenizer_state.state
    finally:
        _tokenizer_state.state = old_state

class PatternTokenType(Enum):
    """Enum representing different types of pattern tokens with SQL:2016 compliance."""
    LITERAL = "LITERAL"                    # Pattern variable (A, B, etc.)
    ALTERNATION = "ALTERNATION"            # | operator
    PERMUTE = "PERMUTE"                    # PERMUTE(...) construct
    GROUP_START = "GROUP_START"            # ( opener
    GROUP_END = "GROUP_END"                # ) closer
    ANCHOR_START = "ANCHOR_START"          # ^ anchor
    ANCHOR_END = "ANCHOR_END"              # $ anchor
    EXCLUSION = "EXCLUSION"                # {- pattern -} exclusion
    EXCLUSION_START = "EXCL_START"         # {- opener
    EXCLUSION_END = "EXCL_END"             # -} closer
    QUANTIFIER = "QUANTIFIER"              # *, +, ?, {n,m}
    SUBSET = "SUBSET"                      # SUBSET definition
    SEQUENCE = "SEQUENCE"                  # Sequence of tokens

class PatternValidationLevel(Enum):
    """Validation levels for pattern parsing."""
    STRICT = "STRICT"        # Full SQL:2016 compliance
    LENIENT = "LENIENT"      # Allow some extensions
    DEBUG = "DEBUG"          # Maximum validation for development

class PatternSyntaxError(Exception):
    """Base class for pattern syntax errors with enhanced context visualization."""
    
    def __init__(self, message: str, position: int, pattern: str, 
                 suggestion: Optional[str] = None, error_code: Optional[str] = None):
        self.message = message
        self.position = position
        self.pattern = pattern
        self.suggestion = suggestion
        self.error_code = error_code
        self.context = self._get_error_context()
        
        full_message = f"{message}\nAt position {position}:\n{self.context}"
        if suggestion:
            full_message += f"\nSuggestion: {suggestion}"
        if error_code:
            full_message += f"\nError Code: {error_code}"
        
        super().__init__(full_message)
    
    def _get_error_context(self) -> str:
        """Get error context with enhanced visualization."""
        if not self.pattern:
            return "Empty pattern"
        
        # Show more context for better debugging
        context_size = min(40, len(self.pattern))
        start = max(0, self.position - context_size // 2)
        end = min(len(self.pattern), start + context_size)
        
        # Adjust start if we're near the end
        if end == len(self.pattern):
            start = max(0, end - context_size)
        
        context = self.pattern[start:end]
        
        # Create pointer with line numbers for long patterns
        pointer_pos = self.position - start
        pointer = " " * pointer_pos + "^"
        
        # Add line numbers if pattern is long
        if len(self.pattern) > 100:
            line_info = f" (char {self.position}/{len(self.pattern)})"
            return f"{context}{line_info}\n{pointer}"
        
        return f"{context}\n{pointer}"

class PermutePatternError(PatternSyntaxError):
    """Error in PERMUTE pattern syntax with specific guidance."""
    
    def __init__(self, message: str, position: int, pattern: str):
        suggestions = {
            "unclosed": "Ensure PERMUTE(...) has matching parentheses",
            "empty": "PERMUTE must contain at least one variable",
            "nested": "Nested PERMUTE patterns require careful syntax",
            "invalid_var": "PERMUTE variables must be valid identifiers"
        }
        
        # Try to determine the specific issue
        suggestion = None
        error_code = None
        
        if "unclosed" in message.lower():
            suggestion = suggestions["unclosed"]
            error_code = "PERMUTE_001"
        elif "empty" in message.lower():
            suggestion = suggestions["empty"]
            error_code = "PERMUTE_002"
        elif "nested" in message.lower():
            suggestion = suggestions["nested"]
            error_code = "PERMUTE_003"
        elif "variable" in message.lower():
            suggestion = suggestions["invalid_var"]
            error_code = "PERMUTE_004"
        
        super().__init__(message, position, pattern, suggestion, error_code)

class QuantifierError(PatternSyntaxError):
    """Error in quantifier syntax with specific guidance."""
    
    def __init__(self, message: str, position: int, pattern: str):
        suggestions = {
            "range": "Use {min,max} format for range quantifiers",
            "negative": "Quantifier bounds must be non-negative",
            "order": "Minimum must be <= maximum in {min,max}",
            "missing": "Quantifier requires a preceding element"
        }
        
        suggestion = None
        error_code = None
        
        if "range" in message.lower() or "format" in message.lower():
            suggestion = suggestions["range"]
            error_code = "QUANT_001"
        elif "negative" in message.lower():
            suggestion = suggestions["negative"]
            error_code = "QUANT_002"
        elif "order" in message.lower() or "minimum" in message.lower():
            suggestion = suggestions["order"]
            error_code = "QUANT_003"
        elif "missing" in message.lower():
            suggestion = suggestions["missing"]
            error_code = "QUANT_004"
        
        super().__init__(message, position, pattern, suggestion, error_code)

class UnbalancedPatternError(PatternSyntaxError):
    """Error for unbalanced pattern elements with enhanced debugging."""
    
    def __init__(self, message: str, position: int, pattern: str):
        suggestions = {
            "parentheses": "Check for matching ( and ) characters",
            "exclusion": "Check for matching {- and -} markers",
            "quotes": "Check for matching quote characters",
            "brackets": "Check for matching [ and ] characters"
        }
        
        suggestion = None
        error_code = None
        
        if "paren" in message.lower():
            suggestion = suggestions["parentheses"]
            error_code = "BALANCE_001"
        elif "exclusion" in message.lower():
            suggestion = suggestions["exclusion"]
            error_code = "BALANCE_002"
        elif "quote" in message.lower():
            suggestion = suggestions["quotes"]
            error_code = "BALANCE_003"
        elif "bracket" in message.lower():
            suggestion = suggestions["brackets"]
            error_code = "BALANCE_004"
        
        super().__init__(message, position, pattern, suggestion, error_code)

@dataclass
class PatternToken:
    """
    Represents a token in a pattern expression with comprehensive metadata.
    
    This production-ready class provides enhanced token representation with:
    - Full quantifier support (greedy, reluctant, possessive)
    - Rich metadata for complex pattern constructs
    - Thread-safe operations with proper validation
    - Performance optimization features
    """
    type: PatternTokenType
    value: str
    quantifier: Optional[str] = None
    greedy: bool = True
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Enhanced fields for production use
    position: int = 0                    # Position in original pattern
    length: int = 0                      # Length in original pattern
    priority: int = 0                    # Priority for conflict resolution
    validation_level: PatternValidationLevel = PatternValidationLevel.STRICT
    
    def __post_init__(self):
        """Initialize metadata and validate token integrity."""
        # Ensure metadata is a proper dictionary
        if self.metadata is None:
            self.metadata = {}
        
        # Initialize type-specific metadata
        if self.type == PatternTokenType.PERMUTE:
            self._initialize_permute_metadata()
        elif self.type == PatternTokenType.QUANTIFIER:
            self._initialize_quantifier_metadata()
        elif self.type == PatternTokenType.ALTERNATION:
            self._initialize_alternation_metadata()
        
        # Validate token consistency
        self._validate_token()
    
    def _initialize_permute_metadata(self) -> None:
        """Initialize PERMUTE-specific metadata."""
        defaults = {
            "variables": [],
            "nested": False,
            "original": "",
            "depth": 0,
            "variable_count": 0,
            "has_alternations": False
        }
        for key, default_value in defaults.items():
            if key not in self.metadata:
                self.metadata[key] = default_value
    
    def _initialize_quantifier_metadata(self) -> None:
        """Initialize quantifier-specific metadata."""
        defaults = {
            "min_repetitions": 1,
            "max_repetitions": 1,
            "is_possessive": False,
            "is_reluctant": False,
            "applies_to": None
        }
        for key, default_value in defaults.items():
            if key not in self.metadata:
                self.metadata[key] = default_value
    
    def _initialize_alternation_metadata(self) -> None:
        """Initialize alternation-specific metadata."""
        defaults = {
            "alternatives": [],
            "priority": 0,
            "is_nested": False,
            "parent_group": None
        }
        for key, default_value in defaults.items():
            if key not in self.metadata:
                self.metadata[key] = default_value
    
    def _validate_token(self) -> None:
        """Validate token consistency and integrity."""
        # Validate value is not empty for most token types
        if self.type not in [PatternTokenType.GROUP_START, PatternTokenType.GROUP_END] and not self.value:
            logger.warning(f"Empty value for token type {self.type}")
        
        # Validate quantifier format if present
        if self.quantifier:
            try:
                parse_quantifier(self.quantifier)
            except ValueError as e:
                logger.warning(f"Invalid quantifier '{self.quantifier}': {e}")
        
        # Validate position and length
        if self.position < 0:
            self.position = 0
        if self.length < 0:
            self.length = len(self.value) if self.value else 0
    
        # Check quantifier target validity
        if self.type == PatternTokenType.QUANTIFIER:
            if not self.quantifier:
                logger.warning(f"Quantifier token {self.value} has no quantifier string")
        
        # Length should be positive
        if self.length < 0:
            logger.warning(f"Token {self.value} has negative length: {self.length}")
    
    def copy_with_modifications(self, **kwargs) -> 'PatternToken':
        """
        Create a copy of this token with specified modifications.
        
        This is useful for token transformation during pattern compilation.
        
        Args:
            **kwargs: Fields to modify in the copy
            
        Returns:
            New PatternToken instance with modifications
        """
        # Start with current values
        new_values = {
            'type': self.type,
            'value': self.value,
            'quantifier': self.quantifier,
            'greedy': self.greedy,
            'metadata': self.metadata.copy(),
            'position': self.position,
            'length': self.length,
            'priority': self.priority,
            'validation_level': self.validation_level
        }
        
        # Apply modifications
        new_values.update(kwargs)
        
        return PatternToken(**new_values)
    
    def to_debug_string(self) -> str:
        """
        Generate a detailed debug representation of this token.
        
        Returns:
            Detailed string representation for debugging
        """
        parts = [f"Token({self.type.value}: '{self.value}'"]
        
        if self.quantifier:
            greedy_str = "greedy" if self.greedy else "reluctant"
            parts.append(f"quantifier='{self.quantifier}' ({greedy_str})")
        
        if self.position > 0 or self.length > 0:
            parts.append(f"pos={self.position}, len={self.length}")
        
        if self.priority != 0:
            parts.append(f"priority={self.priority}")
        
        if self.metadata:
            metadata_summary = []
            for key, value in self.metadata.items():
                if isinstance(value, list) and len(value) > 3:
                    metadata_summary.append(f"{key}=[{len(value)} items]")
                elif isinstance(value, dict) and len(value) > 3:
                    metadata_summary.append(f"{key}={{{len(value)} keys}}")
                else:
                    metadata_summary.append(f"{key}={value}")
            parts.append(f"metadata={{{', '.join(metadata_summary)}}}")
        
        parts.append(")")
        return "".join(parts)
    
    def is_compatible_with(self, other: 'PatternToken') -> bool:
        """
        Check if this token is compatible with another token for composition.
        
        Args:
            other: Another pattern token
            
        Returns:
            True if tokens can be composed together
        """
        if not isinstance(other, PatternToken):
            return False
        
        # Variables can be followed by quantifiers
        if (self.type == PatternTokenType.LITERAL and 
            other.type == PatternTokenType.QUANTIFIER):
            return True
        
        # Group starts/ends must match
        if (self.type == PatternTokenType.GROUP_START and 
            other.type == PatternTokenType.GROUP_END):
            return True
        
        # Check validation level compatibility
        if (self.validation_level == PatternValidationLevel.STRICT and 
            other.validation_level == PatternValidationLevel.DEBUG):
            return False
        
        return True

# Production-ready tokenization functions

def tokenize_pattern(pattern: str, validation_level: PatternValidationLevel = PatternValidationLevel.STRICT) -> List[PatternToken]:
    """
    Tokenize a pattern string into PatternToken objects with comprehensive error handling.
    
    This production-ready function provides:
    - Full support for complex PERMUTE patterns with nesting
    - Advanced quantifier parsing (greedy, reluctant, possessive)
    - Pattern exclusions with proper validation
    - Alternation patterns with priority handling
    - Thread-safe operation with proper context management
    - Comprehensive error reporting with suggestions
    - Performance optimization through intelligent caching
    
    Args:
        pattern: Pattern string to tokenize
        validation_level: Level of validation to apply
        
    Returns:
        List of PatternToken objects representing the pattern
        
    Raises:
        PatternSyntaxError: If pattern has syntax errors
        PermutePatternError: If PERMUTE syntax is invalid
        QuantifierError: If quantifier syntax is invalid
        UnbalancedPatternError: If parentheses/brackets are unbalanced
    """
    if not pattern:
        return []
    
    # Validate pattern length
    if len(pattern) > MAX_PATTERN_LENGTH:
        raise PatternSyntaxError(
            f"Pattern too long: {len(pattern)} characters (max: {MAX_PATTERN_LENGTH})",
            0, pattern, "Break pattern into smaller parts", "PATTERN_001"
        )
    
    # Check pattern cache first (Phase 2 optimization)
    from ..utils.pattern_cache import get_cached_pattern, cache_pattern
    cache_key = f"tokenize:{pattern}:{validation_level.value}"
    
    cached_result = get_cached_pattern(cache_key)
    if cached_result is not None:
        # Extract just the tokenization result (first element)
        return cached_result[0] if isinstance(cached_result, tuple) else cached_result
    
    # Use thread-safe context
    with _tokenization_context(pattern):
        try:
            # Performance timing for large patterns
            with PerformanceTimer("pattern_tokenization") as timer:
                tokens = _tokenize_pattern_internal(pattern, validation_level)
                
                # Cache successful tokenization results
                cache_pattern(cache_key, tokens, None, timer.elapsed)
                
                if len(pattern) > 1000:  # Log performance for large patterns
                    logger.info(f"Tokenized large pattern ({len(pattern)} chars) in {timer.elapsed:.3f}s")
                
                return tokens
                
        except Exception as e:
            if isinstance(e, (PatternSyntaxError, PermutePatternError, QuantifierError, UnbalancedPatternError)):
                raise  # Re-raise known pattern errors
            else:
                # Wrap unexpected errors with context
                raise PatternSyntaxError(
                    f"Unexpected error during tokenization: {str(e)}",
                    0, pattern, "Check pattern syntax", "PATTERN_002"
                ) from e

def _tokenize_pattern_internal(pattern: str, validation_level: PatternValidationLevel) -> List[PatternToken]:
    """Internal tokenization implementation with enhanced error handling."""
    tokens = []
    pos = 0
    
    # Track nesting for validation
    nesting_stack = []
    permute_depth = 0
    
    while pos < len(pattern):
        # Skip whitespace
        while pos < len(pattern) and pattern[pos].isspace():
            pos += 1
        
        if pos >= len(pattern):
            break
        
        # Check for PERMUTE pattern
        if (pos + 7 <= len(pattern) and 
            pattern[pos:pos+7].upper() == 'PERMUTE'):
            token, new_pos = _parse_permute_pattern(pattern, pos, validation_level)
            tokens.append(token)
            pos = new_pos
            continue
        
        # Check for pattern exclusions
        if (pos + 2 <= len(pattern) and 
            pattern[pos:pos+2] == '{-'):
            token, new_pos = _parse_exclusion_pattern(pattern, pos, validation_level)
            tokens.append(token)
            pos = new_pos
            continue
        
        # Check for alternation
        if pattern[pos] == '|':
            token = PatternToken(
                PatternTokenType.ALTERNATION,
                '|',
                position=pos,
                length=1,
                validation_level=validation_level
            )
            tokens.append(token)
            pos += 1
            continue
        
        # Check for groups
        if pattern[pos] == '(':
            nesting_stack.append(('group', pos))
            token = PatternToken(
                PatternTokenType.GROUP_START,
                '(',
                position=pos,
                length=1,
                validation_level=validation_level
            )
            tokens.append(token)
            pos += 1
            continue
        
        if pattern[pos] == ')':
            if not nesting_stack or nesting_stack[-1][0] != 'group':
                raise UnbalancedPatternError(
                    "Unmatched closing parenthesis",
                    pos, pattern
                )
            nesting_stack.pop()
            token = PatternToken(
                PatternTokenType.GROUP_END,
                ')',
                position=pos,
                length=1,
                validation_level=validation_level
            )
            tokens.append(token)
            pos += 1
            continue
        
        # Check for anchor patterns
        if pattern[pos] == '^':
            token = PatternToken(
                PatternTokenType.ANCHOR_START,
                '^',
                position=pos,
                length=1,
                validation_level=validation_level
            )
            tokens.append(token)
            pos += 1
            continue
        
        if pattern[pos] == '$':
            token = PatternToken(
                PatternTokenType.ANCHOR_END,
                '$',
                position=pos,
                length=1,
                validation_level=validation_level
            )
            tokens.append(token)
            pos += 1
            continue
        
        # Check for quantifiers
        if pattern[pos] in '*+?{':
            if not tokens:
                raise QuantifierError(
                    "Quantifier at beginning of pattern with no preceding element",
                    pos, pattern
                )
            
            quantifier, is_greedy, new_pos = parse_quantifier_at(pattern, pos)
            if quantifier:
                # Apply quantifier to the previous token
                prev_token = tokens[-1]
                tokens[-1] = prev_token.copy_with_modifications(
                    quantifier=quantifier,
                    greedy=is_greedy
                )
                pos = new_pos
                continue
        
        # Parse quoted identifiers
        if pattern[pos] == '"':
            quote_start = pos
            pos += 1  # Skip opening quote
            
            # Find closing quote
            while pos < len(pattern) and pattern[pos] != '"':
                if pattern[pos] == '\\' and pos + 1 < len(pattern):
                    pos += 2  # Skip escaped character
                else:
                    pos += 1
            
            if pos >= len(pattern):
                raise PatternSyntaxError(
                    "Unterminated quoted identifier",
                    quote_start, pattern, "Add closing quote", "PATTERN_004"
                )
            
            pos += 1  # Skip closing quote
            var_name = pattern[quote_start:pos]  # Include quotes in the token value
            token = PatternToken(
                PatternTokenType.LITERAL,
                var_name,
                position=quote_start,
                length=pos - quote_start,
                validation_level=validation_level
            )
            tokens.append(token)
            continue
        
        # Parse literal variable/identifier
        var_start = pos
        while (pos < len(pattern) and 
               (pattern[pos].isalnum() or pattern[pos] in '_.')):
            pos += 1
        
        if pos > var_start:
            var_name = pattern[var_start:pos]
            token = PatternToken(
                PatternTokenType.LITERAL,
                var_name,
                position=var_start,
                length=pos - var_start,
                validation_level=validation_level
            )
            tokens.append(token)
            continue
        
        # Handle unknown characters
        if validation_level == PatternValidationLevel.STRICT:
            raise PatternSyntaxError(
                f"Unexpected character '{pattern[pos]}'",
                pos, pattern, "Check pattern syntax", "PATTERN_003"
            )
        else:
            # In lenient mode, skip unknown characters with warning
            logger.warning(f"Skipping unexpected character '{pattern[pos]}' at position {pos}")
            pos += 1
    
    # Validate balanced nesting
    if nesting_stack:
        unmatched_type, unmatched_pos = nesting_stack[-1]
        raise UnbalancedPatternError(
            f"Unmatched {unmatched_type} starting at position {unmatched_pos}",
            unmatched_pos, pattern
        )
    
    # Post-process tokens for optimization and validation
    tokens = _optimize_token_sequence(tokens)
    
    # Validate anchor patterns (SQL:2016 compliance)
    _validate_anchor_patterns(tokens, pattern, validation_level)
    
    return tokens

def _parse_permute_pattern(pattern: str, start_pos: int, 
                          validation_level: PatternValidationLevel) -> Tuple[PatternToken, int]:
    """Parse a PERMUTE pattern with comprehensive validation."""
    pos = start_pos + 7  # Skip "PERMUTE"
    
    # Skip whitespace
    while pos < len(pattern) and pattern[pos].isspace():
        pos += 1
    
    if pos >= len(pattern) or pattern[pos] != '(':
        raise PermutePatternError(
            "Expected '(' after PERMUTE keyword",
            pos, pattern
        )
    
    pos += 1  # Skip opening parenthesis
    variables, pos = process_permute_variables(pattern, pos)
    
    if pos >= len(pattern) or pattern[pos] != ')':
        raise PermutePatternError(
            "Expected ')' to close PERMUTE expression",
            pos, pattern
        )
    
    pos += 1  # Skip closing parenthesis
    
    # Validate PERMUTE constraints
    if len(variables) == 0:
        raise PermutePatternError(
            "PERMUTE expression cannot be empty",
            start_pos, pattern
        )
    
    if len(variables) > MAX_PERMUTE_VARIABLES:
        raise PermutePatternError(
            f"Too many variables in PERMUTE: {len(variables)} (max: {MAX_PERMUTE_VARIABLES})",
            start_pos, pattern
        )
    
    # Create PERMUTE token with rich metadata
    permute_text = pattern[start_pos:pos]
    token = PatternToken(
        PatternTokenType.PERMUTE,
        permute_text,
        position=start_pos,
        length=pos - start_pos,
        validation_level=validation_level,
        metadata={
            "variables": variables,
            "variable_count": len(variables),
            "original": permute_text,
            "has_nested": any(isinstance(v, PatternToken) and v.type == PatternTokenType.PERMUTE 
                             for v in variables),
            "has_alternations": any(isinstance(v, PatternToken) and v.type == PatternTokenType.ALTERNATION 
                                   for v in variables)
        }
    )
    
    return token, pos

def _parse_exclusion_pattern(pattern: str, start_pos: int, 
                           validation_level: PatternValidationLevel) -> Tuple[PatternToken, int]:
    """Parse a pattern exclusion {- ... -} with support for nested exclusions."""
    pos = start_pos + 2  # Skip "{-"
    exclusion_start = pos
    
    # Track nesting level for proper handling of nested exclusions
    nesting_level = 0
    
    # Find the closing "-}" accounting for nesting
    while pos < len(pattern) - 1:
        if pattern[pos:pos+2] == '{-':
            nesting_level += 1
            pos += 2
            continue
        elif pattern[pos:pos+2] == '-}':
            if nesting_level == 0:
                # This is our closing bracket
                break
            else:
                nesting_level -= 1
                pos += 2
                continue
        pos += 1
    
    if pos >= len(pattern) - 1:
        raise UnbalancedPatternError(
            "Unmatched exclusion pattern: missing '-}'",
            start_pos, pattern
        )
    
    exclusion_content = pattern[exclusion_start:pos].strip()
    pos += 2  # Skip "-}"
    
    if not exclusion_content:
        raise PatternSyntaxError(
            "Empty exclusion pattern",
            start_pos, pattern, "Add variables to exclude", "EXCLUSION_001"
        )
    
    # For nested exclusions, treat the whole content as a single complex pattern
    # rather than trying to split by commas
    if '{-' in exclusion_content:
        # This is a complex nested exclusion - store as single pattern
        excluded_vars = [exclusion_content.strip()]
    else:
        # Simple exclusion - split by commas for multiple variables
        excluded_vars = [var.strip() for var in exclusion_content.split(',') if var.strip()]
    
    token = PatternToken(
        PatternTokenType.EXCLUSION,
        pattern[start_pos:pos],
        position=start_pos,
        length=pos - start_pos,
        validation_level=validation_level,
        metadata={
            "excluded_variables": excluded_vars,
            "original": pattern[start_pos:pos],
            "is_nested": '{-' in exclusion_content
        }
    )
    
    return token, pos

def _optimize_token_sequence(tokens: List[PatternToken]) -> List[PatternToken]:
    """
    Optimize a sequence of tokens for better performance.
    
    This function applies various optimizations:
    - Merge adjacent literal tokens when beneficial
    - Simplify redundant quantifiers
    - Optimize alternation patterns
    
    Args:
        tokens: List of tokens to optimize
        
    Returns:
        Optimized list of tokens
    """
    if len(tokens) <= 1:
        return tokens
    
    optimized = []
    i = 0
    
    while i < len(tokens):
        current = tokens[i]
        
        # Check if we can merge with the next token
        if (i + 1 < len(tokens) and 
            current.type == PatternTokenType.LITERAL and 
            tokens[i + 1].type == PatternTokenType.LITERAL and 
            not current.quantifier and not tokens[i + 1].quantifier):
            
            # Don't merge tokens that look like pattern variables
            # Pattern variables should remain separate for proper matching semantics
            # Pattern variables can be:
            # 1. Single letters (A, B, C)
            # 2. Capitalized words (UP, DOWN) 
            # 3. Valid identifiers with underscores (limit_50, decrease_10, etc.)
            def is_pattern_variable(value):
                # Single letter variable
                if len(value) == 1 and value.isalpha():
                    return True
                # All uppercase word
                if value.isupper() and value.isalpha():
                    return True
                # Valid identifier (letters, digits, underscores, starts with letter)
                if re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', value):
                    return True
                return False
            
            is_current_var = is_pattern_variable(current.value)
            is_next_var = is_pattern_variable(tokens[i + 1].value)
            
            # NEVER merge pattern variables - they must remain separate for proper matching
            if is_current_var or is_next_var:
                # Don't merge - keep as separate tokens
                optimized.append(current)
                i += 1
            else:
                # Merge adjacent literals only if they don't look like pattern variables
                merged_value = current.value + tokens[i + 1].value
                merged_token = current.copy_with_modifications(
                    value=merged_value,
                    length=current.length + tokens[i + 1].length,
                    metadata={"merged_from": [current.value, tokens[i + 1].value]}
                )
                optimized.append(merged_token)
                i += 2  # Skip both tokens
        else:
            optimized.append(current)
            i += 1
    
    return optimized

# Validation functions

def validate_pattern_syntax(pattern: str, 
                          validation_level: PatternValidationLevel = PatternValidationLevel.STRICT) -> bool:
    """
    Validate pattern syntax without full tokenization.
    
    This function performs quick syntax validation for performance-critical scenarios.
    
    Args:
        pattern: Pattern to validate
        validation_level: Validation strictness level
        
    Returns:
        True if pattern syntax is valid
        
    Raises:
        PatternSyntaxError: If syntax is invalid and validation_level is STRICT
    """
    try:
        tokens = tokenize_pattern(pattern, validation_level)
        return True
    except (PatternSyntaxError, PermutePatternError, QuantifierError, UnbalancedPatternError):
        if validation_level == PatternValidationLevel.STRICT:
            raise
        return False

def get_pattern_complexity(pattern: str) -> Dict[str, Any]:
    """
    Analyze pattern complexity for performance estimation.
    
    Args:
        pattern: Pattern to analyze
        
    Returns:
        Dictionary with complexity metrics
    """
    try:
        tokens = tokenize_pattern(pattern, PatternValidationLevel.LENIENT)
        
        metrics = {
            "token_count": len(tokens),
            "permute_count": sum(1 for t in tokens if t.type == PatternTokenType.PERMUTE),
            "alternation_count": sum(1 for t in tokens if t.type == PatternTokenType.ALTERNATION),
            "quantifier_count": sum(1 for t in tokens if t.quantifier),
            "exclusion_count": sum(1 for t in tokens if t.type == PatternTokenType.EXCLUSION),
            "max_nesting_depth": _calculate_max_nesting_depth(tokens),
            "estimated_complexity": "LOW"
        }
        
        # Estimate complexity level
        complexity_score = (
            metrics["permute_count"] * 3 +
            metrics["alternation_count"] * 2 +
            metrics["quantifier_count"] * 1 +
            metrics["exclusion_count"] * 2 +
            metrics["max_nesting_depth"] * 2
        )
        
        if complexity_score < 5:
            metrics["estimated_complexity"] = "LOW"
        elif complexity_score < 15:
            metrics["estimated_complexity"] = "MEDIUM"
        else:
            metrics["estimated_complexity"] = "HIGH"
        
        return metrics
        
    except Exception as e:
        return {
            "error": str(e),
            "estimated_complexity": "UNKNOWN"
        }

def _calculate_max_nesting_depth(tokens: List[PatternToken]) -> int:
    """Calculate maximum nesting depth in token sequence."""
    max_depth = 0
    current_depth = 0
    
    for token in tokens:
        if token.type == PatternTokenType.GROUP_START:
            current_depth += 1
            max_depth = max(max_depth, current_depth)
        elif token.type == PatternTokenType.GROUP_END:
            current_depth = max(0, current_depth - 1)
        elif token.type == PatternTokenType.PERMUTE:
            # PERMUTE patterns add to nesting depth
            permute_depth = token.metadata.get("depth", 1)
            max_depth = max(max_depth, current_depth + permute_depth)
    
    return max_depth

# Missing utility functions that are referenced

def parse_quantifier(quantifier: str) -> Tuple[int, int, bool]:
    """
    Parse a quantifier string into min, max, and greedy flag.
    
    Args:
        quantifier: Quantifier string like '*', '+', '?', '{n,m}'
        
    Returns:
        Tuple of (min_repetitions, max_repetitions, is_greedy)
    """
    if not quantifier:
        return (1, 1, True)
    
    # Remove non-greedy marker
    is_greedy = True
    if quantifier.endswith('?'):
        is_greedy = False
        quantifier = quantifier[:-1]
    
    if quantifier == '*':
        return (0, float('inf'), is_greedy)
    elif quantifier == '+':
        return (1, float('inf'), is_greedy)
    elif quantifier == '?':
        return (0, 1, is_greedy)
    elif quantifier.startswith('{') and quantifier.endswith('}'):
        content = quantifier[1:-1]
        if ',' in content:
            parts = content.split(',')
            min_rep = int(parts[0]) if parts[0] else 0
            max_rep = int(parts[1]) if parts[1] else float('inf')
        else:
            min_rep = max_rep = int(content)
        return (min_rep, max_rep, is_greedy)
    
    return (1, 1, True)

def parse_quantifier_at(pattern: str, start_pos: int) -> Tuple[Optional[str], bool, int]:
    """
    Parse quantifier at given position in the pattern.
    
    Args:
        pattern: The full pattern string
        start_pos: Position to start parsing from
        
    Returns:
        Tuple of (quantifier, is_greedy, new_position)
        
    Raises:
        QuantifierError: If the quantifier format is invalid
    """
    if start_pos >= len(pattern):
        return None, True, start_pos
        
    char = pattern[start_pos]
    if char not in "*+?{":
        return None, True, start_pos
        
    if char in "*+?":
        pos = start_pos + 1
        is_greedy = True
        if pos < len(pattern) and pattern[pos] == "?":
            is_greedy = False
            pos += 1
        return char, is_greedy, pos
        
    # Handle {n,m} quantifiers
    pos = start_pos
    brace_depth = 0
    quant_start = pos
    
    while pos < len(pattern):
        if pattern[pos] == "{":
            brace_depth += 1
        elif pattern[pos] == "}":
            brace_depth -= 1
            if brace_depth == 0:
                break
        pos += 1
        
    if brace_depth > 0 or pos >= len(pattern):
        raise QuantifierError("Unclosed brace in quantifier", start_pos, pattern)
        
    pos += 1  # Move past closing brace
    quantifier = pattern[quant_start:pos]
    
    # Validate quantifier format inline
    if quantifier.startswith('{') and quantifier.endswith('}'):
        content = quantifier[1:-1]
        
        # Check for negative numbers first (these should be rejected)
        if '-' in content:
            raise QuantifierError(f"Negative values not allowed in quantifier: {quantifier}", start_pos, pattern)
        
        # Check basic format - only allow positive digits and commas
        if not re.match(r'^\d+(?:,\d*)?$', content):
            raise QuantifierError(f"Invalid quantifier format: {quantifier}", start_pos, pattern)
            
        # Parse and validate bounds
        parts = content.split(',')
        try:
            if len(parts) == 1:
                int(parts[0])  # Validate it's a number
            elif len(parts) == 2:
                min_val = int(parts[0]) if parts[0] else 0
                max_val = int(parts[1]) if parts[1] else None
                
                if max_val is not None and min_val > max_val:
                    raise QuantifierError(
                        f"Invalid quantifier range: minimum ({min_val}) greater than maximum ({max_val})",
                        start_pos, pattern
                    )
            else:
                raise QuantifierError(f"Too many commas in quantifier: {quantifier}", start_pos, pattern)
        except ValueError:
            raise QuantifierError(f"Non-numeric values in quantifier: {quantifier}", start_pos, pattern)
    
    # Check for non-greedy marker
    is_greedy = True
    if pos < len(pattern) and pattern[pos] == "?":
        is_greedy = False
        pos += 1
        
    return quantifier, is_greedy, pos

def process_permute_variables(pattern: str, start_pos: int) -> Tuple[List[Union[str, PatternToken]], int]:
    """
    Process variables in a PERMUTE expression, handling alternations and nested patterns.
    
    Args:
        pattern: The full pattern string
        start_pos: Position to start parsing from (after opening parenthesis)
        
    Returns:
        Tuple of (variables_list, new_position)
        
    Raises:
        PermutePatternError: If PERMUTE syntax is invalid
    """
    variables = []
    pos = start_pos
    current_var = ""
    paren_depth = 0
    
    while pos < len(pattern):
        char = pattern[pos]
        
        if char == '(':
            paren_depth += 1
            current_var += char
        elif char == ')':
            if paren_depth > 0:
                paren_depth -= 1
                current_var += char
            else:
                # This is the closing parenthesis of PERMUTE
                break
        elif char == ',' and paren_depth == 0:
            # End of current variable/pattern (only at top level)
            if current_var.strip():
                processed_var = _process_permute_variable(current_var.strip())
                variables.append(processed_var)
                current_var = ""
        else:
            current_var += char
        
        pos += 1
    
    # Add the last variable if any
    if current_var.strip():
        processed_var = _process_permute_variable(current_var.strip())
        variables.append(processed_var)
    
    return variables, pos


def _process_permute_variable(var_text: str) -> Union[str, PatternToken]:
    """
    Process a single PERMUTE variable which might be:
    - A simple variable: "A"
    - An alternation: "A | B" 
    - A nested PERMUTE: "PERMUTE(X, Y)"
    
    Args:
        var_text: The variable text to process
        
    Returns:
        Either a string (simple variable) or PatternToken (complex pattern)
    """
    var_text = var_text.strip()
    
    # Check for alternation pattern (contains | not inside parentheses)
    if '|' in var_text and not var_text.upper().startswith('PERMUTE'):
        # This is an alternation pattern
        alternatives = []
        current_alt = ""
        paren_depth = 0
        
        for char in var_text:
            if char == '(':
                paren_depth += 1
                current_alt += char
            elif char == ')':
                paren_depth -= 1
                current_alt += char
            elif char == '|' and paren_depth == 0:
                # Top-level alternation separator
                if current_alt.strip():
                    alternatives.append(current_alt.strip())
                    current_alt = ""
            else:
                current_alt += char
        
        # Add the last alternative
        if current_alt.strip():
            alternatives.append(current_alt.strip())
        
        # Create alternation token
        if len(alternatives) > 1:
            return PatternToken(
                PatternTokenType.ALTERNATION,
                var_text,
                metadata={
                    "alternatives": alternatives,
                    "is_permute_argument": True,
                    "original": var_text
                }
            )
    
    # Check for nested PERMUTE
    elif var_text.upper().startswith('PERMUTE'):
        # Parse the nested PERMUTE to extract its variables
        try:
            # Find the parentheses content
            start_paren = var_text.find('(')
            end_paren = var_text.rfind(')')
            
            if start_paren != -1 and end_paren != -1 and end_paren > start_paren:
                # Extract variables from inside the parentheses
                inner_content = var_text[start_paren + 1:end_paren]
                nested_variables, _ = process_permute_variables(inner_content, 0)
                
                # Debug logging for nested PERMUTE processing
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"Processing nested PERMUTE: {var_text}")
                    logger.debug(f"Inner content: {inner_content}")
                    logger.debug(f"Extracted nested variables: {nested_variables}")
                
                return PatternToken(
                    PatternTokenType.PERMUTE,
                    var_text,
                    metadata={
                        "nested": True, 
                        "original": var_text,
                        "variables": nested_variables,
                        "variable_count": len(nested_variables),
                        "depth": 1,
                        "has_alternations": any(isinstance(v, PatternToken) and v.type == PatternTokenType.ALTERNATION 
                                              for v in nested_variables)
                    }
                )
            else:
                # Malformed PERMUTE
                return PatternToken(
                    PatternTokenType.PERMUTE,
                    var_text,
                    metadata={
                        "nested": True, 
                        "original": var_text,
                        "variables": [],
                        "variable_count": 0,
                        "depth": 0,
                        "has_alternations": False
                    }
                )
        except Exception as e:
            # If parsing fails, create empty nested PERMUTE
            return PatternToken(
                PatternTokenType.PERMUTE,
                var_text,
                metadata={
                    "nested": True, 
                    "original": var_text,
                    "variables": [],
                    "variable_count": 0,
                    "depth": 0,
                    "has_alternations": False
                }
            )
    
    # Simple variable
    return var_text

def _validate_anchor_patterns(tokens: List[PatternToken], pattern: str, 
                            validation_level: PatternValidationLevel) -> None:
    """
    Validate anchor patterns according to SQL:2016 standards.
    
    Rules:
    1. ^ (start anchor) can only appear at the beginning of a pattern or after |
    2. $ (end anchor) can only appear at the end of a pattern or before |  
    3. Anchors cannot be quantified
    4. Conflicting anchors (^...$) in the same pattern are invalid
    5. Anchors inside groups have specific semantics
    
    Args:
        tokens: List of parsed tokens
        pattern: Original pattern string for error context
        validation_level: Validation strictness level
        
    Raises:
        PatternSyntaxError: If anchor patterns violate SQL:2016 rules
    """
    if not tokens:
        return
    
    # Track anchor positions and validate placement
    start_anchors = []
    end_anchors = []
    
    for i, token in enumerate(tokens):
        if token.type == PatternTokenType.ANCHOR_START:
            start_anchors.append((i, token))
            
            # Validate ^ anchor position
            valid_position = (
                i == 0 or  # Beginning of pattern
                (i > 0 and tokens[i-1].type == PatternTokenType.ALTERNATION) or  # After |
                (i > 0 and tokens[i-1].type == PatternTokenType.GROUP_START)  # After (
            )
            
            if not valid_position and validation_level == PatternValidationLevel.STRICT:
                # Mark as semantically invalid but parseable (will result in no matches)
                token.metadata['semantic_error'] = 'invalid_anchor_position'
                token.metadata['error_message'] = "Start anchor '^' can only appear at pattern beginning, after '|', or after '('"
                logger.warning(f"Invalid anchor position at {token.position}: {token.metadata['error_message']}")
                # Don't raise exception - let it pass but mark for no-match behavior
                
            # Check if anchor is quantified (invalid)
            if token.quantifier:
                raise PatternSyntaxError(
                    "Anchor patterns cannot be quantified",
                    token.position, pattern,
                    "Remove quantifier from anchor", "ANCHOR_002"
                )
        
        elif token.type == PatternTokenType.ANCHOR_END:
            end_anchors.append((i, token))
            
            # Validate $ anchor position
            valid_position = (
                i == len(tokens) - 1 or  # End of pattern
                (i < len(tokens) - 1 and tokens[i+1].type == PatternTokenType.ALTERNATION) or  # Before |
                (i < len(tokens) - 1 and tokens[i+1].type == PatternTokenType.GROUP_END)  # Before )
            )
            
            if not valid_position and validation_level == PatternValidationLevel.STRICT:
                # Mark as semantically invalid but parseable (will result in no matches)
                token.metadata['semantic_error'] = 'invalid_anchor_position'
                token.metadata['error_message'] = "End anchor '$' can only appear at pattern end, before '|', or before ')'"
                logger.warning(f"Invalid anchor position at {token.position}: {token.metadata['error_message']}")
                # Don't raise exception - let it pass but mark for no-match behavior
                
            # Check if anchor is quantified (invalid)
            if token.quantifier:
                raise PatternSyntaxError(
                    "Anchor patterns cannot be quantified",
                    token.position, pattern,
                    "Remove quantifier from anchor", "ANCHOR_004"
                )
    
    # Check for conflicting anchors in the same alternation branch
    # This is more complex - we need to track alternation boundaries
    alternation_boundaries = [0]  # Start of pattern
    for i, token in enumerate(tokens):
        if token.type == PatternTokenType.ALTERNATION:
            alternation_boundaries.append(i)
    alternation_boundaries.append(len(tokens))  # End of pattern
    
    # Check each alternation branch for conflicting anchors
    for branch_start, branch_end in zip(alternation_boundaries[:-1], alternation_boundaries[1:]):
        branch_start_anchors = []
        branch_end_anchors = []
        
        for i in range(branch_start, branch_end):
            if i < len(tokens):
                if tokens[i].type == PatternTokenType.ANCHOR_START:
                    branch_start_anchors.append(tokens[i])
                elif tokens[i].type == PatternTokenType.ANCHOR_END:
                    branch_end_anchors.append(tokens[i])
        
        # Check for conflicting anchors in the same branch
        if len(branch_start_anchors) > 1:
            if validation_level == PatternValidationLevel.STRICT:
                # Mark as semantically invalid but parseable
                for anchor in branch_start_anchors[1:]:
                    anchor.metadata['semantic_error'] = 'duplicate_start_anchor'
                    anchor.metadata['error_message'] = "Multiple start anchors '^' in the same pattern branch"
                logger.warning(f"Multiple start anchors in pattern at position {branch_start_anchors[1].position}")
        
        if len(branch_end_anchors) > 1:
            if validation_level == PatternValidationLevel.STRICT:
                # Mark as semantically invalid but parseable
                for anchor in branch_end_anchors[1:]:
                    anchor.metadata['semantic_error'] = 'duplicate_end_anchor' 
                    anchor.metadata['error_message'] = "Multiple end anchors '$' in the same pattern branch"
                logger.warning(f"Multiple end anchors in pattern at position {branch_end_anchors[1].position}")
        
        # Check for impossible pattern: ^...$ in same simple branch
        # This is only invalid if there are no other pattern elements between anchors
        if (branch_start_anchors and branch_end_anchors and 
            validation_level == PatternValidationLevel.STRICT):
            
            # Count non-anchor tokens between anchors
            non_anchor_count = 0
            for i in range(branch_start, branch_end):
                if (i < len(tokens) and 
                    tokens[i].type not in [PatternTokenType.ANCHOR_START, PatternTokenType.ANCHOR_END]):
                    non_anchor_count += 1
            
            # If no content between anchors, it's an empty match which may be invalid
            if non_anchor_count == 0:
                logger.warning(f"Pattern '^$' creates empty match at position {branch_start_anchors[0].position}")

# Export the main tokenization function
__all__ = [
    'tokenize_pattern',
    'PatternToken',
    'PatternTokenType',
    'PatternSyntaxError',
    'PermutePatternError',
    'QuantifierError',
    'UnbalancedPatternError',
    'PermuteHandler',
    'validate_pattern_syntax',
    'get_pattern_complexity'
]