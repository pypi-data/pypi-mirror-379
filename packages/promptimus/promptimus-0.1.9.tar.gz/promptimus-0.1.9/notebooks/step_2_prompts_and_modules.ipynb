{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a289d5c2-0cf1-447f-b58d-3cc8021c1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6a09b-9232-4afc-bf5b-ef11fab5c2ca",
   "metadata": {},
   "source": [
    "# Prompts & modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cc2aad-f521-49dd-89a6-8b8b40659b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptimus as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da56c8f9-aecf-4537-9cc4-d3222950aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a provider\n",
    "llm = pm.llms.OpenAILike(\n",
    "    model_name=\"gemma3:4b\", base_url=\"http://lilan:11434/v1\", api_key=\"DUMMY\"\n",
    ")\n",
    "embedder = pm.embedders.OpenAILikeEmbedder(\n",
    "    model_name=\"mxbai-embed-large\", base_url=\"http://lilan:11434/v1\", api_key=\"DUMMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccca2e0-e99a-455c-91a2-9029087ac817",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "A `Prompt` encapsulates the system prompt and `Provider`, allowing to call LLM with pre-defined behavior, constraints, and response style. \n",
    "Core Functionality\n",
    "\n",
    "-  Encapsulates the system prompt, enforcing predefined behavior.\n",
    "-  Requires an LLM provider to execute and generate responses.\n",
    "-  Processes message sequences asynchronously.\n",
    "-  Preferred to be embedded in a Module for persistence and configuration.\n",
    "\n",
    "A Prompt is the primary mechanism for conditioning model output, by desing it's similar to a pytorch Parameter - https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f12419-173b-412b-a3f8-b7fb5395372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a prompt\n",
    "prompt = pm.Prompt(\"You are an AI assitant with name Henry\").with_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3f6d8e-9a3a-4717-808d-be387750ac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'My name is Henry. It‚Äôs nice to meet you! üòä \\n\\nHow can I help you today?'\u001b[0m,\n",
       "    \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mtool_call_id\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.forward(\n",
    "    [\n",
    "        pm.Message(\n",
    "            role=pm.MessageRole.USER,\n",
    "            content=\"What is your name?\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366a1e4-873b-44e9-b896-47507b7b5e15",
   "metadata": {},
   "source": [
    "## **Modules**  \n",
    "\n",
    "A `Module` serves as a container for integrating multiple components, including `Prompts`, other `Modules`, and **state management** or **additional logic**. It encapsulates logic for handling inputs and outputs, organizing them into reusable and configurable components, for more complex workflows.\n",
    "\n",
    "Within a `Module`, submodules and prompts can be defined, and each submodule is configured with the same `LLMProvider` as the parent module, ensuring consistency across the module's components.\n",
    "\n",
    "Modules also support serialization, to store and load the content of a `Prompt`. The idea is to separate `code` logic from `text` prompts.  \n",
    "\n",
    "A `Module` mimics the design of PyTorch's `nn.Module` ([PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)), serving as an abstraction for defining, organizing, and managing components. Like `nn.Module`, it provides a convenient interface for model components, ensuring modularity, reusability, and extensibility, as well as supporting the management of submodules and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa87a8f-6343-4ad4-8fa2-54f113766ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple module with memory\n",
    "\n",
    "\n",
    "class AssistantWithMemory(pm.Module):\n",
    "    \"\"\"Simple module with memory\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # call init just like in pytorch\n",
    "        super().__init__()\n",
    "\n",
    "        self.chat = pm.Prompt(\"Act as an assistant\")\n",
    "        self.memory = []\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        \"\"\"Implement the async forward function with custom logic.\"\"\"\n",
    "        self.memory.append(pm.Message(role=pm.MessageRole.USER, content=question))\n",
    "        response = await self.chat.forward(self.memory)\n",
    "        self.memory.append(response)\n",
    "        return response.content\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d40afc-764a-48dd-a786-2e98f377ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object and set provider to all prompts\n",
    "assistant = AssistantWithMemory().with_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c664021-1778-413e-9194-54091acc9664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Hello Ailadin! It‚Äôs lovely to meet you. What can I do for you today? üòä \\n\\nDo you want to:\\n\\n*   Tell me about yourself?\\n*   Ask me a question?\\n*   Play a game?\\n*   Just chat?'\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# talk to your assistant\n",
    "await assistant.forward(\"Hi my name is ailadin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8a22c6-9eec-43ca-8c89-5ad2216e31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Your name is Ailadin! üòä I just confirmed it with you. \\n\\nIs there anything else you‚Äôd like to tell me about yourself, or would you like to ask me something?'\u001b[0m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62fdfc69-a795-45c1-8622-f26d46ac83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[chat]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can store and load prompts\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb231e8a-f766-4539-9d16-dc13f6410eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[chat]\n",
      "prompt = \"\"\"\n",
      "Act as an pirate assistant\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = assistant.load_dict(\n",
    "    {\n",
    "        \"params\": {},\n",
    "        \"submodules\": {\n",
    "            \"chat\": {\n",
    "                \"params\": {\"prompt\": \"Act as an pirate assistant\", \"role\": \"system\"},\n",
    "                \"submodules\": {},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142b68a2-7779-454a-980b-ebe34e01de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Shiver me timbers, Ailadin! That's a fine question! \\n\\nNo, it isn‚Äôt quite right to say ‚Äúships swim.‚Äù Ships don‚Äôt *swim* like fish. They float on the water, thanks to their construction and the way they displace the water. \\n\\nWe say they ‚Äúsail‚Äù or ‚Äúdrift‚Äù on the water. \\n\\nA clever question, that! You've got a good eye for details, Ailadin.  Do you want to tell me why you asked?\"\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"Is it correct to say thay ships swim?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14e9e57-12ee-4780-8b3c-8a806d016f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a module with a submodule\n",
    "\n",
    "\n",
    "class PrimitiveRagRetriever(pm.Module):\n",
    "    def __init__(self, top_k: int = 3, similarity_thr: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.top_k = pm.Parameter(top_k)\n",
    "        self.similarity_thr = pm.Parameter(similarity_thr)\n",
    "\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "\n",
    "    async def set_documents(self, documents: list[str]):\n",
    "        self.documents = documents\n",
    "        self.embeddings = await self.embedder.aembed_batch(documents)\n",
    "\n",
    "    async def forward(self, query: str) -> list[str]:\n",
    "        q_embedding = await self.embedder.aembed(query)\n",
    "        print(\"Query:\", query)\n",
    "        similarities = [\n",
    "            pm.embedders.ops.cosine(e, q_embedding) for e in self.embeddings\n",
    "        ]\n",
    "        print(\"Similarities:\", similarities)\n",
    "        argsorted = sorted(range(len(similarities)), key=lambda x: -similarities[x])\n",
    "\n",
    "        return [\n",
    "            self.documents[idx]\n",
    "            for idx in argsorted[: self.top_k.value]\n",
    "            if similarities[idx] > self.similarity_thr.value\n",
    "        ]\n",
    "\n",
    "\n",
    "class PrimitiveRagQA(pm.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        top_k: int = 3,\n",
    "        similarity_thr: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.retriver = PrimitiveRagRetriever(top_k, similarity_thr)\n",
    "        self.query_generator = pm.Prompt(\n",
    "            \"Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\",\n",
    "            role=pm.MessageRole.USER,\n",
    "        )\n",
    "        self.responder = pm.Prompt(\n",
    "            \"Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\"\n",
    "        )\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        \"\"\"Implement the async forward function with custom logic.\"\"\"\n",
    "        query = await self.query_generator.forward(question=question)\n",
    "        context = await self.retriver.forward(query.content)\n",
    "\n",
    "        print(\"Context:\", context)\n",
    "\n",
    "        response = await self.responder.forward(\n",
    "            [\n",
    "                pm.Message(\n",
    "                    role=pm.MessageRole.USER,\n",
    "                    content=question,\n",
    "                ),\n",
    "                pm.Message(\n",
    "                    role=pm.MessageRole.USER,\n",
    "                    content=\"Obervation:\\n\" + \"\\n\".join(context),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1d810e3-3247-4c09-bfce-320ed6214838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[retriver]\n",
      "top_k = 1\n",
      "similarity_thr = 0.5\n",
      "\n",
      "[query_generator]\n",
      "prompt = \"\"\"\n",
      "Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "user\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[responder]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa = PrimitiveRagQA(top_k=1).with_llm(llm).with_embedder(embedder)\n",
    "await qa.retriver.set_documents(\n",
    "    [\n",
    "        \"Glumbor is the capital city of the fictional country Nandor.\",\n",
    "        \"Nandor's main export is glowberries, a rare fruit used in luxury cosmetics.\",\n",
    "        \"In Nandor, the Festival of Lights is celebrated every March 3rd.\",\n",
    "    ]\n",
    ")\n",
    "print(qa.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6316fd77-a68c-432c-b685-9446f2412aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When was [Holiday Name] celebrated in Nandor?\n",
      "\n",
      "Similarities: [0.6450673111903641, 0.5861606444083303, 0.7977003210598459]\n",
      "Context: ['In Nandor, the Festival of Lights is celebrated every March 3rd.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Okay! The Festival of Lights in Nandor is celebrated on March 3rd. Do you have any other questions about it, or would you like to explore something else?'\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await qa.forward(\"Hi, im interested in date of a specific holiday in Nandor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b4f947-b179-4dfd-88ef-bdc66bc0fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is the capital?\n",
      "\n",
      "Similarities: [0.5496870513796827, 0.3114991881178381, 0.3673076383558104]\n",
      "Context: ['Glumbor is the capital city of the fictional country Nandor.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'The capital of Nandor is Glumbor.'\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await qa.forward(\"can you help me remember its capital?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907a719-86a3-4e2d-b277-925de1c17893",
   "metadata": {},
   "source": [
    "### Loading & Saving\n",
    "\n",
    "Modules can be saved and loaded from TOML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f5d1fb-9b29-489e-bafe-bccf192d687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.save(\"assets/step_2_qa.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73248033-6fde-48c9-9225-7398abe8a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[retriver]\n",
      "top_k = 1\n",
      "similarity_thr = 0.5\n",
      "\n",
      "[query_generator]\n",
      "prompt = \"\"\"\n",
      "Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "user\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[responder]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat assets/step_2_qa.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218c2bf8-2557-4359-8421-7db94f7fc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = qa.load(\"assets/step_2_qa.toml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptimus",
   "language": "python",
   "name": "promptimus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
