Metadata-Version: 2.4
Name: greeum
Version: 3.1.1
Summary: Context-Dependent Neural Memory System for LLMs with GraphIndex beam search, STM/LTM consolidation, and semantic tagging
Author-email: DryRainEnt <kachirjsdn@gmail.com>
License-Expression: MIT
Keywords: memory,LLM,RAG
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: rich>=13.4.0
Requires-Dist: click>=8.1.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: anyio>=4.5
Requires-Dist: requests>=2.25.0
Requires-Dist: flask>=2.0.0
Requires-Dist: flask-restx>=0.5.1
Requires-Dist: flask-cors>=3.0.10
Requires-Dist: python-dotenv>=0.19.0
Requires-Dist: python-dateutil>=2.8.2
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: fastapi>=0.110.0
Requires-Dist: uvicorn[standard]>=0.22.0
Provides-Extra: test
Requires-Dist: pytest>=7.0; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Requires-Dist: responses>=0.23.0; extra == "test"
Provides-Extra: dev
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Provides-Extra: mcp-http
Requires-Dist: fastapi>=0.110.0; extra == "mcp-http"
Requires-Dist: uvicorn[standard]>=0.22.0; extra == "mcp-http"
Provides-Extra: full
Requires-Dist: sentence-transformers>=2.2.0; extra == "full"
Requires-Dist: openai>=0.27.0; extra == "full"
Requires-Dist: faiss-cpu>=1.7.4; extra == "full"
Requires-Dist: keybert>=0.7.0; extra == "full"
Requires-Dist: scikit-learn>=1.3.0; extra == "full"
Requires-Dist: transformers>=4.40.0; extra == "full"
Requires-Dist: spacy>=3.5.0; extra == "full"
Dynamic: license-file

# Greeum

[![PyPI version](https://badge.fury.io/py/greeum.svg)](https://badge.fury.io/py/greeum)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> Context-aware memory for AI workflows. Keep long-running conversations and project notes persistent across tools.

<p align="center">
  <a href="README.md"><strong>English</strong></a> ¬∑ <a href="docs/README_ko.md">ÌïúÍµ≠Ïñ¥</a>
</p>

---

## 1. Installation & Setup

> **First run checklist**
> 1. Install the package (pipx or pip)
> 2. Run `greeum setup --start-worker` to create the data directory and launch the worker
> 3. Connect your MCP client (Codex, ClaudeCode, Cursor, ‚Ä¶)

üëâ **Need the ultra-short version?** See [`docs/QUICKSTART.md`](docs/QUICKSTART.md) for ‚ÄúÏÑ§Ïπò ‚Üí ÏÖãÏóÖ ‚Üí Ïó∞Îèô‚Äù Ìïú ÌéòÏù¥ÏßÄ ÏöîÏïΩ.

```bash
# Recommended (isolated) install
pipx install --pip-args "--pre" greeum

# or standard pip
pip install --upgrade "greeum"

# initialise data directory, choose where memories live
greeum setup --start-worker
```

### Optional: enable semantic embeddings
```bash
pip install sentence-transformers          # once per machine
greeum mcp warmup                          # downloads the default model
```
- MCP/CLI run with **hash fallback by default** for fast startup.
- Add `--semantic` (or unset `GREEUM_DISABLE_ST`) when you want the SentenceTransformer-enabled search:
  ```bash
  greeum mcp serve --semantic -t stdio
  ```

---

## 2. MCP Integration

### Codex (STDIO)
1. Ensure `greeum setup` has been run at least once.
2. `~/.codex/config.toml`
   ```toml
   [mcp_servers.greeum]
   command = "greeum"
   args    = ["mcp", "serve", "-t", "stdio"]
   env     = { "GREEUM_QUIET" = "true", "PYTORCH_ENABLE_MPS_FALLBACK" = "1" }
   ```
3. Optional semantic mode:
   ```toml
   args = ["mcp", "serve", "-t", "stdio", "--semantic"]
   ```
   > First run may take longer while the model loads. Warm-up before enabling for smoother startup.

### ClaudeCode / Cursor (native MCP)
```bash
greeum mcp serve
```
- Add the command above to the client‚Äôs MCP configuration.
- Semantic mode: `greeum mcp serve --semantic`

### HTTP / URL-based MCP (e.g. ChatGPT)
```bash
greeum mcp serve -t http --host 0.0.0.0 --port 8800
```
Then register `http://127.0.0.1:8800/mcp` as the endpoint.

---

## 3. LLM Prompting Guidelines
- **Always close sessions with a summary**: ‚ÄúCall `add_memory` summarising decisions before ending the shift.‚Äù
- **Retrieve before writing**: run `search_memory` with the task keywords before starting work.
- **Use anchor slots (A/B/C)** for hot contexts:
  ```json
  {
    "name": "search_memory",
    "arguments": { "query": "login flow", "limit": 5, "slot": "A" }
  }
  ```
- Encourage agents to log important facts with `importance` ‚â• 0.6 so team hand‚Äëoffs stay seamless.

---

## 4. CLI Essentials

```bash
# Add context
greeum memory add "Legal copy updated for release"

# Search (global fallback enabled by default)
greeum memory search "release notes" --count 5

# Anchor-based search (slot-aware)
greeum memory search "translations" --slot B --radius 2

# Rebuild branch indices (FAISS + keyword or keyword-only)
greeum memory reindex             # uses FAISS if available
greeum memory reindex --disable-faiss

# Reuse the long-running worker (avoids cold-start on each CLI call)
greeum worker serve --host 127.0.0.1 --port 8800   # terminal 1
export GREEUM_MCP_HTTP="http://127.0.0.1:8800/mcp" # terminal 2
greeum memory add "Sprint hand-off" --use-worker
greeum memory search "hand-off" --use-worker
```

Other useful commands:
- `greeum anchors status` / `set A <block>` / `pin A`
- `greeum workflow search "<topic>"` for scripted MCP calls
- `greeum mcp warmup` to cache the embedding model before enabling semantic mode

---

## 5. Documentation
- [Getting Started](docs/get-started.md)
- [MCP Integration Details](docs/mcp-integration.md)
- [Automation Workflow Guide](docs/greeum-workflow-guide.md)
- [API Reference](docs/api-reference.md)

---

## 6. License
MIT License ‚Äî see [LICENSE](LICENSE).

---

**Greeum** ¬∑ Persistent memory for AI‚Äîbuilt and maintained by the community.
