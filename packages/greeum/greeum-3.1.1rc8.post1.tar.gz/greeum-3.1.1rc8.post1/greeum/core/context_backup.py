"""
Context Backup System for Greeum v2.6.3
STM Architecture Reimagining - Raw Data Backup Schema

Claude Code auto-compact ëŒ€ì‘ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´ ì‹œìŠ¤í…œ
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Any, Optional
from datetime import datetime
import uuid
import json


class ContextType(Enum):
    """ì»¨í…ìŠ¤íŠ¸ ë°±ì—… ìœ í˜•"""
    USER_MESSAGE = "user_message"           # ì‚¬ìš©ì ë©”ì‹œì§€
    ASSISTANT_RESPONSE = "assistant_response" # AI ì‘ë‹µ
    TOOL_RESULT = "tool_result"             # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
    SYSTEM_STATE = "system_state"           # ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
    ERROR_CONTEXT = "error_context"         # ì˜¤ë¥˜ ë°œìƒ ë§¥ë½
    CONVERSATION_TURN = "conversation_turn" # ëŒ€í™” í„´ ì „ì²´
    MCP_INTERACTION = "mcp_interaction"     # MCP ì„œë²„ ìƒí˜¸ì‘ìš©


class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    RAW = "raw"                            # ì›ì‹œ ë°ì´í„° ìƒíƒœ
    EXTRACTING = "extracting"              # ì •ë³´ ì¶”ì¶œ ì¤‘
    READY_FOR_LTM = "ready_for_ltm"       # LTM ì „ì†¡ ì¤€ë¹„ ì™„ë£Œ
    PROCESSED = "processed"                # LTM ì²˜ë¦¬ ì™„ë£Œ
    LOST = "lost"                          # Auto-compactìœ¼ë¡œ ì†ì‹¤
    RECOVERED = "recovered"                # ë°±ì—…ì—ì„œ ë³µêµ¬ë¨
    FAILED = "failed"                      # ì²˜ë¦¬ ì‹¤íŒ¨
    ARCHIVED = "archived"                  # ë³´ê´€ë¨


class BackupStrategy(Enum):
    """ë°±ì—… ì „ëµ"""
    IMMEDIATE = "immediate"                # ì¦‰ì‹œ ë°±ì—…
    PERIODIC = "periodic"                  # ì£¼ê¸°ì  ë°±ì—…
    THRESHOLD_BASED = "threshold_based"    # ì„ê³„ì  ê¸°ë°˜ ë°±ì—…
    USER_TRIGGERED = "user_triggered"      # ì‚¬ìš©ì ìˆ˜ë™ ë°±ì—…
    AUTO_COMPACT_TRIGGERED = "auto_compact_triggered" # Auto-compact ê°ì§€ ì‹œ


class RetentionPriority(Enum):
    """ë³´ì¡´ ìš°ì„ ìˆœìœ„"""
    CRITICAL = "critical"      # ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€ (ì˜êµ¬ ë³´ì¡´)
    HIGH = "high"             # ë†’ì€ ë³´ì¡´ ìš°ì„ ìˆœìœ„ (30ì¼)
    NORMAL = "normal"         # ì¼ë°˜ ë³´ì¡´ (7ì¼)
    LOW = "low"              # ë‚®ì€ ë³´ì¡´ (24ì‹œê°„)
    DISPOSABLE = "disposable" # ì–¸ì œë“  ì‚­ì œ ê°€ëŠ¥ (1ì‹œê°„)


@dataclass
class ContextBackupItem:
    """Raw Context Backup Item for STM Layer"""
    
    # Core Identifiers
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str = ""
    timestamp: datetime = field(default_factory=datetime.now)
    
    # Raw Context Data (í•µì‹¬: ì›ë³¸ ë°ì´í„° ë³´ì¡´)
    raw_content: str = ""
    context_type: ContextType = ContextType.USER_MESSAGE
    
    # Context Position & Sequencing
    sequence_number: int = 0
    parent_context_id: Optional[str] = None
    conversation_turn: int = 0
    
    # Pre-processing Metadata (ì²˜ë¦¬ ì „ ì›ë³¸ ì •ë³´)
    original_length: int = 0
    auto_compact_risk_score: float = 0.0  # 0.0-1.0
    processing_status: ProcessingStatus = ProcessingStatus.RAW
    
    # Extraction Ready Data (LTM ì²˜ë¦¬ ì¤€ë¹„ ë°ì´í„°)
    extracted_intents: List[str] = field(default_factory=list)
    key_entities: List[str] = field(default_factory=list)
    semantic_chunks: List[str] = field(default_factory=list)
    
    # Backup Strategy Metadata
    backup_strategy: BackupStrategy = BackupStrategy.IMMEDIATE
    retention_priority: RetentionPriority = RetentionPriority.NORMAL
    recovery_metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Claude Code Integration
    claudecode_context_id: Optional[str] = None
    tool_usage_context: List[str] = field(default_factory=list)
    
    # Processing Timestamps
    created_at: datetime = field(default_factory=datetime.now)
    processed_at: Optional[datetime] = None
    expires_at: Optional[datetime] = None
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if not self.session_id:
            self.session_id = f"session-{uuid.uuid4().hex[:8]}"
        
        if self.original_length == 0 and self.raw_content:
            self.original_length = len(self.raw_content)
        
        # ë³´ì¡´ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ë§Œë£Œ ì‹œê°„ ì„¤ì •
        if self.expires_at is None:
            self.expires_at = self._calculate_expiry_time()
    
    def _calculate_expiry_time(self) -> Optional[datetime]:
        """ë³´ì¡´ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ë§Œë£Œ ì‹œê°„ ê³„ì‚°"""
        from datetime import timedelta
        
        if self.retention_priority == RetentionPriority.CRITICAL:
            return None  # ì˜êµ¬ ë³´ì¡´
        elif self.retention_priority == RetentionPriority.HIGH:
            return self.created_at + timedelta(days=30)
        elif self.retention_priority == RetentionPriority.NORMAL:
            return self.created_at + timedelta(days=7)
        elif self.retention_priority == RetentionPriority.LOW:
            return self.created_at + timedelta(days=1)
        else:  # DISPOSABLE
            return self.created_at + timedelta(hours=1)
    
    def is_expired(self) -> bool:
        """ë§Œë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if self.expires_at is None:  # CRITICAL = ì˜êµ¬ ë³´ì¡´
            return False
        return datetime.now() > self.expires_at
    
    def is_ready_for_ltm(self) -> bool:
        """LTM ì „ì†¡ ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return (
            self.processing_status == ProcessingStatus.READY_FOR_LTM and
            len(self.extracted_intents) > 0 and
            len(self.semantic_chunks) > 0
        )
    
    def should_immediate_process(self) -> bool:
        """ì¦‰ì‹œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        return (
            self.auto_compact_risk_score > 0.7 or
            self.retention_priority == RetentionPriority.CRITICAL or
            self.backup_strategy == BackupStrategy.AUTO_COMPACT_TRIGGERED
        )
    
    def extract_semantic_info(self) -> bool:
        """ì˜ë¯¸ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            self.processing_status = ProcessingStatus.EXTRACTING
            
            # ê°„ë‹¨í•œ ì˜ë„ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)
            content_lower = self.raw_content.lower()
            intent_keywords = {
                "ì§ˆë¬¸": ["ë­", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "?"],
                "ìš”ì²­": ["í•´ì¤˜", "ë§Œë“¤ì–´", "êµ¬í˜„", "ì‘ì„±", "ìƒì„±", "ì¶”ê°€"],
                "ì„¤ëª…": ["ì„¤ëª…", "ì•Œë ¤ì¤˜", "ë³´ì—¬ì¤˜", "ì´í•´", "íŒŒì•…"],
                "ë¬¸ì œí•´ê²°": ["ì˜¤ë¥˜", "ì—ëŸ¬", "ë¬¸ì œ", "í•´ê²°", "ìˆ˜ì •", "ê³ ì¹˜"],
                "í™•ì¸": ["í™•ì¸", "ì²´í¬", "ê²€ì¦", "í…ŒìŠ¤íŠ¸", "ê²€í† "]
            }
            
            for intent, keywords in intent_keywords.items():
                if any(keyword in content_lower for keyword in keywords):
                    self.extracted_intents.append(intent)
            
            # ê°„ë‹¨í•œ ì—”í‹°í‹° ì¶”ì¶œ (ëŒ€ë¬¸ì ë‹¨ì–´, ì½”ë“œ íŒ¨í„´)
            words = self.raw_content.split()
            for word in words:
                # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ (ê³ ìœ ëª…ì‚¬ ê°€ëŠ¥ì„±)
                if word[0].isupper() and len(word) > 2:
                    self.key_entities.append(word)
                
                # íŒŒì¼ í™•ì¥ì íŒ¨í„´
                if '.' in word and any(ext in word for ext in ['.py', '.js', '.md', '.json', '.txt']):
                    self.key_entities.append(word)
            
            # ì˜ë¯¸ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥ ë‹¨ìœ„)
            sentences = [s.strip() for s in self.raw_content.split('.') if s.strip()]
            self.semantic_chunks = sentences[:10]  # ìµœëŒ€ 10ê°œ ì²­í¬
            
            self.processing_status = ProcessingStatus.READY_FOR_LTM
            self.processed_at = datetime.now()
            
            return True
            
        except Exception as e:
            self.processing_status = ProcessingStatus.FAILED
            self.recovery_metadata['extraction_error'] = str(e)
            return False
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì§ë ¬í™”)"""
        return {
            'id': self.id,
            'session_id': self.session_id,
            'timestamp': self.timestamp.isoformat(),
            'raw_content': self.raw_content,
            'context_type': self.context_type.value,
            'sequence_number': self.sequence_number,
            'parent_context_id': self.parent_context_id,
            'conversation_turn': self.conversation_turn,
            'original_length': self.original_length,
            'auto_compact_risk_score': self.auto_compact_risk_score,
            'processing_status': self.processing_status.value,
            'extracted_intents': self.extracted_intents,
            'key_entities': self.key_entities,
            'semantic_chunks': self.semantic_chunks,
            'backup_strategy': self.backup_strategy.value,
            'retention_priority': self.retention_priority.value,
            'recovery_metadata': self.recovery_metadata,
            'claudecode_context_id': self.claudecode_context_id,
            'tool_usage_context': self.tool_usage_context,
            'created_at': self.created_at.isoformat(),
            'processed_at': self.processed_at.isoformat() if self.processed_at else None,
            'expires_at': self.expires_at.isoformat() if self.expires_at else None
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ContextBackupItem':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ë³µì› (ì—­ì§ë ¬í™”)"""
        item = cls()
        
        item.id = data.get('id', item.id)
        item.session_id = data.get('session_id', item.session_id)
        item.timestamp = datetime.fromisoformat(data['timestamp']) if data.get('timestamp') else item.timestamp
        item.raw_content = data.get('raw_content', '')
        item.context_type = ContextType(data.get('context_type', 'user_message'))
        item.sequence_number = data.get('sequence_number', 0)
        item.parent_context_id = data.get('parent_context_id')
        item.conversation_turn = data.get('conversation_turn', 0)
        item.original_length = data.get('original_length', 0)
        item.auto_compact_risk_score = data.get('auto_compact_risk_score', 0.0)
        item.processing_status = ProcessingStatus(data.get('processing_status', 'raw'))
        item.extracted_intents = data.get('extracted_intents', [])
        item.key_entities = data.get('key_entities', [])
        item.semantic_chunks = data.get('semantic_chunks', [])
        item.backup_strategy = BackupStrategy(data.get('backup_strategy', 'immediate'))
        item.retention_priority = RetentionPriority(data.get('retention_priority', 'normal'))
        item.recovery_metadata = data.get('recovery_metadata', {})
        item.claudecode_context_id = data.get('claudecode_context_id')
        item.tool_usage_context = data.get('tool_usage_context', [])
        item.created_at = datetime.fromisoformat(data['created_at']) if data.get('created_at') else item.created_at
        item.processed_at = datetime.fromisoformat(data['processed_at']) if data.get('processed_at') else None
        item.expires_at = datetime.fromisoformat(data['expires_at']) if data.get('expires_at') else None
        
        return item
    
    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
    
    @classmethod
    def from_json(cls, json_str: str) -> 'ContextBackupItem':
        """JSON ë¬¸ìì—´ì—ì„œ ë³µì›"""
        data = json.loads(json_str)
        return cls.from_dict(data)
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        status_emoji = {
            ProcessingStatus.RAW: "[NOTE]",
            ProcessingStatus.EXTRACTING: "[PROCESS]", 
            ProcessingStatus.READY_FOR_LTM: "âœ…",
            ProcessingStatus.PROCESSED: "ğŸ’¾",
            ProcessingStatus.LOST: "[ERROR]",
            ProcessingStatus.RECOVERED: "[PROCESS]",
            ProcessingStatus.FAILED: "ğŸ’¥",
            ProcessingStatus.ARCHIVED: "ğŸ“¦"
        }
        
        priority_emoji = {
            RetentionPriority.CRITICAL: "ğŸ”¥",
            RetentionPriority.HIGH: "â­", 
            RetentionPriority.NORMAL: "ğŸ“„",
            RetentionPriority.LOW: "ğŸ“ƒ",
            RetentionPriority.DISPOSABLE: "ğŸ—‘ï¸"
        }
        
        content_preview = (self.raw_content[:50] + "...") if len(self.raw_content) > 50 else self.raw_content
        
        return (
            f"{status_emoji.get(self.processing_status, 'â“')} "
            f"{priority_emoji.get(self.retention_priority, 'â“')} "
            f"[{self.context_type.value}] "
            f"{content_preview} "
            f"(risk: {self.auto_compact_risk_score:.1f})"
        )


def create_context_backup(
    content: str,
    context_type: ContextType = ContextType.USER_MESSAGE,
    session_id: str = "",
    retention_priority: RetentionPriority = RetentionPriority.NORMAL,
    auto_compact_risk_score: float = 0.0
) -> ContextBackupItem:
    """í¸ì˜ í•¨ìˆ˜: ContextBackupItem ìƒì„±"""
    
    return ContextBackupItem(
        raw_content=content,
        context_type=context_type,
        session_id=session_id,
        retention_priority=retention_priority,
        auto_compact_risk_score=auto_compact_risk_score,
        backup_strategy=BackupStrategy.IMMEDIATE
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª ContextBackupItem í…ŒìŠ¤íŠ¸")
    
    # ë°±ì—… í•­ëª© ìƒì„±
    backup = create_context_backup(
        content="Claude Codeì—ì„œ STM ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì‹¶ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
        context_type=ContextType.USER_MESSAGE,
        retention_priority=RetentionPriority.HIGH,
        auto_compact_risk_score=0.8
    )
    
    print(f"ìƒì„±ëœ ë°±ì—…: {backup}")
    
    # ì˜ë¯¸ ì •ë³´ ì¶”ì¶œ
    if backup.extract_semantic_info():
        print(f"âœ… ì˜ë¯¸ ì¶”ì¶œ ì™„ë£Œ:")
        print(f"  ì˜ë„: {backup.extracted_intents}")
        print(f"  ì—”í‹°í‹°: {backup.key_entities[:5]}")
        print(f"  ì²­í¬ ìˆ˜: {len(backup.semantic_chunks)}")
    
    # ì§ë ¬í™”/ì—­ì§ë ¬í™” í…ŒìŠ¤íŠ¸
    json_str = backup.to_json()
    restored = ContextBackupItem.from_json(json_str)
    print(f"âœ… ì§ë ¬í™” í…ŒìŠ¤íŠ¸: {backup.id == restored.id}")