#!/usr/bin/env python3
"""
Context Recovery System for Greeum

ë°±ì—…ëœ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë³µì›í•˜ê³  ì¬êµ¬ì„±í•˜ì—¬ 
Claude Code ì„¸ì…˜ ê°„ ì—°ì†ì„±ì„ ë³´ì¥í•˜ëŠ” í•µì‹¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

Author: Greeum Development Team
Version: 2.6.4
"""

import json
import logging
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from pathlib import Path

from .context_backup import ContextBackupItem, ContextType, ProcessingStatus, BackupStrategy, RetentionPriority
from .raw_data_backup_layer import RawDataBackupLayer
from .database_manager import DatabaseManager


@dataclass
class RecoverySession:
    """ë³µì› ì„¸ì…˜ ì •ë³´"""
    session_id: str
    original_session_id: str
    recovery_timestamp: datetime
    items_recovered: int
    recovery_quality_score: float
    context_preview: str


@dataclass
class ContextSegment:
    """ì»¨í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸"""
    content: str
    timestamp: datetime
    importance_score: float
    segment_type: str  # 'dialogue', 'code', 'analysis', 'instruction'
    context_position: int  # ì›ë˜ ì»¨í…ìŠ¤íŠ¸ì—ì„œì˜ ìœ„ì¹˜
    

class ContextRecoveryManager:
    """
    ë°±ì—…ëœ ì»¨í…ìŠ¤íŠ¸ ë³µì› ë° ì¬êµ¬ì„± ê´€ë¦¬ì
    
    Claude Codeì˜ auto-compactë¡œ ì†ì‹¤ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ 
    ë°±ì—… ë°ì´í„°ë¡œë¶€í„° ì§€ëŠ¥ì ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, backup_layer: RawDataBackupLayer):
        self.backup_layer = backup_layer
        self.logger = logging.getLogger(__name__)
        
        # ë³µì› ì„¤ì •
        self.max_recovery_items = 100
        self.min_quality_threshold = 0.5
        self.context_merge_window = timedelta(minutes=30)
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ í‚¤ì›Œë“œ
        self.segment_classifiers = {
            'dialogue': ['ì‚¬ìš©ì:', 'assistant:', 'User:', 'Assistant:', 'ì§ˆë¬¸:', 'ë‹µë³€:'],
            'code': ['```', 'def ', 'class ', 'import ', 'function', 'const ', 'let ', 'var '],
            'analysis': ['ë¶„ì„', 'ê²°ê³¼', 'í‰ê°€', 'ê²€í† ', 'í…ŒìŠ¤íŠ¸', 'analysis', 'result', 'evaluation'],
            'instruction': ['ì§€ì‹œ:', 'ìš”ì²­:', 'ëª…ë ¹:', 'instruction:', 'command:', 'request:']
        }
    
    def recover_session_context(self, session_id: str, limit: int = None) -> Dict[str, Any]:
        """
        íŠ¹ì • ì„¸ì…˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
        
        Args:
            session_id: ë³µì›í•  ì„¸ì…˜ ID
            limit: ë³µì›í•  ìµœëŒ€ í•­ëª© ìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            ë³µì›ëœ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°
        """
        try:
            self.logger.info(f"Starting context recovery for session: {session_id}")
            
            # ë°±ì—… ë°ì´í„° ê²€ìƒ‰
            backup_items = self._get_session_backup_items(session_id, limit or self.max_recovery_items)
            
            if not backup_items:
                return self._create_empty_recovery_result(session_id, "No backup items found")
            
            # ë°±ì—… í•­ëª©ë“¤ì„ ì‹œê°„ìˆœ ì •ë ¬
            backup_items.sort(key=lambda x: x.timestamp)
            
            # ì»¨í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ë° ë¶„ë¥˜
            segments = self._extract_context_segments(backup_items)
            
            # ì„¸ê·¸ë¨¼íŠ¸ í’ˆì§ˆ í‰ê°€ ë° ìš°ì„ ìˆœìœ„ ì§€ì •
            prioritized_segments = self._prioritize_segments(segments)
            
            # ì—°ì†ì ì¸ ì»¨í…ìŠ¤íŠ¸ í”Œë¡œìš° ì¬êµ¬ì„±
            recovered_context = self._rebuild_context_flow(prioritized_segments)
            
            # ë³µì› í’ˆì§ˆ í‰ê°€
            quality_score = self._evaluate_recovery_quality(backup_items, recovered_context)
            
            # ë³µì› ì„¸ì…˜ ê¸°ë¡
            recovery_session = RecoverySession(
                session_id=self._generate_recovery_session_id(),
                original_session_id=session_id,
                recovery_timestamp=datetime.now(),
                items_recovered=len(backup_items),
                recovery_quality_score=quality_score,
                context_preview=recovered_context[:500] + "..." if len(recovered_context) > 500 else recovered_context
            )
            
            result = {
                'success': True,
                'recovery_session': recovery_session,
                'recovered_context': recovered_context,
                'segments_count': len(segments),
                'quality_score': quality_score,
                'original_items_count': len(backup_items),
                'recovery_timestamp': datetime.now().isoformat()
            }
            
            self.logger.info(f"Context recovery completed - Quality: {quality_score:.2f}, Items: {len(backup_items)}")
            return result
            
        except Exception as e:
            self.logger.error(f"Context recovery failed for session {session_id}: {e}")
            return self._create_empty_recovery_result(session_id, f"Recovery failed: {str(e)}")
    
    def rebuild_conversation_flow(self, backup_items: List[ContextBackupItem]) -> str:
        """
        ë°±ì—… í•­ëª©ë“¤ë¡œë¶€í„° ëŒ€í™” íë¦„ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        
        Args:
            backup_items: ë°±ì—…ëœ ì»¨í…ìŠ¤íŠ¸ í•­ëª©ë“¤
            
        Returns:
            ì¬êµ¬ì„±ëœ ëŒ€í™” íë¦„ í…ìŠ¤íŠ¸
        """
        try:
            if not backup_items:
                return ""
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬
            sorted_items = sorted(backup_items, key=lambda x: x.timestamp)
            
            conversation_parts = []
            last_timestamp = None
            
            for item in sorted_items:
                # ì‹œê°„ ê°„ê²©ì´ í´ ê²½ìš° êµ¬ë¶„ì ì¶”ê°€
                if last_timestamp and (item.timestamp - last_timestamp).seconds > 300:  # 5ë¶„
                    conversation_parts.append("\n--- [ì‹œê°„ ê°„ê²©] ---\n")
                
                # ì»¨í…ìŠ¤íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í¬ë§·íŒ…
                formatted_content = self._format_content_by_type(item)
                conversation_parts.append(formatted_content)
                
                last_timestamp = item.timestamp
            
            return "\n\n".join(conversation_parts)
            
        except Exception as e:
            self.logger.error(f"Failed to rebuild conversation flow: {e}")
            return f"[ë³µì› ì˜¤ë¥˜: {str(e)}]"
    
    def smart_context_merge(self, old_context: str, new_context: str) -> str:
        """
        ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ì™€ ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
        
        Args:
            old_context: ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸
            new_context: ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ë³‘í•©ëœ ì»¨í…ìŠ¤íŠ¸
        """
        try:
            if not old_context:
                return new_context
            if not new_context:
                return old_context
            
            # ì¤‘ë³µ ë‚´ìš© ê°ì§€ ë° ì œê±°
            merged_content = self._remove_duplicate_content(old_context, new_context)
            
            # ì»¨í…ìŠ¤íŠ¸ ì—°ê²°ì  ì°¾ê¸°
            connection_point = self._find_context_connection_point(old_context, new_context)
            
            if connection_point:
                # ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì ì´ ìˆëŠ” ê²½ìš°
                merged = self._merge_at_connection_point(old_context, new_context, connection_point)
            else:
                # ì—°ê²°ì ì´ ì—†ëŠ” ê²½ìš° ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                merged = self._merge_by_timestamp(old_context, new_context)
            
            return merged
            
        except Exception as e:
            self.logger.error(f"Smart context merge failed: {e}")
            return f"{old_context}\n\n--- [ë³‘í•© ì˜¤ë¥˜] ---\n\n{new_context}"
    
    def get_recovery_statistics(self, days: int = 7) -> Dict[str, Any]:
        """ìµœê·¼ ë³µì› í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ìµœê·¼ Nì¼ê°„ì˜ ë°±ì—… ë°ì´í„° í†µê³„
            cutoff_date = datetime.now() - timedelta(days=days)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í†µê³„ ì¿¼ë¦¬ (ì‹¤ì œ êµ¬í˜„ ì‹œ SQL ì¿¼ë¦¬ í•„ìš”)
            stats = {
                'recovery_attempts': 0,
                'successful_recoveries': 0,
                'average_quality_score': 0.0,
                'total_items_recovered': 0,
                'most_common_failure_reason': 'No data',
                'average_recovery_time': 0.0,
                'context_types_recovered': {}
            }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Failed to get recovery statistics: {e}")
            return {'error': str(e)}
    
    def _get_session_backup_items(self, session_id: str, limit: int) -> List[ContextBackupItem]:
        """ì„¸ì…˜ì˜ ë°±ì—… í•­ëª©ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            items = []
            
            # ë°±ì—… ê³„ì¸µì˜ ìºì‹œì—ì„œ ì„¸ì…˜ í•­ëª©ë“¤ ê²€ìƒ‰
            for backup_item in self.backup_layer.backup_cache.values():
                if backup_item.session_id == session_id:
                    items.append(backup_item)
            
            # ìºì‹œì— ì—†ìœ¼ë©´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ ì¡°íšŒ
            if not items and hasattr(self.backup_layer, 'db_manager'):
                try:
                    cursor = self.backup_layer.db_manager.conn.cursor()
                    cursor.execute("""
                        SELECT * FROM context_backups 
                        WHERE session_id = ? 
                        ORDER BY timestamp DESC 
                        LIMIT ?
                    """, (session_id, limit))
                    
                    for row in cursor.fetchall():
                        # DB rowë¥¼ ContextBackupItemìœ¼ë¡œ ë³€í™˜
                        backup_item = self._backup_from_db_row(dict(row))
                        if backup_item:
                            items.append(backup_item)
                            
                except Exception as db_e:
                    self.logger.warning(f"Database query failed, using cache only: {db_e}")
            
            self.logger.info(f"Found {len(items)} backup items for session {session_id}")
            return items[:limit]
            
        except Exception as e:
            self.logger.error(f"Failed to get session backup items: {e}")
            return []
    
    def _backup_from_db_row(self, row_dict: Dict[str, Any]) -> Optional[ContextBackupItem]:
        """ë°ì´í„°ë² ì´ìŠ¤ rowë¥¼ ContextBackupItemìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ í•„ë“œë“¤
            item = ContextBackupItem(
                id=row_dict.get('id', ''),
                session_id=row_dict.get('session_id', ''),
                raw_content=row_dict.get('raw_content', ''),
                context_type=ContextType(row_dict.get('context_type', 'user_message')),
                sequence_number=row_dict.get('sequence_number', 0),
                parent_context_id=row_dict.get('parent_context_id'),
                conversation_turn=row_dict.get('conversation_turn', 0),
                original_length=row_dict.get('original_length', 0),
                auto_compact_risk_score=row_dict.get('auto_compact_risk_score', 0.0),
                processing_status=ProcessingStatus(row_dict.get('processing_status', 'raw')),
                backup_strategy=BackupStrategy(row_dict.get('backup_strategy', 'immediate')),
                retention_priority=RetentionPriority(row_dict.get('retention_priority', 'normal'))
            )
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
            if 'timestamp' in row_dict and row_dict['timestamp']:
                item.timestamp = datetime.fromisoformat(row_dict['timestamp'])
            if 'created_at' in row_dict and row_dict['created_at']:
                item.created_at = datetime.fromisoformat(row_dict['created_at'])
            if 'processed_at' in row_dict and row_dict['processed_at']:
                item.processed_at = datetime.fromisoformat(row_dict['processed_at'])
            if 'expires_at' in row_dict and row_dict['expires_at']:
                item.expires_at = datetime.fromisoformat(row_dict['expires_at'])
                
            # JSON í•„ë“œë“¤ íŒŒì‹±
            if 'extracted_intents' in row_dict and row_dict['extracted_intents']:
                item.extracted_intents = json.loads(row_dict['extracted_intents'])
            if 'key_entities' in row_dict and row_dict['key_entities']:
                item.key_entities = json.loads(row_dict['key_entities'])
            if 'semantic_chunks' in row_dict and row_dict['semantic_chunks']:
                item.semantic_chunks = json.loads(row_dict['semantic_chunks'])
            if 'recovery_metadata' in row_dict and row_dict['recovery_metadata']:
                item.recovery_metadata = json.loads(row_dict['recovery_metadata'])
            if 'tool_usage_context' in row_dict and row_dict['tool_usage_context']:
                item.tool_usage_context = json.loads(row_dict['tool_usage_context'])
                
            return item
            
        except Exception as e:
            self.logger.error(f"Failed to convert DB row to ContextBackupItem: {e}")
            return None
    
    def _extract_context_segments(self, backup_items: List[ContextBackupItem]) -> List[ContextSegment]:
        """ë°±ì—… í•­ëª©ë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        segments = []
        
        for i, item in enumerate(backup_items):
            try:
                # ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ë¶„ë¥˜
                segment_type = self._classify_segment_type(item.raw_content)
                
                # ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ ê³„ì‚°
                importance_score = self._calculate_segment_importance(item)
                
                segment = ContextSegment(
                    content=item.raw_content,
                    timestamp=item.timestamp,
                    importance_score=importance_score,
                    segment_type=segment_type,
                    context_position=i
                )
                
                segments.append(segment)
                
            except Exception as e:
                self.logger.warning(f"Failed to extract segment from item {i}: {e}")
                continue
        
        return segments
    
    def _classify_segment_type(self, content: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©ì˜ íƒ€ì…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        content_lower = content.lower()
        
        for segment_type, keywords in self.segment_classifiers.items():
            if any(keyword.lower() in content_lower for keyword in keywords):
                return segment_type
        
        return 'general'
    
    def _calculate_segment_importance(self, item: ContextBackupItem) -> float:
        """ì„¸ê·¸ë¨¼íŠ¸ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        importance = 0.5  # ê¸°ë³¸ê°’
        
        # ë³´ì¡´ ìš°ì„ ìˆœìœ„ ë°˜ì˜ (RetentionPriority enum ì‚¬ìš©)
        priority_weights = {
            RetentionPriority.CRITICAL: 0.3,
            RetentionPriority.HIGH: 0.2,
            RetentionPriority.NORMAL: 0.1,
            RetentionPriority.LOW: 0.05
        }
        importance += priority_weights.get(item.retention_priority, 0.05)
        
        # Auto-compact ìœ„í—˜ë„ ë°˜ì˜
        importance += item.auto_compact_risk_score * 0.2
        
        # ì»¨í…ì¸  ê¸¸ì´ ë°˜ì˜ (ì ë‹¹í•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
        length_factor = min(item.original_length / 1000, 1.0) * 0.15
        importance += length_factor
        
        # ì»¨í…ìŠ¤íŠ¸ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        type_weights = {
            ContextType.ERROR_CONTEXT: 0.3,
            ContextType.USER_MESSAGE: 0.2,
            ContextType.TOOL_RESULT: 0.15,
            ContextType.CONVERSATION_TURN: 0.1,
            ContextType.SYSTEM_STATE: 0.05
        }
        importance += type_weights.get(item.context_type, 0.05)
        
        return min(importance, 1.0)
    
    def _prioritize_segments(self, segments: List[ContextSegment]) -> List[ContextSegment]:
        """ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
        return sorted(segments, key=lambda s: (s.importance_score, s.context_position), reverse=True)
    
    def _rebuild_context_flow(self, segments: List[ContextSegment]) -> str:
        """ìš°ì„ ìˆœìœ„ê°€ ì§€ì •ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ í”Œë¡œìš°ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        if not segments:
            return ""
        
        # ë†’ì€ í’ˆì§ˆì˜ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì„ íƒ
        quality_segments = [s for s in segments if s.importance_score >= self.min_quality_threshold]
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì¬ì •ë ¬
        time_ordered = sorted(quality_segments, key=lambda s: s.timestamp)
        
        # ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        context_parts = []
        for segment in time_ordered:
            formatted_segment = self._format_segment_for_recovery(segment)
            context_parts.append(formatted_segment)
        
        return "\n\n".join(context_parts)
    
    def _format_segment_for_recovery(self, segment: ContextSegment) -> str:
        """ë³µì›ìš© ì„¸ê·¸ë¨¼íŠ¸ í¬ë§·íŒ…"""
        timestamp_str = segment.timestamp.strftime("%H:%M:%S")
        type_marker = {
            'dialogue': 'ğŸ’¬',
            'code': 'ğŸ”§', 
            'analysis': 'ğŸ“Š',
            'instruction': 'ğŸ“‹',
            'general': '[NOTE]'
        }.get(segment.segment_type, '[NOTE]')
        
        return f"[{timestamp_str}] {type_marker} {segment.content}"
    
    def _format_content_by_type(self, item: ContextBackupItem) -> str:
        """ì»¨í…ìŠ¤íŠ¸ íƒ€ì…ì— ë”°ë¼ ë‚´ìš©ì„ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        timestamp = item.timestamp.strftime("%Y-%m-%d %H:%M:%S")
        
        if item.context_type == "emergency_precompact":
            return f"[ALERT] [ê¸´ê¸‰ë°±ì—… {timestamp}]\n{item.raw_content}"
        elif item.context_type == "user_interaction":
            return f"ğŸ‘¤ [ì‚¬ìš©ì {timestamp}]\n{item.raw_content}"
        elif item.context_type == "code_analysis":
            return f"ğŸ”§ [ì½”ë“œë¶„ì„ {timestamp}]\n{item.raw_content}"
        else:
            return f"[NOTE] [{item.context_type} {timestamp}]\n{item.raw_content}"
    
    def _evaluate_recovery_quality(self, original_items: List[ContextBackupItem], recovered_context: str) -> float:
        """ë³µì› í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤."""
        if not original_items or not recovered_context:
            return 0.0
        
        # ê¸°ë³¸ í’ˆì§ˆ ìŠ¤ì½”ì–´
        quality_score = 0.7
        
        # ë³µì›ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„ìœ¨
        total_original_length = sum(item.original_length for item in original_items)
        recovery_ratio = min(len(recovered_context) / max(total_original_length, 1), 1.0)
        quality_score *= recovery_ratio
        
        # ì‹œê°„ ì—°ì†ì„± í‰ê°€
        time_continuity = self._evaluate_time_continuity(original_items)
        quality_score += time_continuity * 0.2
        
        # ì»¨í…ì¸  ë‹¤ì–‘ì„± í‰ê°€
        content_diversity = len(set(item.context_type for item in original_items)) / 5.0  # ìµœëŒ€ 5ê°€ì§€ íƒ€ì…
        quality_score += content_diversity * 0.1
        
        return min(quality_score, 1.0)
    
    def _evaluate_time_continuity(self, items: List[ContextBackupItem]) -> float:
        """ì‹œê°„ ì—°ì†ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        if len(items) < 2:
            return 1.0
        
        sorted_items = sorted(items, key=lambda x: x.timestamp)
        gaps = []
        
        for i in range(1, len(sorted_items)):
            gap = (sorted_items[i].timestamp - sorted_items[i-1].timestamp).total_seconds()
            gaps.append(gap)
        
        # í‰ê·  ê°„ê²©ì´ ì ì ˆí•œì§€ í‰ê°€ (ë„ˆë¬´ í¬ë©´ ì—°ì†ì„± ë‚®ìŒ)
        avg_gap = sum(gaps) / len(gaps)
        continuity_score = max(0, 1.0 - (avg_gap / 3600))  # 1ì‹œê°„ ê¸°ì¤€
        
        return continuity_score
    
    def _remove_duplicate_content(self, old_context: str, new_context: str) -> str:
        """ì¤‘ë³µ ë‚´ìš©ì„ ì œê±°í•©ë‹ˆë‹¤."""
        # ë‹¨ìˆœ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì¤‘ë³µ ê°ì§€ í•„ìš”
        lines_old = set(old_context.split('\n'))
        lines_new = new_context.split('\n')
        
        unique_new_lines = [line for line in lines_new if line not in lines_old]
        
        return '\n'.join(unique_new_lines)
    
    def _find_context_connection_point(self, old_context: str, new_context: str) -> Optional[str]:
        """ë‘ ì»¨í…ìŠ¤íŠ¸ ê°„ì˜ ì—°ê²°ì ì„ ì°¾ìŠµë‹ˆë‹¤."""
        # ë‹¨ìˆœ êµ¬í˜„ - ê³µí†µ ë¬¸ì¥ì´ë‚˜ íŒ¨í„´ ì°¾ê¸°
        old_lines = old_context.split('\n')[-10:]  # ë§ˆì§€ë§‰ 10ì¤„
        new_lines = new_context.split('\n')[:10]   # ì²˜ìŒ 10ì¤„
        
        for old_line in old_lines:
            for new_line in new_lines:
                # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì—°ê²°ì ìœ¼ë¡œ íŒë‹¨
                similarity = self._calculate_line_similarity(old_line, new_line)
                if similarity > 0.7:
                    return old_line
        
        return None
    
    def _calculate_line_similarity(self, line1: str, line2: str) -> float:
        """ë‘ ì¤„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not line1 or not line2:
            return 0.0
        
        # ë‹¨ìˆœ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ìœ ì‚¬ë„ ê³„ì‚° í•„ìš”
        words1 = set(line1.lower().split())
        words2 = set(line2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def _merge_at_connection_point(self, old_context: str, new_context: str, connection_point: str) -> str:
        """ì—°ê²°ì ì—ì„œ ë‘ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
        # ì—°ê²°ì ì„ ê¸°ì¤€ìœ¼ë¡œ splití•˜ê³  merge
        old_parts = old_context.split(connection_point)
        new_parts = new_context.split(connection_point)
        
        if len(old_parts) >= 2 and len(new_parts) >= 2:
            merged = old_parts[0] + connection_point + new_parts[1]
        else:
            merged = old_context + "\n\n--- [ì—°ê²°] ---\n\n" + new_context
        
        return merged
    
    def _merge_by_timestamp(self, old_context: str, new_context: str) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ë‘ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
        return f"{old_context}\n\n--- [ì‹œê°„ìˆœ ë³‘í•©] ---\n\n{new_context}"
    
    def _create_empty_recovery_result(self, session_id: str, reason: str) -> Dict[str, Any]:
        """ë¹ˆ ë³µì› ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            'success': False,
            'session_id': session_id,
            'error_reason': reason,
            'recovered_context': "",
            'segments_count': 0,
            'quality_score': 0.0,
            'recovery_timestamp': datetime.now().isoformat()
        }
    
    def _generate_recovery_session_id(self) -> str:
        """ë³µì› ì„¸ì…˜ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return f"recovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{id(self) % 10000}"


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_recovery_manager(backup_layer: RawDataBackupLayer) -> ContextRecoveryManager:
    """Context Recovery Manager ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return ContextRecoveryManager(backup_layer)


def quick_session_recovery(session_id: str) -> str:
    """ë¹ ë¥¸ ì„¸ì…˜ ë³µì›ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        from .database_manager import DatabaseManager
        
        db_manager = DatabaseManager()
        backup_layer = RawDataBackupLayer(db_manager)
        recovery_manager = create_recovery_manager(backup_layer)
        
        result = recovery_manager.recover_session_context(session_id)
        return result.get('recovered_context', 'Recovery failed')
        
    except Exception as e:
        return f"Quick recovery failed: {str(e)}"


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    print("[PROCESS] Context Recovery System í…ŒìŠ¤íŠ¸")
    
    # ì„ì‹œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_session_id = "test_session_123"
    
    # í…ŒìŠ¤íŠ¸ ë³µì›
    result = quick_session_recovery(test_session_id)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë³µì› ê²°ê³¼: {result}")
    
    print("âœ… Context Recovery System ì¤€ë¹„ ì™„ë£Œ")