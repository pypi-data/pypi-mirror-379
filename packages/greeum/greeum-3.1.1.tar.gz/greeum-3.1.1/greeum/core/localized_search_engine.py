"""
Phase 3: LocalizedSearchEngine - ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì§€ì—­ ê²€ìƒ‰

ì´ ëª¨ë“ˆì€ CheckpointManagerê°€ ìƒì„±í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•˜ì—¬
ì „ì²´ LTM ëŒ€ì‹  ê´€ë ¨ì„± ë†’ì€ ì§€ì—­ë§Œ ê²€ìƒ‰í•˜ëŠ” ì§€ëŠ¥ì  ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.
"""

import time
import numpy as np
from typing import Dict, List, Any, Optional, Tuple


class LocalizedSearchEngine:
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì§€ì—­ ê²€ìƒ‰"""
    
    def __init__(self, checkpoint_manager, block_manager):
        self.checkpoint_manager = checkpoint_manager
        self.block_manager = block_manager
        
        # ê²€ìƒ‰ ì„¤ì •
        self.min_slot_relevance = 0.3  # ìŠ¬ë¡¯ ê´€ë ¨ì„± ìµœì†Œ ì„ê³„ê°’
        self.min_block_relevance = 0.2  # ë¸”ë¡ ê´€ë ¨ì„± ìµœì†Œ ì„ê³„ê°’
        self.default_radius = 15  # ê¸°ë³¸ ê²€ìƒ‰ ë°˜ê²½
        self.max_localized_blocks = 100  # ì§€ì—­ ê²€ìƒ‰ ìµœëŒ€ ë¸”ë¡ ìˆ˜
        self.max_fallback_retries = 3  # fallback ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ë¬´í•œ ì¬ê·€ ë°©ì§€)
        self.block_access_timeout = 5.0  # ë¸”ë¡ ì ‘ê·¼ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.stats = {
            "localized_searches": 0,
            "fallback_searches": 0,
            "total_blocks_searched": 0,
            "avg_search_time_ms": 0.0,
            "checkpoint_hit_rate": 0.0
        }
        
    def search_with_checkpoints(self, query_embedding: List[float], 
                              working_memory, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì§€ì—­ ê²€ìƒ‰"""
        search_start = time.perf_counter()
        
        try:
            # 1. Working Memoryì˜ í™œì„± ìŠ¬ë¡¯ë“¤ì—ì„œ ì²´í¬í¬ì¸íŠ¸ ìˆ˜ì§‘
            active_slots = working_memory.get_active_slots()
            
            if not active_slots:
                return self._fallback_search(query_embedding, top_k, "no_active_slots", 0)
            
            localized_results = []
            used_checkpoints = 0
            
            for slot in active_slots:
                # ìŠ¬ë¡¯ê³¼ ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± ê³„ì‚°
                slot_relevance = self._calculate_slot_relevance(
                    slot.embedding, 
                    query_embedding
                )
                
                print(f"    ğŸ“ ìŠ¬ë¡¯ {slot.slot_id}: ê´€ë ¨ì„± {slot_relevance:.3f}")
                
                # ê´€ë ¨ì„±ì´ ë†’ì€ ìŠ¬ë¡¯ë§Œ ì‚¬ìš©
                if slot_relevance > self.min_slot_relevance:
                    checkpoint_indices = self.checkpoint_manager.get_checkpoint_radius(
                        slot.slot_id, 
                        radius=self._calculate_dynamic_radius(slot_relevance)
                    )
                    
                    if checkpoint_indices:
                        # ì²´í¬í¬ì¸íŠ¸ ì ‘ê·¼ ê¸°ë¡
                        self.checkpoint_manager.update_checkpoint_access(slot.slot_id)
                        used_checkpoints += 1
                        
                        # ì²´í¬í¬ì¸íŠ¸ ì£¼ë³€ ë¸”ë¡ë“¤ë§Œ ê²€ìƒ‰
                        local_results = self._search_localized_blocks(
                            checkpoint_indices, 
                            query_embedding, 
                            top_k * 2  # ì—¬ìœ ë¶„ í™•ë³´
                        )
                        
                        # ìŠ¬ë¡¯ ê´€ë ¨ì„±ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
                        for result in local_results:
                            result["checkpoint_relevance"] = slot_relevance
                            result["source_slot"] = slot.slot_id
                            result["search_method"] = "checkpoint_localized"
                        
                        localized_results.extend(local_results)
                        
                        print(f"      âœ… ì²´í¬í¬ì¸íŠ¸ {len(checkpoint_indices)}ê°œ ë¸”ë¡ â†’ {len(local_results)}ê°œ ê²°ê³¼")
                    else:
                        print(f"      âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
                else:
                    print(f"      [ERROR] ê´€ë ¨ì„± ë¶€ì¡± (< {self.min_slot_relevance})")
            
            # 2. ì²´í¬í¬ì¸íŠ¸ ê²°ê³¼ ì²˜ë¦¬
            if localized_results and used_checkpoints > 0:
                final_results = self._process_localized_results(localized_results, top_k)
                search_time = (time.perf_counter() - search_start) * 1000
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self._update_stats("localized", len(localized_results), search_time)
                
                print(f"    ğŸ¯ ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ì„±ê³µ: {len(final_results)}ê°œ ê²°ê³¼, {search_time:.2f}ms")
                return final_results
            else:
                return self._fallback_search(query_embedding, top_k, "no_checkpoint_results", 0)
                
        except Exception as e:
            print(f"    [ERROR] ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return self._fallback_search(query_embedding, top_k, "error", 0)
    
    def _search_localized_blocks(self, block_indices: List[int], 
                               query_embedding: List[float], limit: int) -> List[Dict[str, Any]]:
        """ì§€ì •ëœ ë¸”ë¡ ì¸ë±ìŠ¤ë“¤ë§Œ ê²€ìƒ‰"""
        results = []
        searched_count = 0
        
        # ê²€ìƒ‰ ë²”ìœ„ ì œí•œ
        indices_to_search = block_indices[:min(limit, self.max_localized_blocks)]
        
        for block_index in indices_to_search:
            try:
                # íƒ€ì„ì•„ì›ƒì„ ê³ ë ¤í•œ ë¸”ë¡ ì ‘ê·¼
                start_time = time.perf_counter()
                block = self.block_manager.get_block_by_index(block_index)
                access_time = time.perf_counter() - start_time
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                if access_time > self.block_access_timeout:
                    print(f"      âš ï¸ ë¸”ë¡ {block_index} ì ‘ê·¼ íƒ€ì„ì•„ì›ƒ ({access_time:.2f}s)")
                    continue
                
                searched_count += 1
                
                if block and "embedding" in block and block["embedding"]:
                    similarity = self._calculate_cosine_similarity(
                        query_embedding, 
                        block["embedding"]
                    )
                    
                    # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’ í™•ì¸
                    if similarity > self.min_block_relevance:
                        results.append({
                            "block_index": block_index,
                            "similarity_score": similarity,
                            "content": block.get("context", ""),
                            "keywords": block.get("keywords", []),
                            "timestamp": block.get("timestamp", ""),
                            "importance": block.get("importance", 0.5)
                        })
                        
            except Exception as e:
                # ê°œë³„ ë¸”ë¡ ì ‘ê·¼ ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë„˜ì–´ê°
                continue
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        return results
    
    def _process_localized_results(self, localized_results: List[Dict[str, Any]], 
                                 top_k: int) -> List[Dict[str, Any]]:
        """ì§€ì—­ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° í†µí•©"""
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ë¸”ë¡ ì¸ë±ìŠ¤)
        unique_results = self._deduplicate_by_block_index(localized_results)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì›ë˜ ì ìˆ˜ + ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ì„±)
        for result in unique_results:
            original_score = result.get("similarity_score", 0.5)
            checkpoint_relevance = result.get("checkpoint_relevance", 0.3)
            importance = result.get("importance", 0.5)
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            result["final_score"] = (
                original_score * 0.6 +           # ìœ ì‚¬ë„ 60%
                checkpoint_relevance * 0.3 +     # ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ì„± 30%
                importance * 0.1                 # ì¤‘ìš”ë„ 10%
            )
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì¬ì •ë ¬
        unique_results.sort(key=lambda x: x["final_score"], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        return unique_results[:top_k]
    
    def _fallback_search(self, query_embedding: List[float], top_k: int, 
                        reason: str, _retry_count: int = 0) -> List[Dict[str, Any]]:
        """ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì „ì²´ LTM ê²€ìƒ‰ (ì¬ê·€ ì œí•œ í¬í•¨)"""
        # ë¬´í•œ ì¬ê·€ ë°©ì§€
        if _retry_count >= self.max_fallback_retries:
            print(f"    [ERROR] Fallback ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({_retry_count}/{self.max_fallback_retries})")
            return []
        fallback_start = time.perf_counter()
        
        try:
            print(f"    [PROCESS] Fallback ê²€ìƒ‰ ì‹œì‘ (ì´ìœ : {reason})")
            
            # ì „ì²´ LTM ê²€ìƒ‰
            fallback_results = self.block_manager.search_by_embedding(
                query_embedding, top_k=top_k
            )
            
            # ê²€ìƒ‰ ë°©ë²• í‘œì‹œ
            for result in fallback_results:
                result["search_method"] = "ltm_fallback"
                result["fallback_reason"] = reason
            
            fallback_time = (time.perf_counter() - fallback_start) * 1000
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats("fallback", len(fallback_results), fallback_time)
            
            print(f"    âœ… Fallback ì™„ë£Œ: {len(fallback_results)}ê°œ ê²°ê³¼, {fallback_time:.2f}ms")
            
            return fallback_results
            
        except Exception as e:
            print(f"    [ERROR] Fallback ê²€ìƒ‰ ì‹¤íŒ¨ (ì¬ì‹œë„ {_retry_count + 1}/{self.max_fallback_retries}): {str(e)}")
            
            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²½ìš° ë‹¤ì‹œ ì‹œë„
            if _retry_count < self.max_fallback_retries - 1:
                time.sleep(0.1)  # ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                return self._fallback_search(query_embedding, top_k, f"{reason}_retry", _retry_count + 1)
            else:
                print(f"    [ERROR] Fallback ìµœì¢… ì‹¤íŒ¨: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
                return []
    
    def _calculate_slot_relevance(self, slot_embedding: List[float], 
                                query_embedding: List[float]) -> float:
        """ìŠ¬ë¡¯ê³¼ ì¿¼ë¦¬ ê°„ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        try:
            return self._calculate_cosine_similarity(slot_embedding, query_embedding)
        except Exception:
            return 0.0
    
    def _calculate_dynamic_radius(self, slot_relevance: float) -> int:
        """ìŠ¬ë¡¯ ê´€ë ¨ì„±ì— ë”°ë¥¸ ë™ì  ê²€ìƒ‰ ë°˜ê²½ ê³„ì‚°"""
        # ê´€ë ¨ì„±ì´ ë†’ì„ìˆ˜ë¡ ë” ë„“ì€ ë°˜ê²½ìœ¼ë¡œ ê²€ìƒ‰
        if slot_relevance > 0.8:
            return 20  # ë§¤ìš° ê´€ë ¨ì„± ë†’ìŒ
        elif slot_relevance > 0.6:
            return 15  # ê´€ë ¨ì„± ë†’ìŒ
        elif slot_relevance > 0.4:
            return 10  # ë³´í†µ ê´€ë ¨ì„±
        else:
            return 5   # ë‚®ì€ ê´€ë ¨ì„±
    
    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not vec1 or not vec2:
                return 0.0
                
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            a = np.array(vec1)
            b = np.array(vec2)
            
            # ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ 0 ë°˜í™˜
            if len(a) != len(b):
                return 0.0
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
            
            similarity = dot_product / (norm_a * norm_b)
            
            # -1 ~ 1 ë²”ìœ„ë¥¼ 0 ~ 1 ë²”ìœ„ë¡œ ë³€í™˜
            return max(0.0, min(1.0, (similarity + 1) / 2))
            
        except Exception as e:
            print(f"    âš ï¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    def _deduplicate_by_block_index(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¸”ë¡ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ë†’ì€ ì ìˆ˜ ìš°ì„ )"""
        seen_indices = set()
        unique_results = []
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
        sorted_results = sorted(
            results, 
            key=lambda x: x.get("similarity_score", 0), 
            reverse=True
        )
        
        for result in sorted_results:
            block_index = result.get("block_index")
            if block_index is not None and block_index not in seen_indices:
                seen_indices.add(block_index)
                unique_results.append(result)
        
        return unique_results
    
    def _update_stats(self, search_type: str, result_count: int, search_time_ms: float):
        """ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        if search_type == "localized":
            self.stats["localized_searches"] += 1
        elif search_type == "fallback":
            self.stats["fallback_searches"] += 1
        
        self.stats["total_blocks_searched"] += result_count
        
        # í‰ê·  ê²€ìƒ‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_searches = self.stats["localized_searches"] + self.stats["fallback_searches"]
        if total_searches > 0:
            current_avg = self.stats["avg_search_time_ms"]
            self.stats["avg_search_time_ms"] = (
                (current_avg * (total_searches - 1) + search_time_ms) / total_searches
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ì ì¤‘ë¥  ê³„ì‚°
            self.stats["checkpoint_hit_rate"] = (
                self.stats["localized_searches"] / total_searches
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """ì§€ì—­ ê²€ìƒ‰ ì—”ì§„ í†µê³„ ë°˜í™˜"""
        return {
            "localized_searches": self.stats["localized_searches"],
            "fallback_searches": self.stats["fallback_searches"],
            "total_searches": self.stats["localized_searches"] + self.stats["fallback_searches"],
            "checkpoint_hit_rate": round(self.stats["checkpoint_hit_rate"], 3),
            "avg_search_time_ms": round(self.stats["avg_search_time_ms"], 3),
            "total_blocks_searched": self.stats["total_blocks_searched"]
        }