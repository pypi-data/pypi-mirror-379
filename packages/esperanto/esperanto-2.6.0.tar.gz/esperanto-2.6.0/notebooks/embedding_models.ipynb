{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esperanto import AIFactory\n",
    "embedding_models = [\n",
    "    # (\"openai\", \"text-embedding-3-small\"),\n",
    "    # (\"azure\", \"text-embedding-3-small\"),\n",
    "    # (\"ollama\", \"mxbai-embed-large\"),\n",
    "    # (\"google\", \"text-embedding-004\"),\n",
    "    # (\"mistral\", \"mistral-embed\"),\n",
    "    # (\"voyage\", \"voyage-3-large\"),\n",
    "    # (\"transformers\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    (\"jina\", \"jina-embeddings-v4\")\n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78ecb2f75e14923b8612fc388a5abcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d915303e7e4f799388b164f30614ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172a12fb6694ad3b8d7fef3be1df174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0191b649e074329a6358a203e855f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee93cb299c344186b5734f71988e37b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d4ac142ad4dd8ae43f0c292969fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for transformers:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "2048\n",
      "2048\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config)\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for transformers:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "2048\n",
      "2048\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config)\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Type Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for google:\n",
      "768\n",
      "768\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for mistral:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for voyage:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for transformers:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "2048\n",
      "2048\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for google:\n",
      "768\n",
      "768\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for mistral:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for voyage:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for transformers:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "2048\n",
      "2048\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from esperanto import AIFactory\n",
    "from esperanto.common_types.task_type import EmbeddingTaskType\n",
    "\n",
    "embedding_models = [\n",
    "    (\"google\", \"text-embedding-004\"),\n",
    "    (\"mistral\", \"mistral-embed\"),\n",
    "    (\"voyage\", \"voyage-3-large\"),\n",
    "    (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    (\"jina\", \"jina-embeddings-v4\")\n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n",
    "\n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jina:\n",
      "128\n",
      "128\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "128\n",
      "128\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from esperanto import AIFactory\n",
    "from esperanto.common_types.task_type import EmbeddingTaskType\n",
    "\n",
    "embedding_models = [\n",
    "    # (\"google\", \"text-embedding-004\"),\n",
    "    # (\"mistral\", \"mistral-embed\"),\n",
    "    # (\"voyage\", \"voyage-3-large\"),\n",
    "    # (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    (\"jina\", \"jina-embeddings-v4\")\n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n",
    "\n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY, \"late_chunking\": True, \"output_dimensions\": 128})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY, \"late_chunking\": True, \"output_dimensions\": 128})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# OpenAI-Compatible Embeddings\n\nThis section demonstrates how to use OpenAI-compatible embedding endpoints such as LM Studio, LocalAI, vLLM, or any custom OpenAI-format API.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# OpenAI-Compatible Embedding Example\n# This works with LM Studio, LocalAI, vLLM, or any OpenAI-compatible endpoint\n\nfrom esperanto import AIFactory\n\n# Example with LM Studio (adjust URL and model name for your setup)\ntry:\n    embedding_model = AIFactory.create_embedding(\n        \"openai-compatible\",\n        model_name=\"nomic-embed-text\",  # Must match the model loaded in your endpoint\n        config={\n            \"base_url\": \"http://localhost:1234/v1\",  # LM Studio default\n            \"api_key\": \"not-required\",  # Often not needed for local endpoints\n            \"timeout\": 120  # 2 minutes timeout for large batches\n        }\n    )\n    \n    texts = [\"Hello, local embeddings!\", \"OpenAI-compatible endpoint working\"]\n    \n    print(f\"Using provider: {embedding_model.provider}\")\n    print(f\"Model: {embedding_model.get_model_name()}\")\n    print(f\"Base URL: {embedding_model.base_url}\")\n    \n    # Generate embeddings\n    embeddings = embedding_model.embed(texts)\n    \n    print(f\"\\nEmbedding dimensions: {len(embeddings[0])}\")\n    print(f\"Number of embeddings: {len(embeddings)}\")\n    print(f\"First few values: {embeddings[0][:5]}\")\n    \nexcept Exception as e:\n    print(f\"Could not connect to OpenAI-compatible endpoint: {e}\")\n    print(\"Make sure your local server (LM Studio, etc.) is running with an embedding model loaded\")\n    print(\"Default LM Studio URL: http://localhost:1234\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}