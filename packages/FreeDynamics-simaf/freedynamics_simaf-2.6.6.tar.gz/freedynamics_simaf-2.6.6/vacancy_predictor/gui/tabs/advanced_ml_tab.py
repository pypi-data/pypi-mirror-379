"""
Tab avanzado de ML que integra funcionalidades de compute_csv_gui.py
Incluye procesamiento, entrenamiento, predicci√≥n y visualizaci√≥n avanzada
"""

import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import pandas as pd
import numpy as np
from pathlib import Path
import threading
import logging
from typing import Callable, Optional, Dict, Any

import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import seaborn as sns

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

logger = logging.getLogger(__name__)

class AdvancedMLTab:
    """Tab avanzado de ML con funcionalidades integradas"""
    
    def __init__(self, parent, data_loaded_callback: Callable):
        self.parent = parent
        self.data_loaded_callback = data_loaded_callback
        
        self.frame = ttk.Frame(parent)
        
        # Variables de configuraci√≥n
        self.atm_total_var = tk.IntVar(value=16384)
        self.energy_min_var = tk.DoubleVar(value=-4.0)
        self.energy_max_var = tk.DoubleVar(value=-2.0)
        self.input_dir_var = tk.StringVar()
        self.output_dir_var = tk.StringVar(value="./ml_results")
        
        # Variables de entrenamiento
        self.n_estimators_var = tk.IntVar(value=100)
        self.test_size_var = tk.DoubleVar(value=0.2)
        self.random_state_var = tk.IntVar(value=42)
        
        # Variables de predicci√≥n
        self.single_file_var = tk.StringVar()
        self.batch_dir_var = tk.StringVar()
        
        # Estado actual
        self.current_data = None
        self.trained_model = None
        self.processing = False


        self.feature_columns = []
        self.target_column = 'vacancies'
        
        self.create_widgets()
    
    def create_widgets(self):
        """Crear widgets del tab"""
        main_container = ttk.Frame(self.frame, padding="10")
        main_container.pack(fill="both", expand=True)
        
        # Crear notebook para sub-tabs
        self.notebook = ttk.Notebook(main_container)
        self.notebook.pack(fill="both", expand=True)
        
        # Sub-tabs
        self.create_config_tab()
        self.create_training_tab()
        self.create_prediction_tab()
        self.create_visualization_tab()
    
    def create_config_tab(self):
        """Tab de configuraci√≥n y procesamiento"""
        config_frame = ttk.Frame(self.notebook)
        self.notebook.add(config_frame, text="Configuraci√≥n & Datos")
        
        # Frame principal con scroll
        canvas = tk.Canvas(config_frame)
        scrollbar = ttk.Scrollbar(config_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Configuraci√≥n general
        config_group = ttk.LabelFrame(scrollable_frame, text="Configuraci√≥n de Procesamiento", padding="10")
        config_group.pack(fill='x', padx=10, pady=5)
        
        # Par√°metros en grid
        ttk.Label(config_group, text="N√∫mero total de √°tomos:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(config_group, textvariable=self.atm_total_var, width=15).grid(row=0, column=1, padx=5, pady=2)
        
        ttk.Label(config_group, text="Energ√≠a m√≠nima (eV):").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(config_group, textvariable=self.energy_min_var, width=15).grid(row=1, column=1, padx=5, pady=2)
        
        ttk.Label(config_group, text="Energ√≠a m√°xima (eV):").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(config_group, textvariable=self.energy_max_var, width=15).grid(row=2, column=1, padx=5, pady=2)
        
        # Directorios
        dirs_group = ttk.LabelFrame(scrollable_frame, text="Directorios", padding="10")
        dirs_group.pack(fill='x', padx=10, pady=5)
        
        # Directorio de entrada
        ttk.Label(dirs_group, text="Directorio de dumps:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(dirs_group, textvariable=self.input_dir_var, width=50).grid(row=0, column=1, padx=5, pady=2)
        ttk.Button(dirs_group, text="Explorar...", command=self.select_input_dir).grid(row=0, column=2, padx=5, pady=2)
        
        # Directorio de salida
        ttk.Label(dirs_group, text="Directorio de salida:").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(dirs_group, textvariable=self.output_dir_var, width=50).grid(row=1, column=1, padx=5, pady=2)
        ttk.Button(dirs_group, text="Explorar...", command=self.select_output_dir).grid(row=1, column=2, padx=5, pady=2)
        
        # Controles de procesamiento
        process_group = ttk.LabelFrame(scrollable_frame, text="Procesamiento de Datos", padding="10")
        process_group.pack(fill='x', padx=10, pady=5)
        
        process_buttons = ttk.Frame(process_group)
        process_buttons.pack(fill='x', pady=5)
        
        self.process_btn = ttk.Button(process_buttons, text="Procesar Dumps ‚Üí CSV", 
                                     command=self.process_dumps)
        self.process_btn.pack(side='left', padx=5)
        
        ttk.Button(process_buttons, text="Cargar CSV Existente", 
                  command=self.load_csv).pack(side='left', padx=5)
        
        ttk.Button(process_buttons, text="Exportar Dataset", 
                  command=self.export_dataset).pack(side='left', padx=5)
        
        # √Årea de estado con scroll
        status_group = ttk.LabelFrame(scrollable_frame, text="Estado del Procesamiento", padding="10")
        status_group.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.status_text = scrolledtext.ScrolledText(status_group, height=12, wrap='word')
        self.status_text.pack(fill='both', expand=True)
        
        # Pack canvas y scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def create_training_tab(self):
        """Tab de entrenamiento del modelo"""
        train_frame = ttk.Frame(self.notebook)
        self.notebook.add(train_frame, text="Entrenamiento")
        
        # Panel izquierdo - Controles
        left_panel = ttk.Frame(train_frame)
        left_panel.pack(side="left", fill="y", padx=(10, 5))
        
        # Par√°metros de entrenamiento
        params_group = ttk.LabelFrame(left_panel, text="Par√°metros del Modelo", padding="10")
        params_group.pack(fill='x', pady=(0, 10))
        
        ttk.Label(params_group, text="N¬∞ Estimadores:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Spinbox(params_group, from_=50, to=500, textvariable=self.n_estimators_var, width=10).grid(row=0, column=1, padx=5, pady=2)
        
        ttk.Label(params_group, text="Tama√±o de test:").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(params_group, textvariable=self.test_size_var, width=10).grid(row=1, column=1, padx=5, pady=2)
        
        ttk.Label(params_group, text="Random State:").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(params_group, textvariable=self.random_state_var, width=10).grid(row=2, column=1, padx=5, pady=2)
        
        # Botones de entrenamiento
        train_buttons = ttk.LabelFrame(left_panel, text="Controles de Entrenamiento", padding="10")
        train_buttons.pack(fill='x', pady=(0, 10))
        
        self.train_btn = ttk.Button(train_buttons, text="Entrenar Modelo", 
                                   command=self.train_model)
        self.train_btn.pack(fill='x', pady=2)
        
        self.cv_btn = ttk.Button(train_buttons, text="Validaci√≥n Cruzada", 
                                command=self.cross_validate, state="disabled")
        self.cv_btn.pack(fill='x', pady=2)
        
        self.save_model_btn = ttk.Button(train_buttons, text="Guardar Modelo", 
                                        command=self.save_model, state="disabled")
        self.save_model_btn.pack(fill='x', pady=2)
        
        ttk.Button(train_buttons, text="Cargar Modelo", 
                  command=self.load_model).pack(fill='x', pady=2)
        
        # Panel derecho - M√©tricas
        right_panel = ttk.Frame(train_frame)
        right_panel.pack(side="right", fill="both", expand=True, padx=(5, 10))
        
        metrics_group = ttk.LabelFrame(right_panel, text="M√©tricas y Resultados", padding="10")
        metrics_group.pack(fill='both', expand=True)
        
        self.metrics_text = scrolledtext.ScrolledText(metrics_group, height=20, wrap='word')
        self.metrics_text.pack(fill='both', expand=True)
    
    def create_prediction_tab(self):
        """Tab de predicci√≥n"""
        pred_frame = ttk.Frame(self.notebook)
        self.notebook.add(pred_frame, text="Predicci√≥n")
        
        # Panel superior - Controles
        top_panel = ttk.Frame(pred_frame)
        top_panel.pack(fill='x', padx=10, pady=5)
        
        # Predicci√≥n individual
        single_group = ttk.LabelFrame(top_panel, text="Predicci√≥n Individual", padding="10")
        single_group.pack(fill='x', pady=(0, 10))
        
        single_frame = ttk.Frame(single_group)
        single_frame.pack(fill='x')
        
        ttk.Label(single_frame, text="Archivo dump:").pack(side='left')
        ttk.Entry(single_frame, textvariable=self.single_file_var, width=40).pack(side='left', padx=5, fill='x', expand=True)
        ttk.Button(single_frame, text="Explorar...", command=self.select_single_file).pack(side='right', padx=5)
        ttk.Button(single_frame, text="Predecir", command=self.predict_single).pack(side='right')
        
        # Predicci√≥n por lotes
        batch_group = ttk.LabelFrame(top_panel, text="Predicci√≥n por Lotes", padding="10")
        batch_group.pack(fill='x')
        
        batch_frame = ttk.Frame(batch_group)
        batch_frame.pack(fill='x')
        
        ttk.Label(batch_frame, text="Directorio:").pack(side='left')
        ttk.Entry(batch_frame, textvariable=self.batch_dir_var, width=40).pack(side='left', padx=5, fill='x', expand=True)
        ttk.Button(batch_frame, text="Explorar...", command=self.select_batch_dir).pack(side='right', padx=5)
        ttk.Button(batch_frame, text="Predecir Lote", command=self.predict_batch).pack(side='right')
        
        # Panel inferior - Resultados
        bottom_panel = ttk.Frame(pred_frame)
        bottom_panel.pack(fill='both', expand=True, padx=10, pady=5)
        
        results_group = ttk.LabelFrame(bottom_panel, text="Resultados de Predicci√≥n", padding="10")
        results_group.pack(fill='both', expand=True)
        
        self.prediction_text = scrolledtext.ScrolledText(results_group, height=20, wrap='word')
        self.prediction_text.pack(fill='both', expand=True)
    




    def visualize_kfold(self):
        """Visualizaci√≥n de K-Fold Cross Validation con ventana separada"""
        if self.current_data is None or self.trained_model is None:
            messagebox.showwarning("Advertencia", "Carga datos y entrena un modelo primero")
            return
        
        try:
            # Preparar datos
            X = self.current_data[self.feature_columns]
            y = self.current_data[self.target_column]
            
            # Crear modelo con tus par√°metros
            model = RandomForestRegressor(
                n_estimators=self.n_estimators_var.get(),
                random_state=self.random_state_var.get(),
                n_jobs=-1
            )
            
            # Llamar funci√≥n de visualizaci√≥n K-Fold (definida abajo)
            fig, results_df, stats = self.plot_kfold_results(model, X, y, cv_folds=5)
            
            # Mostrar en ventana separada
            plt.show()
            
            # Opcional: Mostrar estad√≠sticas en el log
            self.log_message(f"K-Fold completado - R¬≤ promedio: {stats['mean_r2']:.3f} ¬± {stats['std_r2']:.3f}")
            
            return fig, results_df, stats
            
        except Exception as e:
            messagebox.showerror("Error", f"Error en visualizaci√≥n K-Fold:\n{str(e)}")
            self.log_message(f"Error K-Fold: {str(e)}")

    def plot_kfold_results(self, model, X, y, cv_folds=5):
        """
        Genera visualizaci√≥n completa de K-Fold Cross Validation
        
        Returns:
            fig: figura de matplotlib
            results_df: DataFrame con resultados por fold
            stats: diccionario con estad√≠sticas
        """
        from sklearn.model_selection import KFold
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Configurar K-Fold
        kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state_var.get())
        
        # Almacenar resultados
        fold_results = []
        all_y_true = []
        all_y_pred = []
        
        self.log_message(f"Iniciando {cv_folds}-Fold Cross Validation...")
        
        # Iterar por cada fold
        for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X), 1):
            self.log_message(f"Procesando Fold {fold_idx}/{cv_folds}...")
            
            # Dividir datos
            X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]
            y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]
            
            # Entrenar modelo
            fold_model = model.__class__(**model.get_params())
            fold_model.fit(X_train_fold, y_train_fold)
            
            # Predicciones
            y_pred_fold = fold_model.predict(X_test_fold)
            
            # Calcular m√©tricas
            mae = mean_absolute_error(y_test_fold, y_pred_fold)
            rmse = np.sqrt(mean_squared_error(y_test_fold, y_pred_fold))
            r2 = r2_score(y_test_fold, y_pred_fold)
            
            # Guardar resultados
            fold_results.append({
                'fold': fold_idx,
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'n_train': len(X_train_fold),
                'n_test': len(X_test_fold)
            })
            
            # Acumular predicciones para gr√°fico global
            all_y_true.extend(y_test_fold.values)
            all_y_pred.extend(y_pred_fold)
        
        # Crear DataFrame con resultados
        results_df = pd.DataFrame(fold_results)
        
        # Calcular estad√≠sticas generales
        stats = {
            'mean_mae': results_df['mae'].mean(),
            'std_mae': results_df['mae'].std(),
            'mean_rmse': results_df['rmse'].mean(),
            'std_rmse': results_df['rmse'].std(),
            'mean_r2': results_df['r2'].mean(),
            'std_r2': results_df['r2'].std(),
            'cv_folds': cv_folds
        }
        
        # CREAR VISUALIZACI√ìN
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'{cv_folds}-Fold Cross Validation Results', fontsize=16, fontweight='bold')
        
        # 1. M√©tricas por Fold
        ax1 = axes[0, 0]
        x_pos = np.arange(len(results_df))
        ax1.bar(x_pos - 0.25, results_df['mae'], 0.25, label='MAE', alpha=0.8, color='red')
        ax1.bar(x_pos, results_df['rmse'], 0.25, label='RMSE', alpha=0.8, color='blue')
        ax1.bar(x_pos + 0.25, results_df['r2'] * 10, 0.25, label='R¬≤ √ó 10', alpha=0.8, color='green')
        ax1.set_xlabel('Fold')
        ax1.set_ylabel('Valor de M√©trica')
        ax1.set_title('M√©tricas por Fold')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels([f'Fold {i+1}' for i in range(cv_folds)])
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Boxplot de m√©tricas
        ax2 = axes[0, 1]
        metrics_data = [results_df['mae'], results_df['rmse'], results_df['r2']]
        ax2.boxplot(metrics_data, labels=['MAE', 'RMSE', 'R¬≤'])
        ax2.set_title('Distribuci√≥n de M√©tricas')
        ax2.set_ylabel('Valor')
        ax2.grid(True, alpha=0.3)
        
        # 3. Predicciones vs Reales (global)
        ax3 = axes[0, 2]
        ax3.scatter(all_y_true, all_y_pred, alpha=0.6, color='purple', s=30)
        min_val = min(min(all_y_true), min(all_y_pred))
        max_val = max(max(all_y_true), max(all_y_pred))
        ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')
        ax3.set_xlabel('Valores Reales')
        ax3.set_ylabel('Predicciones')
        ax3.set_title(f'Predicciones vs Reales (Global)\nR¬≤ = {r2_score(all_y_true, all_y_pred):.3f}')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Consistencia de R¬≤
        ax4 = axes[1, 0]
        ax4.plot(range(1, cv_folds + 1), results_df['r2'], 'o-', linewidth=2, markersize=8, color='darkgreen')
        ax4.axhline(y=results_df['r2'].mean(), color='red', linestyle='--', alpha=0.7, label=f'Promedio: {results_df["r2"].mean():.3f}')
        ax4.fill_between(range(1, cv_folds + 1), 
                        results_df['r2'].mean() - results_df['r2'].std(),
                        results_df['r2'].mean() + results_df['r2'].std(),
                        alpha=0.2, color='red', label=f'¬±1 Desv.Est.')
        ax4.set_xlabel('Fold')
        ax4.set_ylabel('R¬≤ Score')
        ax4.set_title('Consistencia del R¬≤ por Fold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_xticks(range(1, cv_folds + 1))
        
        # 5. Distribuci√≥n de residuos (global)
        ax5 = axes[1, 1]
        residuals = np.array(all_y_true) - np.array(all_y_pred)
        ax5.hist(residuals, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax5.axvline(x=0, color='red', linestyle='--', linewidth=2)
        ax5.set_xlabel('Residuos (Real - Predicho)')
        ax5.set_ylabel('Frecuencia')
        ax5.set_title('Distribuci√≥n de Residuos (Global)')
        ax5.grid(True, alpha=0.3)
        
        # 6. Tabla de estad√≠sticas
        ax6 = axes[1, 2]
        ax6.axis('off')
        
        # Crear tabla de texto con estad√≠sticas
        stats_text = f"""Estad√≠sticas K-Fold Cross Validation

    Configuraci√≥n:
    ‚Ä¢ Folds: {cv_folds}
    ‚Ä¢ Muestras totales: {len(X)}
    ‚Ä¢ Random State: {self.random_state_var.get()}

    M√©tricas Promedio:
    ‚Ä¢ MAE: {stats['mean_mae']:.4f} ¬± {stats['std_mae']:.4f}
    ‚Ä¢ RMSE: {stats['mean_rmse']:.4f} ¬± {stats['std_rmse']:.4f}
    ‚Ä¢ R¬≤: {stats['mean_r2']:.4f} ¬± {stats['std_r2']:.4f}

    Consistencia del Modelo:
    ‚Ä¢ Coef. Var. MAE: {(stats['std_mae']/stats['mean_mae']*100):.1f}%
    ‚Ä¢ Coef. Var. R¬≤: {(stats['std_r2']/stats['mean_r2']*100):.1f}%

    Interpretaci√≥n:
    ‚Ä¢ Calidad: {'Excelente' if stats['mean_r2'] > 0.9 else 'Buena' if stats['mean_r2'] > 0.7 else 'Moderada'}
    ‚Ä¢ Estabilidad: {'Alta' if stats['std_r2'] < 0.05 else 'Media' if stats['std_r2'] < 0.1 else 'Baja'}
    """
        
        ax6.text(0.1, 0.9, stats_text, transform=ax6.transAxes, fontsize=10,
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
                verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        
        # Log de resultados
        self.log_message(f"K-Fold completado:")
        self.log_message(f"  R¬≤: {stats['mean_r2']:.3f} ¬± {stats['std_r2']:.3f}")
        self.log_message(f"  MAE: {stats['mean_mae']:.3f} ¬± {stats['std_mae']:.3f}")
        self.log_message(f"  RMSE: {stats['mean_rmse']:.3f} ¬± {stats['std_rmse']:.3f}")
        
        return fig, results_df, stats

    # TAMBI√âN MODIFICA el create_visualization_tab para hacer el bot√≥n m√°s visible:

    def create_visualization_tab(self):
        """Tab de visualizaci√≥n - VERSI√ìN CORREGIDA"""
        viz_frame = ttk.Frame(self.notebook)
        self.notebook.add(viz_frame, text="Visualizaci√≥n")
        
        # Controles superiores - SEPARAR EN DOS FILAS
        controls_frame = ttk.Frame(viz_frame)
        controls_frame.pack(fill='x', padx=10, pady=5)
        
        # Primera fila de botones
        row1_frame = ttk.Frame(controls_frame)
        row1_frame.pack(fill='x', pady=2)
        
        ttk.Button(row1_frame, text="üìä Distribuciones", 
                command=self.plot_distributions).pack(side='left', padx=5)
        ttk.Button(row1_frame, text="üìà Feature Importance", 
                command=self.plot_feature_importance).pack(side='left', padx=5)
        ttk.Button(row1_frame, text="üéØ Predicciones vs Real", 
                command=self.plot_predictions).pack(side='left', padx=5)
        
        # Segunda fila de botones - BOT√ìN K-FOLD M√ÅS VISIBLE
        row2_frame = ttk.Frame(controls_frame)
        row2_frame.pack(fill='x', pady=5)  # M√°s padding para separar
        
        # Bot√≥n K-Fold destacado
        self.kfold_btn = ttk.Button(row2_frame, 
                                text="üîÑ K-Fold Cross Validation", 
                                command=self.visualize_kfold,
                                width=25)  # Ancho fijo para hacerlo m√°s prominente
        self.kfold_btn.pack(side='left', padx=5, pady=2)
        
        # Etiqueta de estado
        self.kfold_status_label = ttk.Label(row2_frame, text="(Requiere modelo entrenado)", 
                                        foreground="gray")
        self.kfold_status_label.pack(side='left', padx=10, pady=2)
        
        # Bot√≥n adicional para limpiar gr√°ficos
        ttk.Button(row2_frame, text="üßπ Limpiar", 
                command=self.clear_plots).pack(side='right', padx=5)
        
        # √Årea de gr√°ficos
        self.viz_area = ttk.Frame(viz_frame)
        self.viz_area.pack(fill='both', expand=True, padx=10, pady=5)
        
        # Crear figura matplotlib
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.fig.tight_layout(pad=3.0)
        
        self.canvas = FigureCanvasTkAgg(self.fig, self.viz_area)
        self.canvas.get_tk_widget().pack(fill='both', expand=True)
        
        # Inicializar estado del bot√≥n K-Fold
        self.update_kfold_button_state()
        
        print("Tab de visualizaci√≥n creado correctamente")  # Debug

    # TAMBI√âN aseg√∫rate de que este m√©todo est√© en tu clase:

    def update_kfold_button_state(self):
        """Actualizar estado del bot√≥n K-Fold seg√∫n disponibilidad del modelo"""
        if hasattr(self, 'kfold_btn') and hasattr(self, 'kfold_status_label'):
            if self.trained_model is not None and self.current_data is not None:
                self.kfold_btn.config(state="normal")
                self.kfold_status_label.config(text="‚úÖ Listo para K-Fold", foreground="green")
            else:
                self.kfold_btn.config(state="disabled") 
                self.kfold_status_label.config(text="‚ùå Requiere modelo entrenado", foreground="red")
        else:
            print("Advertencia: botones K-Fold no inicializados")  # Debug

    # MODIFICA tu m√©todo train_model para incluir al final:

    # Al final de train_model(), despu√©s de self.save_model_btn.config(state="normal"):
    self.update_kfold_button_state()

    # TAMBI√âN a√±ade este m√©todo si no existe:

    def clear_plots(self):
        """Limpiar todos los gr√°ficos"""
        if hasattr(self, 'axes'):
            for ax in self.axes.flat:
                ax.clear()
                ax.text(0.5, 0.5, 'Gr√°fico limpio\nUsa los botones para visualizar', 
                    ha='center', va='center', transform=ax.transAxes, fontsize=12, 
                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
                ax.set_xticks([])
                ax.set_yticks([])
            
            if hasattr(self, 'canvas'):
                self.canvas.draw()
        
        print("Gr√°ficos limpiados")  # Debug

    def update_kfold_button_state(self):
        """Actualizar estado del bot√≥n K-Fold seg√∫n disponibilidad del modelo"""
        if hasattr(self, 'kfold_btn') and hasattr(self, 'kfold_status_label'):
            if self.trained_model is not None and self.current_data is not None:
                self.kfold_btn.config(state="normal")
                self.kfold_status_label.config(text="‚úÖ Listo para K-Fold")
            else:
                self.kfold_btn.config(state="disabled") 
                self.kfold_status_label.config(text="‚ùå Requiere modelo entrenado")
    # =============================================================================
    # M√âTODOS DE UTILIDAD
    # =============================================================================
    
    def log_message(self, message):
        """A√±adir mensaje al √°rea de estado"""
        self.status_text.insert(tk.END, f"{message}\n")
        self.status_text.see(tk.END)
        self.frame.update()
    
    def select_input_dir(self):
        """Seleccionar directorio de entrada"""
        directory = filedialog.askdirectory(title="Seleccionar directorio con archivos .dump")
        if directory:
            self.input_dir_var.set(directory)
    
    def select_output_dir(self):
        """Seleccionar directorio de salida"""
        directory = filedialog.askdirectory(title="Seleccionar directorio de salida")
        if directory:
            self.output_dir_var.set(directory)
    
    def select_single_file(self):
        """Seleccionar archivo individual para predicci√≥n"""
        filename = filedialog.askopenfilename(
            title="Seleccionar archivo dump",
            filetypes=[("Dump files", "*.dump"), ("All files", "*.*")]
        )
        if filename:
            self.single_file_var.set(filename)
    
    def select_batch_dir(self):
        """Seleccionar directorio para predicci√≥n por lotes"""
        directory = filedialog.askdirectory(title="Seleccionar directorio para predicci√≥n por lotes")
        if directory:
            self.batch_dir_var.set(directory)
    
    def load_csv(self):
        """Cargar archivo CSV existente"""
        filename = filedialog.askopenfilename(
            title="Cargar dataset CSV",
            filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
        )
        if filename:
            try:
                self.current_data = pd.read_csv(filename)
                self.log_message(f"CSV cargado exitosamente: {Path(filename).name}")
                self.log_message(f"Shape: {self.current_data.shape}")
                self.log_message(f"Columnas: {list(self.current_data.columns)}")
             # Definir autom√°ticamente features y target
                if 'vacancies' in self.current_data.columns:
                    self.target_column = 'vacancies'
                
                exclude_cols = ['filename', self.target_column]
                self.feature_columns = [col for col in self.current_data.columns 
                                    if col not in exclude_cols and 
                                    self.current_data[col].dtype in ['int64', 'float64']]
                # ===============================
                            
                # Notificar callback principal
                self.data_loaded_callback(self.current_data)
                self.log_message(f"CSV cargado: {Path(filename).name}")
                self.log_message(f"Features: {len(self.feature_columns)}, Target: {self.target_column}")
                
                self.data_loaded_callback(self.current_data)                
            except Exception as e:
                messagebox.showerror("Error", f"Error al cargar CSV: {str(e)}")
    
    def export_dataset(self):
        """Exportar dataset actual"""
        if self.current_data is None:
            messagebox.showwarning("Advertencia", "No hay dataset para exportar")
            return
        
        filename = filedialog.asksaveasfilename(
            title="Exportar dataset",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if filename:
            try:
                if filename.endswith('.xlsx'):
                    self.current_data.to_excel(filename, index=False)
                else:
                    self.current_data.to_csv(filename, index=False)
                
                messagebox.showinfo("√âxito", f"Dataset exportado a:\n{filename}")
                self.log_message(f"Dataset exportado: {filename}")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error al exportar: {str(e)}")
    
    # =============================================================================
    # M√âTODOS DE PROCESAMIENTO
    # =============================================================================
    
    def extract_atoms_from_lammps_file(self, filepath):
        """Extraer n√∫mero de √°tomos del encabezado de un archivo LAMMPS"""
        try:
            with open(filepath, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line == "ITEM: NUMBER OF ATOMS":
                        next_line = next(f).strip()
                        return int(next_line)
            return None
        except Exception as e:
            self.log_message(f"Error leyendo {filepath}: {str(e)}")
            return None
    
    def create_histograms(self, dump_data):
        """Crear histogramas de coordinaci√≥n y energ√≠a"""
        features = {}
        
        # Histograma de coordinaci√≥n (4-12, bins de 2)
        coord_col = 'c_coord' if 'c_coord' in dump_data.columns else None
        if coord_col:
            coord_data = dump_data[coord_col].dropna()
            coord_bins = np.arange(4, 14, 2)  # [4, 6, 8, 10, 12]
            coord_hist, _ = np.histogram(coord_data, bins=coord_bins)
            
            for i, (start, end) in enumerate(zip(coord_bins[:-1], coord_bins[1:])):
                features[f'coord_hist_{int(start)}_{int(end)}'] = coord_hist[i]
        
        # Histograma de energ√≠a (rango personalizable, 10 bins)
        energy_col = 'c_peatom' if 'c_peatom' in dump_data.columns else None
        if energy_col:
            energy_data = dump_data[energy_col].dropna()
            energy_bins = np.linspace(self.energy_min_var.get(), self.energy_max_var.get(), 11)
            energy_hist, _ = np.histogram(energy_data, bins=energy_bins)
            
            for i in range(len(energy_hist)):
                features[f'energy_hist_bin_{i+1}'] = energy_hist[i]
        
        return features
    
    def process_dumps(self):
        """Procesar archivos dump y crear CSV con histogramas"""
        if not self.input_dir_var.get():
            messagebox.showwarning("Advertencia", "Selecciona el directorio de entrada")
            return
        
        if self.processing:
            messagebox.showwarning("Advertencia", "Ya hay un procesamiento en curso")
            return
        
        def process_thread():
            self.processing = True
            self.process_btn.config(state="disabled")
            
            try:
                input_path = Path(self.input_dir_var.get())
                output_path = Path(self.output_dir_var.get())
                output_path.mkdir(exist_ok=True)
                
                dump_files = list(input_path.glob("*.dump"))
                
                if not dump_files:
                    self.log_message("No se encontraron archivos .dump")
                    return
                
                self.log_message(f"Procesando {len(dump_files)} archivos dump...")
                
                all_data = []
                
                for i, dump_file in enumerate(dump_files, 1):
                    self.log_message(f"Procesando ({i}/{len(dump_files)}): {dump_file.name}")
                    
                    # Extraer n√∫mero de √°tomos del encabezado
                    n_atoms = self.extract_atoms_from_lammps_file(dump_file)
                    if n_atoms is None:
                        self.log_message(f"Error: No se pudo leer {dump_file.name}")
                        continue
                    
                    # Calcular vacancias
                    vacancies = self.atm_total_var.get() - n_atoms
                    
                    try:
                        # Leer datos del dump (asumiendo formato est√°ndar)
                        dump_data = pd.read_csv(dump_file, sep=' ', skiprows=9, 
                                              names=['id', 'type', 'x', 'y', 'z', 'c_peatom', 
                                                    'c_satom1', 'c_satom2', 'c_satom3', 'c_satom4', 
                                                    'c_satom5', 'c_satom6', 'c_coord', 'c_voro1', 'c_keatom'])
                        
                        # Calcular estad√≠sticas b√°sicas
                        stats = {
                            'filename': dump_file.name,
                            'n_atoms': n_atoms,
                            'vacancies': vacancies,
                            'mean_energy': dump_data['c_peatom'].mean(),
                            'std_energy': dump_data['c_peatom'].std(),
                            'mean_coord': dump_data['c_coord'].mean(),
                            'std_coord': dump_data['c_coord'].std(),
                            'min_energy': dump_data['c_peatom'].min(),
                            'max_energy': dump_data['c_peatom'].max(),
                            'min_coord': dump_data['c_coord'].min(),
                            'max_coord': dump_data['c_coord'].max()
                        }
                        
                        # Crear histogramas
                        hist_features = self.create_histograms(dump_data)
                        stats.update(hist_features)
                        
                        all_data.append(stats)
                        
                    except Exception as e:
                        self.log_message(f"Error procesando {dump_file.name}: {str(e)}")
                        continue
                
                # Crear DataFrame y guardar
                if all_data:
                    df = pd.DataFrame(all_data)
                    csv_output = output_path / "advanced_dataset.csv"
                    df.to_csv(csv_output, index=False)
                    
                    self.current_data = df
                    
                    self.log_message(f"Procesamiento completado!")
                    self.log_message(f"Archivo guardado: {csv_output}")
                    self.log_message(f"Total de muestras: {len(df)}")
                    self.log_message(f"Features generadas: {len(df.columns)}")
                    
                    # Notificar callback principal
                    self.data_loaded_callback(df)
                    
                    # Mostrar estad√≠sticas b√°sicas
                    self.log_message(f"\nEstad√≠sticas del dataset:")
                    self.log_message(f"Vacancias - Min: {df['vacancies'].min()}, Max: {df['vacancies'].max()}, Mean: {df['vacancies'].mean():.2f}")
                    
                else:
                    self.log_message("No se pudo procesar ning√∫n archivo")
                    
            except Exception as e:
                self.log_message(f"Error en procesamiento: {str(e)}")
                messagebox.showerror("Error", f"Error en procesamiento: {str(e)}")
            finally:
                self.processing = False
                self.process_btn.config(state="normal")
        
        # Ejecutar en hilo separado
        threading.Thread(target=process_thread, daemon=True).start()
    
    # =============================================================================
    # M√âTODOS DE ENTRENAMIENTO
    # =============================================================================
    
    def train_model(self):
        """Entrenar modelo de ML"""
        if self.current_data is None:
            messagebox.showwarning("Advertencia", "Primero carga o procesa los datos")
            return
        
        def train_thread():
            try:
                self.train_btn.config(state="disabled")
                self.log_message("Iniciando entrenamiento...")
                
                # Preparar datos
                # DESPU√âS (atributos de instancia):
                self.feature_columns = [col for col in self.current_data.columns 
                                    if col not in ['filename', 'vacancies']]
                X = self.current_data[self.feature_columns]
                y = self.current_data['vacancies']
                
                self.log_message(f"Features utilizadas: {len(self.feature_columns)}")
                self.log_message(f"Muestras totales: {len(X)}")
                
                # Divisi√≥n train/test
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=self.test_size_var.get(), 
                    random_state=self.random_state_var.get()
                )
                
                # Crear pipeline
                preprocessor = ColumnTransformer([
                    ('scaler', StandardScaler(), self.feature_columns)
                ])
                
                self.trained_model = Pipeline([
                    ('preprocessor', preprocessor),
                    ('regressor', RandomForestRegressor(
                        n_estimators=self.n_estimators_var.get(),
                        random_state=self.random_state_var.get(),
                        n_jobs=-1
                    ))
                ])
                
                # Entrenar
                self.log_message("Entrenando modelo...")
                self.trained_model.fit(X_train, y_train)
                
                # Evaluar
                train_pred = self.trained_model.predict(X_train)
                test_pred = self.trained_model.predict(X_test)
                
                train_mae = mean_absolute_error(y_train, train_pred)
                test_mae = mean_absolute_error(y_test, test_pred)
                train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                
                # Mostrar resultados
                metrics_text = f"""
=== M√âTRICAS DEL MODELO ===
Algoritmo: Random Forest ({self.n_estimators_var.get()} estimadores)
Features: {len(self.feature_columns)}
Muestras entrenamiento: {len(X_train)}
Muestras prueba: {len(X_test)}

RENDIMIENTO:
Train MAE: {train_mae:.4f}
Test MAE: {test_mae:.4f}
Train RMSE: {train_rmse:.4f}
Test RMSE: {test_rmse:.4f}
Train R¬≤: {train_r2:.4f}
Test R¬≤: {test_r2:.4f}

INTERPRETACI√ìN:
- MAE: Error promedio en n√∫mero de vacancias
- RMSE: Error cuadr√°tico medio
- R¬≤: Proporci√≥n de varianza explicada (0-1, mejor=1)

FEATURES M√ÅS IMPORTANTES:
"""
                
                # Feature importance
                feature_importance = self.trained_model.named_steps['regressor'].feature_importances_
                importance_df = pd.DataFrame({
                    'feature': self.feature_columns,
                    'importance': feature_importance
                }).sort_values('importance', ascending=False)
                
                for i, row in importance_df.head(10).iterrows():
                    metrics_text += f"{row['feature'][:30]:30s}: {row['importance']:.4f}\n"
                
                self.metrics_text.delete(1.0, tk.END)
                self.metrics_text.insert(1.0, metrics_text)
                
                self.log_message("Entrenamiento completado exitosamente!")
                
                # Habilitar botones
                self.cv_btn.config(state="normal")
                self.save_model_btn.config(state="normal")
                self.update_kfold_button_state()    
                
            except Exception as e:
                self.log_message(f"Error en entrenamiento: {str(e)}")
                messagebox.showerror("Error", f"Error en entrenamiento: {str(e)}")
            finally:
                self.train_btn.config(state="normal")
                self.update_kfold_button_state()
        
        threading.Thread(target=train_thread, daemon=True).start()
    
    def cross_validate(self):
        """Realizar validaci√≥n cruzada"""
        if self.current_data is None or self.trained_model is None:
            messagebox.showwarning("Advertencia", "Primero entrena el modelo")
            return
        
        def cv_thread():
            try:
                self.cv_btn.config(state="disabled")
                self.log_message("Realizando validaci√≥n cruzada...")
                
                feature_columns = [col for col in self.current_data.columns 
                                 if col not in ['filename', 'vacancies']]
                
                X = self.current_data[feature_columns]
                y = self.current_data['vacancies']
                
                # Validaci√≥n cruzada con diferentes m√©tricas
                cv_mae = cross_val_score(self.trained_model, X, y, cv=5, 
                                       scoring='neg_mean_absolute_error')
                cv_rmse = cross_val_score(self.trained_model, X, y, cv=5, 
                                        scoring='neg_root_mean_squared_error')
                cv_r2 = cross_val_score(self.trained_model, X, y, cv=5, 
                                      scoring='r2')
                
                cv_text = f"""

=== VALIDACI√ìN CRUZADA (5-Fold) ===
MAE Scores: {[-score for score in cv_mae]}
MAE Mean: {-cv_mae.mean():.4f} ¬± {cv_mae.std():.4f}

RMSE Scores: {[-score for score in cv_rmse]}
RMSE Mean: {-cv_rmse.mean():.4f} ¬± {cv_rmse.std():.4f}

R¬≤ Scores: {cv_r2}
R¬≤ Mean: {cv_r2.mean():.4f} ¬± {cv_r2.std():.4f}

CONSISTENCIA DEL MODELO:
- Desviaci√≥n est√°ndar baja = modelo consistente
- R¬≤ > 0.8 = buen modelo predictivo
- MAE < 5 vacancias = predicciones √∫tiles
"""
                
                self.metrics_text.insert(tk.END, cv_text)
                self.log_message("Validaci√≥n cruzada completada")
                
            except Exception as e:
                self.log_message(f"Error en validaci√≥n cruzada: {str(e)}")
            finally:
                self.cv_btn.config(state="normal")
        
        threading.Thread(target=cv_thread, daemon=True).start()
    
    def save_model(self):
        """Guardar modelo entrenado"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "No hay modelo entrenado")
            return
        
        filename = filedialog.asksaveasfilename(
            title="Guardar modelo",
            defaultextension=".joblib",
            filetypes=[("Joblib files", "*.joblib"), ("Pickle files", "*.pkl")]
        )
        
        if filename:
            try:
                # Guardar modelo y metadatos
                model_data = {
                    'model': self.trained_model,
                    'feature_columns': [col for col in self.current_data.columns 
                                      if col not in ['filename', 'vacancies']],
                    'training_params': {
                        'n_estimators': self.n_estimators_var.get(),
                        'test_size': self.test_size_var.get(),
                        'random_state': self.random_state_var.get(),
                        'atm_total': self.atm_total_var.get(),
                        'energy_min': self.energy_min_var.get(),
                        'energy_max': self.energy_max_var.get()
                    }
                }
                
                joblib.dump(model_data, filename)
                self.log_message(f"Modelo guardado: {filename}")
                messagebox.showinfo("√âxito", f"Modelo guardado exitosamente")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error guardando modelo: {str(e)}")
    
    def load_model(self):
        """Cargar modelo previamente entrenado"""
        filename = filedialog.askopenfilename(
            title="Cargar modelo",
            filetypes=[("Joblib files", "*.joblib"), ("Pickle files", "*.pkl")]
        )
        
        if filename:
            try:
                model_data = joblib.load(filename)
                
                if isinstance(model_data, dict):
                    self.trained_model = model_data['model']
                    
                    # Restaurar par√°metros si est√°n disponibles
                    if 'training_params' in model_data:
                        params = model_data['training_params']
                        self.n_estimators_var.set(params.get('n_estimators', 100))
                        self.atm_total_var.set(params.get('atm_total', 16384))
                        self.energy_min_var.set(params.get('energy_min', -4.0))
                        self.energy_max_var.set(params.get('energy_max', -2.0))
                    
                    self.log_message(f"Modelo cargado: {filename}")
                    self.log_message(f"Features requeridas: {len(model_data.get('feature_columns', []))}")
                    
                else:
                    # Modelo legacy
                    self.trained_model = model_data
                    self.log_message(f"Modelo legacy cargado: {filename}")
                
                # Habilitar botones
                self.cv_btn.config(state="normal")
                self.save_model_btn.config(state="normal")
                
                messagebox.showinfo("√âxito", f"Modelo cargado exitosamente")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error cargando modelo: {str(e)}")
    
    # =============================================================================
    # M√âTODOS DE PREDICCI√ìN
    # =============================================================================
    
    def predict_single(self):
        """Predecir archivo individual"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "Primero carga un modelo entrenado")
            return
        
        if not self.single_file_var.get():
            messagebox.showwarning("Advertencia", "Selecciona un archivo")
            return
        
        def predict_thread():
            try:
                dump_file = Path(self.single_file_var.get())
                
                self.log_message(f"Prediciendo: {dump_file.name}")
                
                # Procesar archivo
                n_atoms = self.extract_atoms_from_lammps_file(dump_file)
                if n_atoms is None:
                    self.log_message("Error leyendo archivo")
                    return
                
                # Leer datos del dump
                dump_data = pd.read_csv(dump_file, sep=' ', skiprows=9, 
                                      names=['id', 'type', 'x', 'y', 'z', 'c_peatom', 
                                            'c_satom1', 'c_satom2', 'c_satom3', 'c_satom4', 
                                            'c_satom5', 'c_satom6', 'c_coord', 'c_voro1', 'c_keatom'])
                
                # Crear features
                stats = {
                    'n_atoms': n_atoms,
                    'mean_energy': dump_data['c_peatom'].mean(),
                    'std_energy': dump_data['c_peatom'].std(),
                    'mean_coord': dump_data['c_coord'].mean(),
                    'std_coord': dump_data['c_coord'].std(),
                    'min_energy': dump_data['c_peatom'].min(),
                    'max_energy': dump_data['c_peatom'].max(),
                    'min_coord': dump_data['c_coord'].min(),
                    'max_coord': dump_data['c_coord'].max()
                }
                
                hist_features = self.create_histograms(dump_data)
                stats.update(hist_features)
                
                # Crear DataFrame para predicci√≥n
                feature_columns = [col for col in self.current_data.columns 
                                 if col not in ['filename', 'vacancies']]
                
                # Asegurar que todas las features est√°n presentes
                for col in feature_columns:
                    if col not in stats:
                        stats[col] = 0.0  # Valor por defecto
                
                X_pred = pd.DataFrame([stats])[feature_columns]
                
                # Predecir
                prediction = self.trained_model.predict(X_pred)[0]
                real_vacancies = self.atm_total_var.get() - n_atoms
                error = abs(prediction - real_vacancies)
                
                result_text = f"""
=== PREDICCI√ìN INDIVIDUAL ===
Archivo: {dump_file.name}
Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

DATOS DEL SISTEMA:
- √Åtomos encontrados: {n_atoms}
- √Åtomos totales configurados: {self.atm_total_var.get()}

RESULTADOS:
- Vacancias reales: {real_vacancies}
- Vacancias predichas: {prediction:.2f}
- Error absoluto: {error:.2f}
- Error relativo: {(error/real_vacancies*100):.1f}%

CARACTER√çSTICAS DEL SISTEMA:
- Energ√≠a promedio: {stats['mean_energy']:.4f} eV
- Coordinaci√≥n promedio: {stats['mean_coord']:.2f}
- Rango energ√≠a: [{stats['min_energy']:.4f}, {stats['max_energy']:.4f}]
- Rango coordinaci√≥n: [{stats['min_coord']:.0f}, {stats['max_coord']:.0f}]

CALIDAD DE PREDICCI√ìN:
{"‚úì Excelente" if error < 5 else "‚ö† Aceptable" if error < 10 else "‚úó Revisar modelo"}
"""
                
                self.prediction_text.delete(1.0, tk.END)
                self.prediction_text.insert(1.0, result_text)
                
            except Exception as e:
                self.log_message(f"Error en predicci√≥n: {str(e)}")
                messagebox.showerror("Error", f"Error en predicci√≥n: {str(e)}")
        
        threading.Thread(target=predict_thread, daemon=True).start()
    
    def predict_batch(self):
        """Predecir lote de archivos"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "Primero carga un modelo entrenado")
            return
        
        if not self.batch_dir_var.get():
            messagebox.showwarning("Advertencia", "Selecciona un directorio")
            return
        
        def predict_batch_thread():
            try:
                batch_path = Path(self.batch_dir_var.get())
                dump_files = list(batch_path.glob("*.dump"))
                
                if not dump_files:
                    self.log_message("No se encontraron archivos .dump")
                    return
                
                self.log_message(f"Prediciendo {len(dump_files)} archivos...")
                
                results = []
                feature_columns = [col for col in self.current_data.columns 
                                 if col not in ['filename', 'vacancies']]
                
                for i, dump_file in enumerate(dump_files, 1):
                    try:
                        self.log_message(f"Procesando ({i}/{len(dump_files)}): {dump_file.name}")
                        
                        # Similar al proceso individual
                        n_atoms = self.extract_atoms_from_lammps_file(dump_file)
                        if n_atoms is None:
                            continue
                        
                        dump_data = pd.read_csv(dump_file, sep=' ', skiprows=9, 
                                              names=['id', 'type', 'x', 'y', 'z', 'c_peatom', 
                                                    'c_satom1', 'c_satom2', 'c_satom3', 'c_satom4', 
                                                    'c_satom5', 'c_satom6', 'c_coord', 'c_voro1', 'c_keatom'])
                        
                        stats = {
                            'n_atoms': n_atoms,
                            'mean_energy': dump_data['c_peatom'].mean(),
                            'std_energy': dump_data['c_peatom'].std(),
                            'mean_coord': dump_data['c_coord'].mean(),
                            'std_coord': dump_data['c_coord'].std(),
                            'min_energy': dump_data['c_peatom'].min(),
                            'max_energy': dump_data['c_peatom'].max(),
                            'min_coord': dump_data['c_coord'].min(),
                            'max_coord': dump_data['c_coord'].max()
                        }
                        
                        hist_features = self.create_histograms(dump_data)
                        stats.update(hist_features)
                        
                        # Asegurar features completas
                        for col in feature_columns:
                            if col not in stats:
                                stats[col] = 0.0
                        
                        X_pred = pd.DataFrame([stats])[feature_columns]
                        prediction = self.trained_model.predict(X_pred)[0]
                        real_vacancies = self.atm_total_var.get() - n_atoms
                        
                        results.append({
                            'filename': dump_file.name,
                            'n_atoms': n_atoms,
                            'real_vacancies': real_vacancies,
                            'predicted_vacancies': prediction,
                            'error': abs(prediction - real_vacancies),
                            'rel_error_pct': abs(prediction - real_vacancies) / real_vacancies * 100 if real_vacancies > 0 else 0,
                            'mean_energy': stats['mean_energy'],
                            'mean_coord': stats['mean_coord']
                        })
                        
                    except Exception as e:
                        self.log_message(f"Error procesando {dump_file.name}: {str(e)}")
                        continue
                
                if results:
                    # Crear DataFrame con resultados
                    results_df = pd.DataFrame(results)
                    
                    # Guardar resultados
                    output_file = Path(self.output_dir_var.get()) / "batch_predictions.csv"
                    Path(self.output_dir_var.get()).mkdir(exist_ok=True)
                    results_df.to_csv(output_file, index=False)
                    
                    # Calcular estad√≠sticas
                    mean_error = results_df['error'].mean()
                    median_error = results_df['error'].median()
                    max_error = results_df['error'].max()
                    min_error = results_df['error'].min()
                    mean_rel_error = results_df['rel_error_pct'].mean()
                    
                    # Contar predicciones por calidad
                    excellent = len(results_df[results_df['error'] < 5])
                    good = len(results_df[(results_df['error'] >= 5) & (results_df['error'] < 10)])
                    poor = len(results_df[results_df['error'] >= 10])
                    
                    batch_text = f"""
=== PREDICCI√ìN POR LOTES ===
Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Archivos procesados: {len(results)}
Archivo de resultados: {output_file}

ESTAD√çSTICAS DE ERROR:
- Error promedio: {mean_error:.2f} vacancias
- Error mediano: {median_error:.2f} vacancias
- Error m√≠nimo: {min_error:.2f} vacancias
- Error m√°ximo: {max_error:.2f} vacancias
- Error relativo promedio: {mean_rel_error:.1f}%

CALIDAD DE PREDICCIONES:
- Excelentes (error < 5): {excellent} ({excellent/len(results)*100:.1f}%)
- Buenas (error 5-10): {good} ({good/len(results)*100:.1f}%)
- Revisar (error > 10): {poor} ({poor/len(results)*100:.1f}%)

PRIMEROS 15 RESULTADOS:
{"Archivo":<25} {"Real":<6} {"Pred":<6} {"Error":<6} {"Rel%":<6}
{"-"*55}
"""
                    
                    for i, row in results_df.head(15).iterrows():
                        filename = row['filename'][:22] + "..." if len(row['filename']) > 25 else row['filename']
                        batch_text += f"{filename:<25} {row['real_vacancies']:<6.0f} {row['predicted_vacancies']:<6.1f} {row['error']:<6.1f} {row['rel_error_pct']:<6.1f}\n"
                    
                    if len(results_df) > 15:
                        batch_text += f"\n... y {len(results_df) - 15} archivos m√°s (ver CSV completo)"
                    
                    self.prediction_text.delete(1.0, tk.END)
                    self.prediction_text.insert(1.0, batch_text)
                    
                    self.log_message(f"Predicci√≥n por lotes completada: {len(results)} archivos")
                    messagebox.showinfo("Completado", 
                                       f"Predicci√≥n completada!\n\n"
                                       f"Archivos procesados: {len(results)}\n"
                                       f"Error promedio: {mean_error:.2f} vacancias\n"
                                       f"Resultados guardados en: {output_file}")
                
            except Exception as e:
                self.log_message(f"Error en predicci√≥n por lotes: {str(e)}")
                messagebox.showerror("Error", f"Error en predicci√≥n por lotes: {str(e)}")
        
        threading.Thread(target=predict_batch_thread, daemon=True).start()
    
    # =============================================================================
    # M√âTODOS DE VISUALIZACI√ìN
    # =============================================================================
    
    def clear_plots(self):
        """Limpiar todos los gr√°ficos"""
        for ax in self.axes.flat:
            ax.clear()
        self.canvas.draw()
    
    def plot_distributions(self):
        """Mostrar distribuciones de los datos"""
        if self.current_data is None:
            messagebox.showwarning("Advertencia", "Primero carga los datos")
            return
        
        # Limpiar plots anteriores
        self.clear_plots()
        
        try:
            # Plot 1: Distribuci√≥n de vacancias
            self.axes[0,0].hist(self.current_data['vacancies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            self.axes[0,0].set_title('Distribuci√≥n de Vacancias')
            self.axes[0,0].set_xlabel('N√∫mero de Vacancias')
            self.axes[0,0].set_ylabel('Frecuencia')
            self.axes[0,0].grid(True, alpha=0.3)
            
            # Plot 2: Distribuci√≥n de energ√≠a promedio
            if 'mean_energy' in self.current_data.columns:
                self.axes[0,1].hist(self.current_data['mean_energy'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
                self.axes[0,1].set_title('Distribuci√≥n de Energ√≠a Promedio')
                self.axes[0,1].set_xlabel('Energ√≠a Promedio (eV)')
                self.axes[0,1].set_ylabel('Frecuencia')
                self.axes[0,1].grid(True, alpha=0.3)
            
            # Plot 3: Distribuci√≥n de coordinaci√≥n promedio
            if 'mean_coord' in self.current_data.columns:
                self.axes[1,0].hist(self.current_data['mean_coord'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
                self.axes[1,0].set_title('Distribuci√≥n de Coordinaci√≥n Promedio')
                self.axes[1,0].set_xlabel('Coordinaci√≥n Promedio')
                self.axes[1,0].set_ylabel('Frecuencia')
                self.axes[1,0].grid(True, alpha=0.3)
            
            # Plot 4: Scatter plot energ√≠a vs vacancias
            if 'mean_energy' in self.current_data.columns:
                scatter = self.axes[1,1].scatter(self.current_data['mean_energy'], self.current_data['vacancies'], 
                                               alpha=0.6, color='purple', s=30)
                self.axes[1,1].set_title('Energ√≠a vs Vacancias')
                self.axes[1,1].set_xlabel('Energ√≠a Promedio (eV)')
                self.axes[1,1].set_ylabel('N√∫mero de Vacancias')
                self.axes[1,1].grid(True, alpha=0.3)
                
                # A√±adir l√≠nea de tendencia
                z = np.polyfit(self.current_data['mean_energy'], self.current_data['vacancies'], 1)
                p = np.poly1d(z)
                self.axes[1,1].plot(self.current_data['mean_energy'], p(self.current_data['mean_energy']), 
                                   "r--", alpha=0.8, linewidth=2)
            
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            messagebox.showerror("Error", f"Error creando distribuciones: {str(e)}")
    
    def plot_feature_importance(self):
        """Mostrar importancia de features"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "Primero entrena el modelo")
            return
        
        self.clear_plots()
        
        try:
            # Obtener importancias
            regressor = self.trained_model.named_steps['regressor']
            feature_names = [col for col in self.current_data.columns 
                           if col not in ['filename', 'vacancies']]
            
            importances = regressor.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            # Top 20 features
            top_n = min(20, len(importances))
            
            # Plot 1: Barras horizontales (top 10)
            top_10_idx = indices[:10]
            top_10_names = [feature_names[i][:20] for i in top_10_idx]  # Truncar nombres largos
            top_10_scores = importances[top_10_idx]
            
            y_pos = np.arange(len(top_10_names))
            self.axes[0,0].barh(y_pos, top_10_scores, color='steelblue', alpha=0.7)
            self.axes[0,0].set_yticks(y_pos)
            self.axes[0,0].set_yticklabels(top_10_names, fontsize=8)
            self.axes[0,0].set_title('Top 10 Features M√°s Importantes')
            self.axes[0,0].set_xlabel('Importancia')
            self.axes[0,0].grid(True, alpha=0.3)
            
            # Plot 2: Importancia acumulada
            cumsum = np.cumsum(importances[indices])
            self.axes[0,1].plot(range(1, len(cumsum)+1), cumsum, 'b-', linewidth=2, marker='o', markersize=2)
            self.axes[0,1].axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% varianza')
            self.axes[0,1].axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% varianza')
            self.axes[0,1].set_title('Importancia Acumulada')
            self.axes[0,1].set_xlabel('N√∫mero de Features')
            self.axes[0,1].set_ylabel('Importancia Acumulada')
            self.axes[0,1].legend()
            self.axes[0,1].grid(True, alpha=0.3)
            
            # Plot 3: Histograma de importancias
            self.axes[1,0].hist(importances, bins=20, alpha=0.7, color='green', edgecolor='black')
            self.axes[1,0].set_title('Distribuci√≥n de Importancias')
            self.axes[1,0].set_xlabel('Importancia')
            self.axes[1,0].set_ylabel('N√∫mero de Features')
            self.axes[1,0].grid(True, alpha=0.3)
            
            # Plot 4: Features por categor√≠a
            categories = ['coord', 'energy', 'stress', 'voro', 'n_atoms', 'other']
            cat_importance = {cat: 0 for cat in categories}
            
            for i, name in enumerate(feature_names):
                importance = importances[i]
                if 'coord' in name.lower():
                    cat_importance['coord'] += importance
                elif 'energy' in name.lower() or 'peatom' in name.lower():
                    cat_importance['energy'] += importance
                elif 'stress' in name.lower() or 'satom' in name.lower():
                    cat_importance['stress'] += importance
                elif 'voro' in name.lower():
                    cat_importance['voro'] += importance
                elif 'n_atoms' in name.lower():
                    cat_importance['n_atoms'] += importance
                else:
                    cat_importance['other'] += importance
            
            # Filtrar categor√≠as con importancia > 0
            cats_with_data = {k: v for k, v in cat_importance.items() if v > 0}
            
            if cats_with_data:
                wedges, texts, autotexts = self.axes[1,1].pie(cats_with_data.values(), 
                                                            labels=cats_with_data.keys(), 
                                                            autopct='%1.1f%%',
                                                            startangle=90)
                self.axes[1,1].set_title('Importancia por Categor√≠a')
            
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            messagebox.showerror("Error", f"Error mostrando feature importance: {str(e)}")
    
    def plot_predictions(self):
        """Mostrar predicciones vs valores reales"""
        if self.trained_model is None or self.current_data is None:
            messagebox.showwarning("Advertencia", "Primero entrena el modelo")
            return
        
        self.clear_plots()
        
        try:
            # Preparar datos
            feature_columns = [col for col in self.current_data.columns 
                             if col not in ['filename', 'vacancies']]
            
            X = self.current_data[feature_columns]
            y = self.current_data['vacancies']
            
            # Hacer predicciones
            predictions = self.trained_model.predict(X)
            residuals = y - predictions
            
            # Plot 1: Predicciones vs reales
            self.axes[0,0].scatter(y, predictions, alpha=0.6, color='blue', s=30)
            
            # L√≠nea diagonal perfecta
            min_val = min(y.min(), predictions.min())
            max_val = max(y.max(), predictions.max())
            self.axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n perfecta')
            
            self.axes[0,0].set_xlabel('Vacancias Reales')
            self.axes[0,0].set_ylabel('Vacancias Predichas')
            self.axes[0,0].set_title('Predicciones vs Valores Reales')
            self.axes[0,0].legend()
            self.axes[0,0].grid(True, alpha=0.3)
            
            # Calcular y mostrar R¬≤
            r2 = r2_score(y, predictions)
            self.axes[0,0].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=self.axes[0,0].transAxes, 
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            # Plot 2: Histograma de residuos
            self.axes[0,1].hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            self.axes[0,1].axvline(x=0, color='red', linestyle='--', linewidth=2)
            self.axes[0,1].set_xlabel('Residuos (Real - Predicho)')
            self.axes[0,1].set_ylabel('Frecuencia')
            self.axes[0,1].set_title('Distribuci√≥n de Residuos')
            self.axes[0,1].grid(True, alpha=0.3)
            
            # Plot 3: Residuos vs predicciones
            self.axes[1,0].scatter(predictions, residuals, alpha=0.6, color='green', s=30)
            self.axes[1,0].axhline(y=0, color='red', linestyle='--', linewidth=2)
            self.axes[1,0].set_xlabel('Predicciones')
            self.axes[1,0].set_ylabel('Residuos')
            self.axes[1,0].set_title('Residuos vs Predicciones')
            self.axes[1,0].grid(True, alpha=0.3)
            
            # Plot 4: Errores absolutos
            abs_errors = np.abs(residuals)
            self.axes[1,1].scatter(y, abs_errors, alpha=0.6, color='orange', s=30)
            self.axes[1,1].set_xlabel('Vacancias Reales')
            self.axes[1,1].set_ylabel('Error Absoluto')
            self.axes[1,1].set_title('Error Absoluto vs Valores Reales')
            self.axes[1,1].grid(True, alpha=0.3)
            
            # A√±adir estad√≠sticas de error
            mae = mean_absolute_error(y, predictions)
            rmse = np.sqrt(mean_squared_error(y, predictions))
            self.axes[1,1].text(0.05, 0.95, f'MAE = {mae:.2f}\nRMSE = {rmse:.2f}', 
                               transform=self.axes[1,1].transAxes,
                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            messagebox.showerror("Error", f"Error mostrando predicciones: {str(e)}")
    
    def reset(self):
        """Reset completo del tab"""
        # Reset variables
        self.atm_total_var.set(16384)
        self.energy_min_var.set(-4.0)
        self.energy_max_var.set(-2.0)
        self.input_dir_var.set("")
        self.output_dir_var.set("./ml_results")
        self.n_estimators_var.set(100)
        self.test_size_var.set(0.2)
        self.random_state_var.set(42)
        self.single_file_var.set("")
        self.batch_dir_var.set("")
        
        # Reset estado
        self.current_data = None
        self.trained_model = None
        self.processing = False
        
        # Reset displays
        self.status_text.delete(1.0, tk.END)
        self.status_text.insert(1.0, "Tab reiniciado. Listo para procesar datos.")
        
        self.metrics_text.delete(1.0, tk.END)
        self.prediction_text.delete(1.0, tk.END)
        
        # Reset botones
        self.process_btn.config(state="normal")
        self.train_btn.config(state="normal")
        self.cv_btn.config(state="disabled")
        self.save_model_btn.config(state="disabled")
        
        # Reset gr√°ficos
        self.clear_plots()
        
        self.log_message("Advanced ML Tab reiniciado")
"""
Advanced ML Tab con visualizaciones integradas
Incluye gr√°ficos de predicciones vs reales, residuos, feature importance
"""
"""
Advanced ML Tab con visualizaciones integradas - VERSI√ìN COMPLETA
Incluye gr√°ficos de predicciones vs reales, residuos normalizados, feature importance
"""
"""
Advanced ML Tab con visualizaciones integradas - VERSI√ìN COMPLETA
Incluye gr√°ficos de predicciones vs reales, residuos normalizados, feature importance
"""

import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import seaborn as sns

class AdvancedMLTabWithPlots:
    """Advanced ML Tab con visualizaciones integradas"""
    
    def __init__(self, parent, data_loaded_callback):
        self.parent = parent
        self.data_loaded_callback = data_loaded_callback
        
        self.frame = ttk.Frame(parent)
        
        # Variables de entrenamiento
        self.n_estimators_var = tk.IntVar(value=100)
        self.test_size_var = tk.DoubleVar(value=0.2)
        self.random_state_var = tk.IntVar(value=42)
        
        # Estado actual
        self.current_data = None
        self.trained_model = None
        self.feature_columns = []
        self.target_column = 'vacancies'
        self.X_test = None
        self.y_test = None
        self.test_predictions = None
        
        self.create_widgets()
    
    def create_widgets(self):
        """Crear widgets del tab"""
        main_container = ttk.Frame(self.frame, padding="10")
        main_container.pack(fill="both", expand=True)
        
        # Crear notebook para sub-tabs
        self.notebook = ttk.Notebook(main_container)
        self.notebook.pack(fill="both", expand=True)
        
        self.create_data_tab()
        self.create_training_tab()
        self.create_results_tab()
        self.create_feature_analysis_tab()
        self.create_prediction_tab()
    
    def create_data_tab(self):
        """Tab de carga y exploraci√≥n de datos"""
        data_frame = ttk.Frame(self.notebook)
        self.notebook.add(data_frame, text="üìä Datos")
        
        # Secci√≥n de carga
        load_frame = ttk.LabelFrame(data_frame, text="Cargar Dataset", padding="10")
        load_frame.pack(fill="x", padx=10, pady=5)
        
        ttk.Button(load_frame, text="Cargar CSV/Excel", 
                  command=self.load_dataset).pack(side="left", padx=5)
        
        # Informaci√≥n del dataset
        info_frame = ttk.LabelFrame(data_frame, text="Informaci√≥n del Dataset", padding="10")
        info_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        self.info_text = scrolledtext.ScrolledText(info_frame, height=20, wrap='word')
        self.info_text.pack(fill="both", expand=True)
    
    def create_training_tab(self):
        """Tab de entrenamiento"""
        train_frame = ttk.Frame(self.notebook)
        self.notebook.add(train_frame, text="ü§ñ Entrenamiento")
        
        # Panel izquierdo - Controles
        left_panel = ttk.Frame(train_frame)
        left_panel.pack(side="left", fill="y", padx=(10, 5))
        
        # Par√°metros
        params_group = ttk.LabelFrame(left_panel, text="Par√°metros Random Forest", padding="10")
        params_group.pack(fill='x', pady=(0, 10))
        
        ttk.Label(params_group, text="N¬∞ Estimadores:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Spinbox(params_group, from_=50, to=500, textvariable=self.n_estimators_var, width=10).grid(row=0, column=1, padx=5, pady=2)
        
        ttk.Label(params_group, text="Test Size:").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(params_group, textvariable=self.test_size_var, width=10).grid(row=1, column=1, padx=5, pady=2)
        
        ttk.Label(params_group, text="Random State:").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(params_group, textvariable=self.random_state_var, width=10).grid(row=2, column=1, padx=5, pady=2)
        
        # Botones
        buttons_group = ttk.LabelFrame(left_panel, text="Acciones", padding="10")
        buttons_group.pack(fill='x', pady=(0, 10))
        
        self.train_btn = ttk.Button(buttons_group, text="Entrenar Modelo", 
                                   command=self.train_model)
        self.train_btn.pack(fill='x', pady=2)
        
        self.save_btn = ttk.Button(buttons_group, text="Guardar Modelo", 
                                  command=self.save_model, state="disabled")
        self.save_btn.pack(fill='x', pady=2)
        
        ttk.Button(buttons_group, text="Cargar Modelo", 
                  command=self.load_model).pack(fill='x', pady=2)
        
        # Panel derecho - Resultados de texto
        right_panel = ttk.Frame(train_frame)
        right_panel.pack(side="right", fill="both", expand=True, padx=(5, 10))
        
        results_group = ttk.LabelFrame(right_panel, text="M√©tricas del Entrenamiento", padding="10")
        results_group.pack(fill='both', expand=True)
        
        self.results_text = scrolledtext.ScrolledText(results_group, height=20, wrap='word')
        self.results_text.pack(fill='both', expand=True)
    
    def create_results_tab(self):
        """Tab de resultados con visualizaciones"""
        results_frame = ttk.Frame(self.notebook)
        self.notebook.add(results_frame, text="üìä Resultados")
        
        # Botones de control de gr√°ficos
        control_frame = ttk.Frame(results_frame)
        control_frame.pack(fill="x", padx=10, pady=5)
        
        ttk.Button(control_frame, text="Actualizar Gr√°ficos", 
                  command=self.update_plots).pack(side="left", padx=5)
        ttk.Button(control_frame, text="Guardar Gr√°ficos", 
                  command=self.save_plots).pack(side="left", padx=5)
        
        # Frame para los gr√°ficos
        self.plots_frame = ttk.Frame(results_frame)
        self.plots_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        # Crear figura de matplotlib
        self.fig, self.axes = plt.subplots(2, 3, figsize=(18, 12))
        self.fig.tight_layout(pad=3.0)
        
        # Canvas para mostrar los gr√°ficos
        self.canvas = FigureCanvasTkAgg(self.fig, self.plots_frame)
        self.canvas.get_tk_widget().pack(fill="both", expand=True)
    
    def create_feature_analysis_tab(self):
        """Tab de an√°lisis detallado de features"""
        analysis_frame = ttk.Frame(self.notebook)
        self.notebook.add(analysis_frame, text="üîç An√°lisis Features")
        
        # Botones de control
        control_frame = ttk.Frame(analysis_frame)
        control_frame.pack(fill="x", padx=10, pady=5)
        
        ttk.Button(control_frame, text="Actualizar An√°lisis", 
                  command=self.update_feature_analysis).pack(side="left", padx=5)
        ttk.Button(control_frame, text="Exportar Feature Importance", 
                  command=self.export_feature_importance).pack(side="left", padx=5)
        ttk.Button(control_frame, text="Exportar Modelo RF", 
                  command=self.export_rf_model).pack(side="left", padx=5)
        
        # Crear notebook interno para diferentes an√°lisis
        self.analysis_notebook = ttk.Notebook(analysis_frame)
        self.analysis_notebook.pack(fill="both", expand=True, padx=10, pady=5)
        
        # Sub-tab 1: Feature Importance
        self.create_importance_subtab()
        
        # Sub-tab 2: Correlaci√≥n de Features
        self.create_correlation_subtab()
        
        # Sub-tab 3: M√©tricas del Modelo
        self.create_model_metrics_subtab()
    
    def create_importance_subtab(self):
        """Sub-tab de importancia de features"""
        importance_frame = ttk.Frame(self.analysis_notebook)
        self.analysis_notebook.add(importance_frame, text="Feature Importance")
        
        # √Årea de gr√°fico de importancia
        self.importance_fig, self.importance_ax = plt.subplots(figsize=(12, 8))
        self.importance_canvas = FigureCanvasTkAgg(self.importance_fig, importance_frame)
        self.importance_canvas.get_tk_widget().pack(fill="both", expand=True, padx=5, pady=5)
        
        # Tabla de importancia
        table_frame = ttk.LabelFrame(importance_frame, text="Top Features", padding="5")
        table_frame.pack(fill="x", padx=5, pady=5)
        
        # Crear tabla con scrollbar
        table_container = ttk.Frame(table_frame)
        table_container.pack(fill="both", expand=True)
        
        # Columnas de la tabla
        columns = ('Rank', 'Feature', 'Importance', 'Cumulative %')
        self.importance_tree = ttk.Treeview(table_container, columns=columns, show='headings', height=8)
        
        # Configurar encabezados
        self.importance_tree.heading('Rank', text='#')
        self.importance_tree.heading('Feature', text='Feature')
        self.importance_tree.heading('Importance', text='Importancia')
        self.importance_tree.heading('Cumulative %', text='Acumulado %')
        
        # Configurar anchos
        self.importance_tree.column('Rank', width=50, anchor='center')
        self.importance_tree.column('Feature', width=300, anchor='w')
        self.importance_tree.column('Importance', width=100, anchor='center')
        self.importance_tree.column('Cumulative %', width=100, anchor='center')
        
        # Scrollbar para tabla
        importance_scrollbar = ttk.Scrollbar(table_container, orient="vertical", 
                                           command=self.importance_tree.yview)
        self.importance_tree.configure(yscrollcommand=importance_scrollbar.set)
        
        self.importance_tree.pack(side="left", fill="both", expand=True)
        importance_scrollbar.pack(side="right", fill="y")
    
    def create_correlation_subtab(self):
        """Sub-tab de correlaci√≥n entre features"""
        correlation_frame = ttk.Frame(self.analysis_notebook)
        self.analysis_notebook.add(correlation_frame, text="Correlaciones")
        
        # √Årea de texto para correlaciones
        corr_text_frame = ttk.LabelFrame(correlation_frame, text="Correlaciones con Target", padding="10")
        corr_text_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        self.correlation_text = scrolledtext.ScrolledText(corr_text_frame, height=20, wrap='word')
        self.correlation_text.pack(fill="both", expand=True)
    
    def create_model_metrics_subtab(self):
        """Sub-tab de m√©tricas detalladas del modelo"""
        metrics_frame = ttk.Frame(self.analysis_notebook)
        self.analysis_notebook.add(metrics_frame, text="M√©tricas del Modelo")
        
        # √Årea de m√©tricas detalladas
        detailed_metrics_frame = ttk.LabelFrame(metrics_frame, text="M√©tricas Detalladas Random Forest", padding="10")
        detailed_metrics_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        self.detailed_metrics_text = scrolledtext.ScrolledText(detailed_metrics_frame, height=20, wrap='word')
        self.detailed_metrics_text.pack(fill="both", expand=True)
    
    def create_prediction_tab(self):
        """Tab de predicci√≥n"""
        pred_frame = ttk.Frame(self.notebook)
        self.notebook.add(pred_frame, text="üîÆ Predicci√≥n")
        
        # Predicci√≥n individual
        single_group = ttk.LabelFrame(pred_frame, text="Predicci√≥n Individual", padding="10")
        single_group.pack(fill='x', padx=10, pady=5)
        
        ttk.Label(single_group, text="Ingrese valores para las features principales:").pack(anchor='w')
        
        # Frame para inputs de features
        self.features_frame = ttk.Frame(single_group)
        self.features_frame.pack(fill='x', pady=10)
        
        ttk.Button(single_group, text="Predecir", command=self.predict_single).pack()
        
        # Resultados de predicci√≥n
        pred_results_group = ttk.LabelFrame(pred_frame, text="Resultados", padding="10")
        pred_results_group.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.prediction_text = scrolledtext.ScrolledText(pred_results_group, height=15, wrap='word')
        self.prediction_text.pack(fill='both', expand=True)
    
    def load_dataset(self):
        """Cargar dataset desde archivo"""
        file_path = filedialog.askopenfilename(
            title="Cargar Dataset",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if file_path:
            try:
                if file_path.endswith('.xlsx'):
                    data = pd.read_excel(file_path)
                else:
                    data = pd.read_csv(file_path, index_col=0)
                
                self.load_dataset_from_dataframe(data)
                
            except Exception as e:
                messagebox.showerror("Error", f"Error cargando dataset:\n{str(e)}")
    
    def load_dataset_from_dataframe(self, data):
        """Cargar dataset desde un DataFrame"""
        try:
            self.current_data = data.copy()
            
            # Identificar columnas de texto y num√©ricas
            text_columns = []
            numeric_columns = []
            
            for col in data.columns:
                if data[col].dtype == 'object' or data[col].dtype.name == 'string':
                    text_columns.append(col)
                else:
                    numeric_columns.append(col)
            
            # Verificar que existe la columna target
            if self.target_column not in data.columns:
                raise ValueError(f"No se encontr√≥ la columna target '{self.target_column}' en el dataset")
            
            # Definir feature columns (num√©ricas, excluyendo target)
            self.feature_columns = [col for col in numeric_columns if col != self.target_column]
            
            if not self.feature_columns:
                raise ValueError("No se encontraron columnas num√©ricas para usar como features")
            
            # Actualizar informaci√≥n
            self.update_dataset_info(text_columns)
            
            # Notificar al callback
            self.data_loaded_callback(data)
            
            # Actualizar interfaz de predicci√≥n
            self.update_prediction_interface()
            
            messagebox.showinfo("√âxito", 
                               f"Dataset cargado exitosamente!\n\n"
                               f"Filas: {len(data)}\n"
                               f"Features num√©ricas: {len(self.feature_columns)}\n"
                               f"Columnas de texto excluidas: {len(text_columns)}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Error procesando dataset:\n{str(e)}")
    
    def update_dataset_info(self, text_columns):
        """Actualizar informaci√≥n del dataset"""
        if self.current_data is None:
            return
        
        info_lines = [
            "INFORMACI√ìN DEL DATASET",
            "=" * 40,
            f"Filas: {len(self.current_data)}",
            f"Columnas totales: {len(self.current_data.columns)}",
            "",
            f"TARGET COLUMN: {self.target_column}",
            f"Valores √∫nicos de target: {self.current_data[self.target_column].nunique()}",
            f"Rango de target: {self.current_data[self.target_column].min()} - {self.current_data[self.target_column].max()}",
            "",
            f"FEATURE COLUMNS ({len(self.feature_columns)}):",
            "-" * 20
        ]
        
        # Mostrar primeras 20 features
        for i, col in enumerate(self.feature_columns[:20]):
            info_lines.append(f"  {i+1:2d}. {col}")
        
        if len(self.feature_columns) > 20:
            info_lines.append(f"  ... y {len(self.feature_columns) - 20} m√°s")
        
        if text_columns:
            info_lines.extend([
                "",
                f"COLUMNAS DE TEXTO EXCLUIDAS ({len(text_columns)}):",
                "-" * 25
            ])
            for col in text_columns:
                info_lines.append(f"  ‚Ä¢ {col}")
        
        # Estad√≠sticas b√°sicas del target
        target_stats = self.current_data[self.target_column].describe()
        info_lines.extend([
            "",
            "ESTAD√çSTICAS DEL TARGET:",
            "-" * 25,
            f"  Media: {target_stats['mean']:.2f}",
            f"  Mediana: {target_stats['50%']:.2f}",
            f"  Desv. est√°ndar: {target_stats['std']:.2f}",
            f"  Min: {target_stats['min']:.0f}",
            f"  Max: {target_stats['max']:.0f}"
        ])
        
        self.info_text.delete(1.0, tk.END)
        self.info_text.insert(1.0, "\n".join(info_lines))
    
    def train_model(self):
        """Entrenar modelo Random Forest"""
        if self.current_data is None:
            messagebox.showwarning("Advertencia", "Primero carga un dataset")
            return
        
        try:
            self.train_btn.config(state="disabled")
            
            # Preparar datos
            X = self.current_data[self.feature_columns]
            y = self.current_data[self.target_column]
            
            # Verificar que no hay valores NaN
            if X.isnull().any().any():
                from sklearn.impute import SimpleImputer
                imputer = SimpleImputer(strategy='median')
                X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)
            else:
                X_clean = X
            
            # Divisi√≥n train/test
            from sklearn.model_selection import train_test_split
            X_train, self.X_test, y_train, self.y_test = train_test_split(
                X_clean, y, 
                test_size=self.test_size_var.get(),
                random_state=self.random_state_var.get()
            )
            
            # Crear y entrenar modelo
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
            
            self.trained_model = RandomForestRegressor(
                n_estimators=self.n_estimators_var.get(),
                random_state=self.random_state_var.get(),
                n_jobs=-1
            )
            
            self.trained_model.fit(X_train, y_train)
            
            # Evaluar modelo
            train_pred = self.trained_model.predict(X_train)
            self.test_predictions = self.trained_model.predict(self.X_test)
            
            train_mae = mean_absolute_error(y_train, train_pred)
            test_mae = mean_absolute_error(self.y_test, self.test_predictions)
            train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
            test_rmse = np.sqrt(mean_squared_error(self.y_test, self.test_predictions))
            train_r2 = r2_score(y_train, train_pred)
            test_r2 = r2_score(self.y_test, self.test_predictions)
            
            # Validaci√≥n cruzada
            from sklearn.model_selection import cross_val_score
            cv_mae = -cross_val_score(self.trained_model, X_clean, y, cv=5, scoring='neg_mean_absolute_error')
            cv_r2 = cross_val_score(self.trained_model, X_clean, y, cv=5, scoring='r2')
            
            # Feature importance
            self.feature_importance = pd.DataFrame({
                'feature': self.feature_columns,
                'importance': self.trained_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            # Mostrar resultados de texto
            results_text = f"""RESULTADOS DEL ENTRENAMIENTO
===========================

CONFIGURACI√ìN:
  Random Forest con {self.n_estimators_var.get()} estimadores
  Test size: {self.test_size_var.get():.1%}
  Features utilizadas: {len(self.feature_columns)}
  Muestras de entrenamiento: {len(X_train)}
  Muestras de prueba: {len(self.X_test)}

M√âTRICAS DE RENDIMIENTO:
  Train MAE:  {train_mae:.3f}
  Test MAE:   {test_mae:.3f}
  Train RMSE: {train_rmse:.3f}
  Test RMSE:  {test_rmse:.3f}
  Train R¬≤:   {train_r2:.3f}
  Test R¬≤:    {test_r2:.3f}

VALIDACI√ìN CRUZADA (5-fold):
  CV MAE:  {cv_mae.mean():.3f} ¬± {cv_mae.std():.3f}
  CV R¬≤:   {cv_r2.mean():.3f} ¬± {cv_r2.std():.3f}

TOP 15 FEATURES M√ÅS IMPORTANTES:
"""
            
            for i, row in self.feature_importance.head(15).iterrows():
                results_text += f"  {row['feature'][:40]:40s}: {row['importance']:.4f}\n"
            
            results_text += f"""

INTERPRETACI√ìN:
  {'üü¢ Excelente' if test_r2 > 0.9 else 'üü° Bueno' if test_r2 > 0.7 else 'üî¥ Mejorable'} (R¬≤ = {test_r2:.3f})
  {'üü¢ Bajo error' if test_mae < 5 else 'üü° Error moderado' if test_mae < 10 else 'üî¥ Error alto'} (MAE = {test_mae:.1f})
"""
            
            self.results_text.delete(1.0, tk.END)
            self.results_text.insert(1.0, results_text)
            
            # Habilitar bot√≥n de guardado
            self.save_btn.config(state="normal")
            
            # Actualizar gr√°ficos
            self.update_plots()
            
            # Cambiar al tab de resultados para mostrar gr√°ficos
            self.notebook.select(2)  # Tab de Resultados
            
            # Actualizar interfaz de predicci√≥n con features importantes
            self.update_prediction_interface_with_importance()
            
            messagebox.showinfo("Entrenamiento Completado", 
                               f"Modelo entrenado exitosamente!\n\n"
                               f"Test R¬≤: {test_r2:.3f}\n"
                               f"Test MAE: {test_mae:.3f}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Error entrenando modelo:\n{str(e)}")
        finally:
            self.train_btn.config(state="normal")
    
    def update_feature_analysis(self):
        """Actualizar an√°lisis de features"""
        if self.trained_model is None or not hasattr(self, 'feature_importance'):
            messagebox.showwarning("Advertencia", "Primero entrena un modelo")
            return
        
        try:
            # Actualizar gr√°fico de importancia
            self.update_importance_plot()
            
            # Actualizar tabla de importancia
            self.update_importance_table()
            
            # Actualizar correlaciones
            self.update_correlations()
            
            # Actualizar m√©tricas detalladas
            self.update_detailed_metrics()
            
        except Exception as e:
            messagebox.showerror("Error", f"Error actualizando an√°lisis:\n{str(e)}")
    
    def update_importance_plot(self):
        """Actualizar gr√°fico de importancia de features"""
        self.importance_ax.clear()
        
        # Tomar top 20 features
        top_features = self.feature_importance.head(20)
        
        # Crear gr√°fico de barras horizontales
        bars = self.importance_ax.barh(range(len(top_features)), top_features['importance'], 
                                      color='steelblue', alpha=0.8)
        
        # Configurar ejes
        self.importance_ax.set_yticks(range(len(top_features)))
        self.importance_ax.set_yticklabels(top_features['feature'], fontsize=10)
        self.importance_ax.set_xlabel('Importancia', fontsize=12)
        self.importance_ax.set_title('Top 20 importancias de features (RF)', fontsize=14, fontweight='bold')
        
        # Invertir eje Y para mostrar m√°s importante arriba
        self.importance_ax.invert_yaxis()
        
        # Agregar valores en las barras
        for i, bar in enumerate(bars):
            width = bar.get_width()
            self.importance_ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                                  f'{width:.3f}', ha='left', va='center', fontsize=9)
        
        # Mejorar apariencia
        self.importance_ax.grid(axis='x', alpha=0.3)
        self.importance_ax.spines['top'].set_visible(False)
        self.importance_ax.spines['right'].set_visible(False)
        
        self.importance_fig.tight_layout()
        self.importance_canvas.draw()
    
    def update_importance_table(self):
        """Actualizar tabla de importancia"""
        # Limpiar tabla anterior
        for item in self.importance_tree.get_children():
            self.importance_tree.delete(item)
        
        # Calcular porcentaje acumulativo
        cumulative_importance = np.cumsum(self.feature_importance['importance'])
        total_importance = cumulative_importance.iloc[-1]
        cumulative_percent = (cumulative_importance / total_importance) * 100
        
        # Llenar tabla con todas las features
        for i, (_, row) in enumerate(self.feature_importance.iterrows()):
            self.importance_tree.insert('', 'end', values=(
                i + 1,
                row['feature'],
                f"{row['importance']:.4f}",
                f"{cumulative_percent.iloc[i]:.1f}%"
            ))
    
    def update_correlations(self):
        """Actualizar an√°lisis de correlaciones"""
        if self.current_data is None:
            return
        
        # Calcular correlaciones con el target
        correlations = self.current_data[self.feature_columns + [self.target_column]].corr()[self.target_column]
        correlations = correlations.drop(self.target_column).sort_values(key=abs, ascending=False)
        
        # Generar texto de correlaciones
        corr_text = """CORRELACIONES CON TARGET (VACANCIES)
=====================================

Las correlaciones muestran la relaci√≥n lineal entre cada feature y el target.
Valores cercanos a ¬±1 indican fuerte correlaci√≥n, cercanos a 0 indican poca correlaci√≥n.

TOP 20 CORRELACIONES M√ÅS ALTAS:
"""
        
        for i, (feature, corr) in enumerate(correlations.head(20).items()):
            direction = "Positiva" if corr > 0 else "Negativa"
            strength = "Muy fuerte" if abs(corr) > 0.8 else "Fuerte" if abs(corr) > 0.6 else "Moderada" if abs(corr) > 0.4 else "D√©bil"
            corr_text += f"{i+1:2d}. {feature[:40]:40s}: {corr:+.4f} ({direction}, {strength})\n"
        
        corr_text += "\n\nTOP 10 CORRELACIONES M√ÅS BAJAS (MENOS INFORMATIVAS):\n"
        
        for i, (feature, corr) in enumerate(correlations.tail(10).items()):
            corr_text += f"{i+1:2d}. {feature[:40]:40s}: {corr:+.4f}\n"
        
        # Estad√≠sticas de correlaciones
        corr_text += f"""

ESTAD√çSTICAS DE CORRELACIONES:
  Media absoluta: {abs(correlations).mean():.4f}
  Mediana absoluta: {abs(correlations).median():.4f}
  M√°xima correlaci√≥n: {correlations.max():.4f} ({correlations.idxmax()})
  M√≠nima correlaci√≥n: {correlations.min():.4f} ({correlations.idxmin()})
  
  Features con correlaci√≥n > 0.5: {sum(abs(correlations) > 0.5)}
  Features con correlaci√≥n > 0.7: {sum(abs(correlations) > 0.7)}
  Features con correlaci√≥n > 0.9: {sum(abs(correlations) > 0.9)}
"""
        
        self.correlation_text.delete(1.0, tk.END)
        self.correlation_text.insert(1.0, corr_text)
    
    def update_detailed_metrics(self):
        """Actualizar m√©tricas detalladas del modelo"""
        if self.trained_model is None:
            return
        
        # Obtener informaci√≥n detallada del Random Forest
        rf_info = f"""INFORMACI√ìN DETALLADA DEL RANDOM FOREST
======================================

CONFIGURACI√ìN DEL MODELO:
  N√∫mero de estimadores: {self.trained_model.n_estimators}
  Criterio de divisi√≥n: {self.trained_model.criterion}
  Profundidad m√°xima: {self.trained_model.max_depth if self.trained_model.max_depth else 'Sin l√≠mite'}
  Min muestras por split: {self.trained_model.min_samples_split}
  Min muestras por hoja: {self.trained_model.min_samples_leaf}
  Max features por split: {self.trained_model.max_features}
  Bootstrap: {'S√≠' if self.trained_model.bootstrap else 'No'}
  Random state: {self.trained_model.random_state}

ESTAD√çSTICAS DE LOS √ÅRBOLES:
"""
        
        if hasattr(self.trained_model, 'estimators_'):
            tree_depths = [tree.tree_.max_depth for tree in self.trained_model.estimators_]
            tree_nodes = [tree.tree_.node_count for tree in self.trained_model.estimators_]
            tree_leaves = [tree.tree_.n_leaves for tree in self.trained_model.estimators_]
            
            rf_info += f"""  Profundidad promedio: {np.mean(tree_depths):.1f} ¬± {np.std(tree_depths):.1f}
  Profundidad m√≠nima: {min(tree_depths)}
  Profundidad m√°xima: {max(tree_depths)}
  
  Nodos promedio por √°rbol: {np.mean(tree_nodes):.1f} ¬± {np.std(tree_nodes):.1f}
  Hojas promedio por √°rbol: {np.mean(tree_leaves):.1f} ¬± {np.std(tree_leaves):.1f}

AN√ÅLISIS DE FEATURES:
  Total features disponibles: {len(self.feature_columns)}
  Features con importancia > 0.01: {sum(self.feature_importance['importance'] > 0.01)}
  Features con importancia > 0.05: {sum(self.feature_importance['importance'] > 0.05)}
  Features con importancia > 0.10: {sum(self.feature_importance['importance'] > 0.10)}
  
  Top 5 features representan: {self.feature_importance.head(5)['importance'].sum():.1%} de la importancia
  Top 10 features representan: {self.feature_importance.head(10)['importance'].sum():.1%} de la importancia
  Top 20 features representan: {self.feature_importance.head(20)['importance'].sum():.1%} de la importancia
"""
        
        # Informaci√≥n de rendimiento si est√° disponible
        if self.X_test is not None and self.y_test is not None:
            # Calcular m√©tricas adicionales
            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score
            
            mae = mean_absolute_error(self.y_test, self.test_predictions)
            rmse = np.sqrt(mean_squared_error(self.y_test, self.test_predictions))
            r2 = r2_score(self.y_test, self.test_predictions)
            evs = explained_variance_score(self.y_test, self.test_predictions)
            
            # Error percentual absoluto medio (MAPE)
            mape = np.mean(np.abs((self.y_test - self.test_predictions) / self.y_test)) * 100
            
            # Error mediano absoluto
            median_ae = np.median(np.abs(self.y_test - self.test_predictions))
            
            rf_info += f"""

M√âTRICAS DE RENDIMIENTO DETALLADAS:
  R¬≤ Score: {r2:.6f}
  Explained Variance Score: {evs:.6f}
  Mean Absolute Error: {mae:.4f}
  Root Mean Squared Error: {rmse:.4f}
  Mean Absolute Percentage Error: {mape:.2f}%
  Median Absolute Error: {median_ae:.4f}
  
  Error est√°ndar: {np.std(self.y_test - self.test_predictions):.4f}
  Error m√≠nimo: {np.min(np.abs(self.y_test - self.test_predictions)):.4f}
  Error m√°ximo: {np.max(np.abs(self.y_test - self.test_predictions)):.4f}

DISTRIBUCI√ìN DE ERRORES:
  25% de predicciones tienen error < {np.percentile(np.abs(self.y_test - self.test_predictions), 25):.2f}
  50% de predicciones tienen error < {np.percentile(np.abs(self.y_test - self.test_predictions), 50):.2f}
  75% de predicciones tienen error < {np.percentile(np.abs(self.y_test - self.test_predictions), 75):.2f}
  90% de predicciones tienen error < {np.percentile(np.abs(self.y_test - self.test_predictions), 90):.2f}
  95% de predicciones tienen error < {np.percentile(np.abs(self.y_test - self.test_predictions), 95):.2f}
"""
        
        self.detailed_metrics_text.delete(1.0, tk.END)
        self.detailed_metrics_text.insert(1.0, rf_info)
    
    def export_feature_importance(self):
        """Exportar tabla de feature importance"""
        if not hasattr(self, 'feature_importance'):
            messagebox.showwarning("Advertencia", "No hay feature importance para exportar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Exportar Feature Importance",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if file_path:
            try:
                # A√±adir porcentaje acumulativo
                cumulative_importance = np.cumsum(self.feature_importance['importance'])
                total_importance = cumulative_importance.iloc[-1]
                cumulative_percent = (cumulative_importance / total_importance) * 100
                
                export_df = self.feature_importance.copy()
                export_df['cumulative_importance'] = cumulative_importance
                export_df['cumulative_percent'] = cumulative_percent
                export_df['rank'] = range(1, len(export_df) + 1)
                
                # Reordenar columnas
                export_df = export_df[['rank', 'feature', 'importance', 'cumulative_importance', 'cumulative_percent']]
                
                if file_path.endswith('.xlsx'):
                    export_df.to_excel(file_path, index=False)
                else:
                    export_df.to_csv(file_path, index=False)
                
                messagebox.showinfo("√âxito", f"Feature importance exportado a:\n{file_path}")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error exportando feature importance:\n{str(e)}")
    
    def export_rf_model(self):
        """Exportar modelo Random Forest completo con metadatos"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "No hay modelo entrenado para exportar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Exportar Modelo Random Forest",
            defaultextension=".joblib",
            filetypes=[("Joblib files", "*.joblib")]
        )
        
        if file_path:
            try:
                import joblib
                from datetime import datetime
                
                # Crear paquete completo del modelo
                model_package = {
                    'model': self.trained_model,
                    'feature_columns': self.feature_columns,
                    'target_column': self.target_column,
                    'feature_importance': self.feature_importance,
                    'training_params': {
                        'n_estimators': self.n_estimators_var.get(),
                        'test_size': self.test_size_var.get(),
                        'random_state': self.random_state_var.get()
                    },
                    'model_info': {
                        'n_features': len(self.feature_columns),
                        'n_samples_train': len(self.current_data) if self.current_data is not None else None,
                        'training_date': datetime.now().isoformat(),
                        'model_type': 'RandomForestRegressor',
                        'sklearn_version': None  # Se puede a√±adir si se importa sklearn
                    }
                }
                
                # A√±adir m√©tricas de rendimiento si est√°n disponibles
                if self.X_test is not None and self.y_test is not None:
                    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
                    
                    mae = mean_absolute_error(self.y_test, self.test_predictions)
                    rmse = np.sqrt(mean_squared_error(self.y_test, self.test_predictions))
                    r2 = r2_score(self.y_test, self.test_predictions)
                    
                    model_package['performance_metrics'] = {
                        'test_mae': mae,
                        'test_rmse': rmse,
                        'test_r2': r2,
                        'n_test_samples': len(self.X_test)
                    }
                
                # Guardar modelo
                joblib.dump(model_package, file_path)
                
                messagebox.showinfo("√âxito", 
                                   f"Modelo Random Forest exportado exitosamente!\n\n"
                                   f"Archivo: {file_path}\n"
                                   f"Incluye: modelo, features, importancia, m√©tricas\n"
                                   f"Features: {len(self.feature_columns)}\n"
                                   f"Estimadores: {self.trained_model.n_estimators}")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error exportando modelo:\n{str(e)}")
    
    def update_plots(self):
        """Actualizar todos los gr√°ficos con residuos normalizados"""
        if self.trained_model is None or self.X_test is None:
            # Limpiar gr√°ficos si no hay modelo
            for ax in self.axes.flat:
                ax.clear()
                ax.text(0.5, 0.5, 'Entrena un modelo\npara ver gr√°ficos', 
                       ha='center', va='center', transform=ax.transAxes, fontsize=12)
                ax.set_xticks([])
                ax.set_yticks([])
            self.canvas.draw()
            return
        
        try:
            # Limpiar gr√°ficos anteriores
            for ax in self.axes.flat:
                ax.clear()
            
            # Configurar estilo
            plt.style.use('default')
            sns.set_palette("husl")
            
            # CALCULAR RESIDUOS NORMALIZADOS: 1 - pred/real
            # Evitar divisi√≥n por cero
            y_test_safe = self.y_test.copy()
            y_test_safe[y_test_safe == 0] = 1e-10  # Reemplazar ceros con valor muy peque√±o
            
            normalized_residuals = 1 - (self.test_predictions / y_test_safe)
            
            # 1. Predicciones vs Valores Reales
            ax1 = self.axes[0, 0]
            ax1.scatter(self.y_test, self.test_predictions, alpha=0.6, color='red', s=50)
            
            # L√≠nea diagonal perfecta
            min_val = min(self.y_test.min(), self.test_predictions.min())
            max_val = max(self.y_test.max(), self.test_predictions.max())
            ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n perfecta')
            
            ax1.set_xlabel('Valores Reales (y_true)')
            ax1.set_ylabel('Predicciones (y_pred)')
            ax1.set_title('Predicciones vs Valores Reales')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Calcular y mostrar m√©tricas
            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
            r2 = r2_score(self.y_test, self.test_predictions)
            mae = mean_absolute_error(self.y_test, self.test_predictions)
            rmse = np.sqrt(mean_squared_error(self.y_test, self.test_predictions))
            
            ax1.text(0.05, 0.95, f'R¬≤ = {r2:.3f}\nMAE = {mae:.3f}\nRMSE = {rmse:.3f}', 
                    transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                    verticalalignment='top')
            
            # 2. Gr√°fico de Residuos NORMALIZADOS
            ax2 = self.axes[0, 1]
            ax2.scatter(self.test_predictions, normalized_residuals, alpha=0.6, color='red', s=50)
            ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
            ax2.set_xlabel('Valores Predichos (y_pred)')
            ax2.set_ylabel('Residuos Normalizados (1 - pred/real)')
            ax2.set_title('Gr√°fico de Residuos Normalizados')
            ax2.grid(True, alpha=0.3)
            
            # 3. Distribuci√≥n de Residuos NORMALIZADOS
            ax3 = self.axes[0, 2]
            ax3.hist(normalized_residuals, bins=20, alpha=0.7, color='red', edgecolor='black')
            ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)
            ax3.set_xlabel('Residuos Normalizados')
            ax3.set_ylabel('Frecuencia')
            ax3.set_title('Distribuci√≥n de Residuos Normalizados')
            ax3.grid(True, alpha=0.3)
            
            # 4. Serie de Predicciones por Muestra
            ax4 = self.axes[1, 0]
            indices = range(len(self.y_test))
            ax4.plot(indices, self.y_test.values, 'o-', label='Valores Reales', color='orange', alpha=0.7)
            ax4.plot(indices, self.test_predictions, 'o-', label='Predicciones', color='blue', alpha=0.7)
            ax4.set_xlabel('√çndice de Muestra')
            ax4.set_ylabel('Valor')
            ax4.set_title('Serie de Predicciones por Muestra')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
            
            # 5. Errores Absolutos NORMALIZADOS por Muestra
            ax5 = self.axes[1, 1]
            abs_normalized_errors = np.abs(normalized_residuals)
            ax5.bar(indices, abs_normalized_errors, alpha=0.7, color='purple')
            ax5.set_xlabel('√çndice de Muestra')
            ax5.set_ylabel('Error Absoluto Normalizado')
            ax5.set_title('Errores Absolutos Normalizados por Muestra')
            ax5.grid(True, alpha=0.3)
            
            # 6. M√©tricas de Evaluaci√≥n ACTUALIZADAS
            ax6 = self.axes[1, 2]
            ax6.axis('off')  # Sin ejes para este panel
            
            # Calcular m√©tricas adicionales con residuos normalizados
            mean_norm_error = np.mean(np.abs(normalized_residuals))
            std_norm_error = np.std(normalized_residuals)
            max_norm_error = np.max(abs_normalized_errors)
            
            # MAPE usando la f√≥rmula original
            mape = np.mean(np.abs((self.y_test - self.test_predictions) / self.y_test)) * 100 if (self.y_test != 0).all() else 0
            
            metrics_text = f"""M√©tricas de Evaluaci√≥n

MAE: {mae:.4f}
RMSE: {rmse:.4f}
R¬≤: {r2:.4f}
MAPE: {mape:.2f}%
Max Error: {np.max(np.abs(self.y_test - self.test_predictions)):.4f}

Residuos Normalizados:
Mean |Error Norm|: {mean_norm_error:.4f}
Std Error Norm: {std_norm_error:.4f}
Max Error Norm: {max_norm_error:.4f}

Muestras: {len(self.y_test)}
Rango real: [{self.y_test.min():.2f}, {self.y_test.max():.2f}]
Rango pred: [{self.test_predictions.min():.2f}, {self.test_predictions.max():.2f}]"""
            
            ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, fontsize=10,
                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
                    verticalalignment='top', fontfamily='monospace')
            
            # Ajustar layout
            self.fig.tight_layout(pad=2.0)
            self.canvas.draw()
            
            # Actualizar an√°lisis de features si est√° disponible
            if hasattr(self, 'update_feature_analysis'):
                self.update_feature_analysis()
            
        except Exception as e:
            print(f"Error actualizando gr√°ficos: {e}")
    
    def save_plots(self):
        """Guardar gr√°ficos como imagen"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "No hay gr√°ficos para guardar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Guardar Gr√°ficos",
            defaultextension=".png",
            filetypes=[("PNG files", "*.png"), ("PDF files", "*.pdf"), ("SVG files", "*.svg")]
        )
        
        if file_path:
            try:
                self.fig.savefig(file_path, dpi=300, bbox_inches='tight')
                messagebox.showinfo("√âxito", f"Gr√°ficos guardados en:\n{file_path}")
            except Exception as e:
                messagebox.showerror("Error", f"Error guardando gr√°ficos:\n{str(e)}")
    
    def update_prediction_interface(self):
        """Actualizar interfaz de predicci√≥n con features principales"""
        # Limpiar frame anterior
        for widget in self.features_frame.winfo_children():
            widget.destroy()
        
        # Mostrar las primeras 10 features
        display_features = self.feature_columns[:10]
        self.feature_vars = {}
        
        for i, feature in enumerate(display_features):
            row = i // 2
            col = (i % 2) * 3
            
            ttk.Label(self.features_frame, text=f"{feature}:").grid(row=row, column=col, sticky='w', padx=5, pady=2)
            
            var = tk.DoubleVar(value=0.0)
            self.feature_vars[feature] = var
            
            entry = ttk.Entry(self.features_frame, textvariable=var, width=15)
            entry.grid(row=row, column=col+1, padx=5, pady=2)
    
    def update_prediction_interface_with_importance(self):
        """Actualizar interfaz de predicci√≥n con features m√°s importantes"""
        if not hasattr(self, 'feature_importance'):
            return
        
        # Limpiar frame anterior
        for widget in self.features_frame.winfo_children():
            widget.destroy()
        
        # Mostrar top 10 features m√°s importantes
        top_features = self.feature_importance.head(10)['feature'].tolist()
        self.feature_vars = {}
        
        for i, feature in enumerate(top_features):
            row = i // 2
            col = (i % 2) * 3
            
            # Obtener valor promedio para sugerir
            avg_value = self.current_data[feature].mean()
            
            label_text = f"{feature} (avg: {avg_value:.2f}):"
            ttk.Label(self.features_frame, text=label_text).grid(row=row, column=col, sticky='w', padx=5, pady=2)
            
            var = tk.DoubleVar(value=avg_value)
            self.feature_vars[feature] = var
            
            entry = ttk.Entry(self.features_frame, textvariable=var, width=15)
            entry.grid(row=row, column=col+1, padx=5, pady=2)
    
    def predict_single(self):
        """Realizar predicci√≥n individual"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "Primero entrena un modelo")
            return
        
        try:
            # Crear vector de features con valores por defecto
            feature_vector = []
            input_features = []
            
            for feature in self.feature_columns:
                if feature in self.feature_vars:
                    value = self.feature_vars[feature].get()
                    input_features.append(f"{feature}: {value:.3f}")
                else:
                    value = self.current_data[feature].mean()
                
                feature_vector.append(value)
            
            # Realizar predicci√≥n
            X_pred = np.array(feature_vector).reshape(1, -1)
            prediction = self.trained_model.predict(X_pred)[0]
            
            # Obtener intervalo de confianza aproximado
            tree_predictions = [tree.predict(X_pred)[0] for tree in self.trained_model.estimators_]
            pred_std = np.std(tree_predictions)
            
            # Mostrar resultado
            result_text = f"""PREDICCI√ìN INDIVIDUAL
====================

VALORES DE ENTRADA (top features):
"""
            
            for input_feat in input_features:
                result_text += f"  ‚Ä¢ {input_feat}\n"
            
            result_text += f"""

RESULTADO:
  Vacancias predichas: {prediction:.2f}
  Rango estimado: {prediction - 1.96*pred_std:.2f} - {prediction + 1.96*pred_std:.2f}
  Incertidumbre: ¬± {1.96*pred_std:.2f}

INTERPRETACI√ìN:
  El modelo predice {prediction:.0f} vacancias con una 
  incertidumbre de ¬±{1.96*pred_std:.1f} vacancias (95% confianza).
"""
            
            self.prediction_text.delete(1.0, tk.END)
            self.prediction_text.insert(1.0, result_text)
            
        except Exception as e:
            messagebox.showerror("Error", f"Error en predicci√≥n:\n{str(e)}")
    
    def save_model(self):
        """Guardar modelo entrenado"""
        if self.trained_model is None:
            messagebox.showwarning("Advertencia", "No hay modelo entrenado")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Guardar Modelo",
            defaultextension=".joblib",
            filetypes=[("Joblib files", "*.joblib")]
        )
        
        if file_path:
            try:
                import joblib
                
                # Guardar modelo con metadatos
                model_data = {
                    'model': self.trained_model,
                    'feature_columns': self.feature_columns,
                    'target_column': self.target_column,
                    'training_params': {
                        'n_estimators': self.n_estimators_var.get(),
                        'test_size': self.test_size_var.get(),
                        'random_state': self.random_state_var.get()
                    },
                    'feature_importance': self.feature_importance if hasattr(self, 'feature_importance') else None
                }
                
                joblib.dump(model_data, file_path)
                messagebox.showinfo("√âxito", f"Modelo guardado en:\n{file_path}")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error guardando modelo:\n{str(e)}")
    
    def load_model(self):
        """Cargar modelo previamente entrenado"""
        file_path = filedialog.askopenfilename(
            title="Cargar Modelo",
            filetypes=[("Joblib files", "*.joblib")]
        )
        
        if file_path:
            try:
                import joblib
                
                model_data = joblib.load(file_path)
                
                if isinstance(model_data, dict):
                    self.trained_model = model_data['model']
                    
                    # Verificar compatibilidad de features
                    saved_features = model_data.get('feature_columns', [])
                    if self.current_data is not None:
                        missing_features = set(saved_features) - set(self.current_data.columns)
                        if missing_features:
                            messagebox.showwarning("Advertencia", 
                                                 f"El dataset actual no tiene las features:\n{missing_features}")
                        else:
                            self.feature_columns = saved_features
                    else:
                        self.feature_columns = saved_features
                    
                    # Restaurar par√°metros
                    params = model_data.get('training_params', {})
                    self.n_estimators_var.set(params.get('n_estimators', 100))
                    self.test_size_var.set(params.get('test_size', 0.2))
                    self.random_state_var.set(params.get('random_state', 42))
                    
                    # Restaurar feature importance si est√° disponible
                    if 'feature_importance' in model_data:
                        self.feature_importance = model_data['feature_importance']
                    
                    self.save_btn.config(state="normal")
                    messagebox.showinfo("√âxito", f"Modelo cargado desde:\n{file_path}")
                    
                else:
                    # Modelo legacy sin metadatos
                    self.trained_model = model_data
                    messagebox.showinfo("√âxito", "Modelo legacy cargado")
                
            except Exception as e:
                messagebox.showerror("Error", f"Error cargando modelo:\n{str(e)}")
    
    def export_model(self):
        """Exportar modelo (wrapper para compatibilidad)"""
        self.save_model()
    
    def reset(self):
        """Reset del tab"""
        self.current_data = None
        self.trained_model = None
        self.feature_columns = []
        self.X_test = None
        self.y_test = None
        self.test_predictions = None
        
        # Reset displays
        self.info_text.delete(1.0, tk.END)
        self.info_text.insert(1.0, "Carga un dataset para comenzar")
        
        self.results_text.delete(1.0, tk.END)
        self.prediction_text.delete(1.0, tk.END)
        
        # Reset botones
        self.save_btn.config(state="disabled")
        
        # Limpiar interfaz de predicci√≥n
        for widget in self.features_frame.winfo_children():
            widget.destroy()
        
        # Limpiar gr√°ficos
        for ax in self.axes.flat:
            ax.clear()
            ax.text(0.5, 0.5, 'Carga un dataset\ny entrena un modelo', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12)
            ax.set_xticks([])
            ax.set_yticks([])
        self.canvas.draw()


# Funci√≥n adicional para crear gr√°fico de feature importance como en tu imagen
def create_feature_importance_plot():
    """Crear gr√°fico separado de feature importance (similar a tu imagen)"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    def plot_feature_importance_standalone(feature_importance_df, top_n=20):
        """
        Crear gr√°fico de barras horizontales de feature importance
        Par√°metros:
        - feature_importance_df: DataFrame con columnas 'feature' e 'importance'  
        - top_n: n√∫mero de features a mostrar
        """
        # Configurar estilo
        plt.style.use('default')
        sns.set_palette("husl")
        
        # Tomar top N features
        top_features = feature_importance_df.head(top_n)
        
        # Crear figura
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Crear gr√°fico de barras horizontales
        bars = ax.barh(range(len(top_features)), top_features['importance'], 
                      color='steelblue', alpha=0.8)
        
        # Configurar ejes
        ax.set_yticks(range(len(top_features)))
        ax.set_yticklabels(top_features['feature'])
        ax.set_xlabel('Importancia', fontsize=12)
        ax.set_title(f'Top {top_n} importancias de features (RF)', fontsize=14, fontweight='bold')
        
        # Invertir eje Y para mostrar m√°s importante arriba
        ax.invert_yaxis()
        
        # Agregar valores en las barras
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                   f'{width:.3f}', ha='left', va='center', fontsize=9)
        
        # Mejorar apariencia
        ax.grid(axis='x', alpha=0.3)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        plt.tight_layout()
        return fig, ax
    
    return plot_feature_importance_standalone


# Ejemplo de uso principal
if __name__ == "__main__":
    """
    Ejemplo de c√≥mo usar la clase AdvancedMLTabWithPlots
    """
    import tkinter as tk
    from tkinter import ttk
    
    def data_callback(data):
        print(f"Dataset cargado: {data.shape}")
    
    # Crear ventana principal
    root = tk.Tk()
    root.title("Advanced ML with Plots - Ejemplo")
    root.geometry("1400x900")
    
    # Crear notebook
    notebook = ttk.Notebook(root)
    notebook.pack(fill="both", expand=True, padx=10, pady=10)
    
    # Crear instancia del tab avanzado
    ml_tab = AdvancedMLTabWithPlots(notebook, data_callback)
    notebook.add(ml_tab.frame, text="Advanced ML")
    
    # Iniciar aplicaci√≥n
    root.mainloop()


# Funciones auxiliares adicionales para an√°lisis
def calculate_feature_stability(model, X_data, y_data, n_bootstraps=100):
    """
    Calcular estabilidad de importancia de features usando bootstrap
    
    Par√°metros:
    - model: modelo Random Forest entrenado
    - X_data: datos de features para bootstrap
    - y_data: datos de target correspondientes
    - n_bootstraps: n√∫mero de iteraciones bootstrap
    
    Retorna:
    - DataFrame con estad√≠sticas de estabilidad por feature
    """
    from sklearn.utils import resample
    
    feature_importances = []
    
    for _ in range(n_bootstraps):
        # Bootstrap sample
        X_bootstrap, y_bootstrap = resample(X_data, y_data, random_state=None)
        
        # Re-entrenar modelo en muestra bootstrap
        bootstrap_model = type(model)(**model.get_params())
        bootstrap_model.fit(X_bootstrap, y_bootstrap)
        
        # Guardar importancias
        feature_importances.append(bootstrap_model.feature_importances_)
    
    # Convertir a DataFrame
    importances_df = pd.DataFrame(feature_importances, columns=X_data.columns)
    
    # Calcular estad√≠sticas
    stability_stats = pd.DataFrame({
        'feature': X_data.columns,
        'mean_importance': importances_df.mean(),
        'std_importance': importances_df.std(),
        'cv_importance': importances_df.std() / importances_df.mean(),  # Coeficiente de variaci√≥n
        'min_importance': importances_df.min(),
        'max_importance': importances_df.max()
    }).sort_values('mean_importance', ascending=False)
    
    return stability_stats


def generate_model_report(ml_tab_instance, output_path=None):
    """
    Generar reporte completo del modelo en formato texto
    
    Par√°metros:
    - ml_tab_instance: instancia de AdvancedMLTabWithPlots
    - output_path: ruta para guardar reporte (opcional)
    
    Retorna:
    - String con reporte completo
    """
    if ml_tab_instance.trained_model is None:
        return "No hay modelo entrenado para generar reporte"
    
    from datetime import datetime
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    
    # Calcular m√©tricas
    mae = mean_absolute_error(ml_tab_instance.y_test, ml_tab_instance.test_predictions)
    rmse = np.sqrt(mean_squared_error(ml_tab_instance.y_test, ml_tab_instance.test_predictions))
    r2 = r2_score(ml_tab_instance.y_test, ml_tab_instance.test_predictions)
    
    # Calcular residuos normalizados
    y_test_safe = ml_tab_instance.y_test.copy()
    y_test_safe[y_test_safe == 0] = 1e-10
    normalized_residuals = 1 - (ml_tab_instance.test_predictions / y_test_safe)
    
    report = f"""
REPORTE DE MODELO RANDOM FOREST
===============================
Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

CONFIGURACI√ìN DEL MODELO:
  Estimadores: {ml_tab_instance.trained_model.n_estimators}
  Criterio: {ml_tab_instance.trained_model.criterion}
  Features utilizadas: {len(ml_tab_instance.feature_columns)}
  Target: {ml_tab_instance.target_column}

DATOS:
  Total muestras: {len(ml_tab_instance.current_data)}
  Muestras entrenamiento: {len(ml_tab_instance.y_test) * (1 / ml_tab_instance.test_size_var.get() - 1):.0f}
  Muestras prueba: {len(ml_tab_instance.y_test)}
  Test size: {ml_tab_instance.test_size_var.get():.1%}

M√âTRICAS DE RENDIMIENTO:
  R¬≤ Score: {r2:.4f}
  MAE: {mae:.4f}
  RMSE: {rmse:.4f}
  MAPE: {np.mean(np.abs((ml_tab_instance.y_test - ml_tab_instance.test_predictions) / ml_tab_instance.y_test)) * 100:.2f}%

RESIDUOS NORMALIZADOS (1 - pred/real):
  Error normalizado medio: {np.mean(np.abs(normalized_residuals)):.4f}
  Desviaci√≥n est√°ndar: {np.std(normalized_residuals):.4f}
  Error m√°ximo normalizado: {np.max(np.abs(normalized_residuals)):.4f}

TOP 10 FEATURES M√ÅS IMPORTANTES:
"""
    
    for i, (_, row) in enumerate(ml_tab_instance.feature_importance.head(10).iterrows()):
        report += f"  {i+1:2d}. {row['feature'][:50]:50s}: {row['importance']:.4f}\n"
    
    report += f"""

DISTRIBUCI√ìN DE ERRORES:
  P25: {np.percentile(np.abs(ml_tab_instance.y_test - ml_tab_instance.test_predictions), 25):.2f}
  P50: {np.percentile(np.abs(ml_tab_instance.y_test - ml_tab_instance.test_predictions), 50):.2f}
  P75: {np.percentile(np.abs(ml_tab_instance.y_test - ml_tab_instance.test_predictions), 75):.2f}
  P90: {np.percentile(np.abs(ml_tab_instance.y_test - ml_tab_instance.test_predictions), 90):.2f}
  P95: {np.percentile(np.abs(ml_tab_instance.y_test - ml_tab_instance.test_predictions), 95):.2f}

INTERPRETACI√ìN:
  Calidad del modelo: {'Excelente' if r2 > 0.9 else 'Bueno' if r2 > 0.7 else 'Moderado' if r2 > 0.5 else 'Pobre'}
  Precisi√≥n: {'Alta' if mae < 5 else 'Media' if mae < 10 else 'Baja'}
  
RECOMENDACIONES:
"""
    
    if r2 < 0.7:
        report += "  - Considerar m√°s features o ingenier√≠a de features\n"
        report += "  - Probar hiperpar√°metros diferentes\n"
    if mae > 10:
        report += "  - Revisar outliers en los datos\n"
        report += "  - Considerar transformaciones de datos\n"
    if np.std(normalized_residuals) > 0.3:
        report += "  - Alta variabilidad en errores normalizados\n"
        report += "  - Considerar estratificaci√≥n por rangos de target\n"
    
    # Guardar reporte si se especifica ruta
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"Reporte guardado en: {output_path}")
    
    return report