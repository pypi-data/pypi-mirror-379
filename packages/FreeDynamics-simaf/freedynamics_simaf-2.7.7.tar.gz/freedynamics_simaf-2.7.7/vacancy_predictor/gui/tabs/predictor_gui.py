import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import seaborn as sns
from pathlib import Path
import traceback
import pickle
import gzip
import io
from typing import Tuple, Dict, Any, List
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LAMMPSFileParser:
    """Parser especializado para archivos LAMMPS dump"""
    
    def __init__(self):
        self.supported_extensions = ['.dump', '.dump.gz', '.lammpstrj', '.lammpstrj.gz']
        self.ATM_TOTAL = 16384  # Configuraci√≥n por defecto
    
    def _open_file(self, path: str):
        """Abrir archivo, detectando si est√° comprimido"""
        p = Path(path)
        if p.suffix == ".gz":
            return io.TextIOWrapper(gzip.open(p, "rb"), encoding="utf-8", newline="")
        return open(p, "r", encoding="utf-8", newline="")
    
    def parse_last_frame(self, path: str) -> Tuple[pd.DataFrame, int, Dict[str, Any]]:
        """Parser robusto del √∫ltimo frame de un archivo LAMMPS dump"""
        try:
            with self._open_file(path) as f:
                lines = f.read().splitlines()
        except Exception as e:
            raise RuntimeError(f"Error leyendo archivo {path}: {str(e)}")
        
        # Buscar la √∫ltima secci√≥n ATOMS
        atoms_indices = [i for i, line in enumerate(lines) 
                        if line.startswith("ITEM: ATOMS")]
        
        if not atoms_indices:
            raise RuntimeError(f"No se encontr√≥ 'ITEM: ATOMS' en {path}")
        
        start_idx = atoms_indices[-1]
        
        # Extraer metadata del frame
        metadata = self._extract_metadata(lines, start_idx)
        
        # Parsear header de √°tomos
        header_line = lines[start_idx].replace("ITEM: ATOMS", "").strip()
        columns = header_line.split()
        
        if not columns:
            raise RuntimeError(f"Header de ATOMS vac√≠o en {path}")
        
        # Determinar n√∫mero de √°tomos
        n_atoms = self._find_num_atoms(lines, start_idx, metadata)
        
        if n_atoms <= 0:
            raise RuntimeError(f"N√∫mero inv√°lido de √°tomos ({n_atoms}) en {path}")
        
        # Parsear datos de √°tomos
        df = self._parse_atomic_data(lines, start_idx + 1, n_atoms, columns)
        
        if df.empty:
            raise RuntimeError(f"No se pudieron extraer datos de √°tomos de {path}")
        
        logger.debug(f"Parseado {Path(path).name}: {len(df)} √°tomos, {len(df.columns)} columnas")
        
        return df, len(df), metadata
    
    def _extract_metadata(self, lines: List[str], atoms_start: int) -> Dict[str, Any]:
        """Extraer metadata del frame"""
        metadata = {}
        
        # Buscar timestep
        for i in range(max(0, atoms_start - 20), atoms_start):
            if i < len(lines) and "ITEM: TIMESTEP" in lines[i]:
                try:
                    if i + 1 < len(lines):
                        metadata['timestep'] = int(lines[i + 1])
                except (IndexError, ValueError):
                    pass
                break
        
        # Buscar box bounds
        for i in range(max(0, atoms_start - 20), atoms_start):
            if i < len(lines) and "ITEM: BOX BOUNDS" in lines[i]:
                try:
                    # Leer 3 l√≠neas de bounds
                    bounds = []
                    for j in range(3):
                        line_idx = i + 1 + j
                        if line_idx < len(lines):
                            bounds.append(lines[line_idx].split())
                        else:
                            break
                    if len(bounds) == 3:  # Solo agregar si tenemos todas las bounds
                        metadata['box_bounds'] = bounds
                except (IndexError, ValueError):
                    pass
                break
        
        return metadata
    
    def _find_num_atoms(self, lines: List[str], atoms_start: int, metadata: Dict) -> int:
        """Determinar n√∫mero de √°tomos en el frame"""
        # Buscar "ITEM: NUMBER OF ATOMS"
        for i in range(max(0, atoms_start - 20), atoms_start):
            if i < len(lines) and "ITEM: NUMBER OF ATOMS" in lines[i]:
                try:
                    if i + 1 < len(lines):
                        return int(lines[i + 1])
                except (IndexError, ValueError):
                    pass
        
        # Si no se encuentra, calcular hasta la siguiente secci√≥n
        data_start = atoms_start + 1
        count = 0
        for i in range(data_start, len(lines)):
            line = lines[i].strip()
            if not line or line.startswith("ITEM:"):
                break
            count += 1
        
        return count
    
    def _parse_atomic_data(self, lines: List[str], start_idx: int, n_atoms: int, columns: List[str]) -> pd.DataFrame:
        """Parsear datos at√≥micos"""
        data_rows = []
        
        for i in range(start_idx, min(start_idx + n_atoms, len(lines))):
            line = lines[i].strip()
            if not line:
                continue
            
            parts = line.split()
            if len(parts) >= len(columns):
                try:
                    row = []
                    for j, part in enumerate(parts[:len(columns)]):
                        try:
                            row.append(float(part))
                        except ValueError:
                            row.append(np.nan)
                    data_rows.append(row)
                except Exception as e:
                    logger.warning(f"Error parseando l√≠nea {i}: {line[:50]}...")
                    continue
        
        if not data_rows:
            return pd.DataFrame()
        
        df = pd.DataFrame(data_rows, columns=columns)
        df = df.replace([np.inf, -np.inf], np.nan)
        
        return df

class DumpFeatureExtractor:
    """Extractor de features desde archivos dump procesados"""
    
    def __init__(self):
        self.ATM_TOTAL = 16384
        self.ENERGY_BINS = 10
        self.ENERGY_MIN = -4.0
        self.ENERGY_MAX = -3.0
    
    def extract_features(self, atomic_df: pd.DataFrame, filename: str = "") -> pd.Series:
        """Extraer features para ML desde datos at√≥micos"""
        if atomic_df.empty:
            raise ValueError("DataFrame at√≥mico vac√≠o, no se pueden extraer features")
        
        features = {}
        
        # Basic atom statistics
        features['n_atoms'] = len(atomic_df)
        
        # Features basadas en energ√≠a (si existe columna 'c_pe')
        if 'c_pe' in atomic_df.columns:
            energies = atomic_df['c_pe'].dropna()
            if len(energies) > 0:
                features.update({
                    'energy_mean': energies.mean(),
                    'energy_std': energies.std() if len(energies) > 1 else 0.0,
                    'energy_min': energies.min(),
                    'energy_max': energies.max(),
                    'energy_q25': energies.quantile(0.25),
                    'energy_q75': energies.quantile(0.75),
                    'energy_range': energies.max() - energies.min()
                })
                
                # Distribuci√≥n de energ√≠a en bins
                try:
                    energy_hist, _ = np.histogram(energies, bins=self.ENERGY_BINS, 
                                                range=(self.ENERGY_MIN, self.ENERGY_MAX))
                    for i, count in enumerate(energy_hist):
                        features[f'energy_bin_{i}'] = count
                except Exception as e:
                    logger.warning(f"Error calculando histograma de energ√≠a: {e}")
                    # Agregar bins vac√≠os como fallback
                    for i in range(self.ENERGY_BINS):
                        features[f'energy_bin_{i}'] = 0
        
        # Features espaciales (si existen coordenadas x, y, z)
        coord_cols = [col for col in ['x', 'y', 'z'] if col in atomic_df.columns]
        if coord_cols:
            for col in coord_cols:
                coords = atomic_df[col].dropna()
                if len(coords) > 0:
                    features.update({
                        f'{col}_mean': coords.mean(),
                        f'{col}_std': coords.std() if len(coords) > 1 else 0.0,
                        f'{col}_range': coords.max() - coords.min()
                    })
        
        # Features de tipo de √°tomo (si existe columna 'type')
        if 'type' in atomic_df.columns:
            type_counts = atomic_df['type'].value_counts()
            total_atoms = len(atomic_df)
            for atom_type, count in type_counts.items():
                features[f'type_{atom_type}_count'] = count
                features[f'type_{atom_type}_fraction'] = count / total_atoms if total_atoms > 0 else 0.0
        
        # Features de velocidad (si existen vx, vy, vz)
        vel_cols = [col for col in ['vx', 'vy', 'vz'] if col in atomic_df.columns]
        if vel_cols:
            for col in vel_cols:
                velocities = atomic_df[col].dropna()
                if len(velocities) > 0:
                    features.update({
                        f'{col}_mean': velocities.mean(),
                        f'{col}_std': velocities.std() if len(velocities) > 1 else 0.0,
                        f'speed_{col}': np.sqrt((velocities**2).mean())  # RMS velocity
                    })
        
        # Calcular vacancies estimadas (diferencia con ATM_TOTAL)
        estimated_vacancies = max(0, self.ATM_TOTAL - len(atomic_df))
        features['estimated_vacancies'] = estimated_vacancies
        
        # Metadata del archivo
        features['file'] = filename
        
        return pd.Series(features)

class PredictorGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("üîÆ Predictor de Vacancies - Extendido con Dump Processor")
        self.root.geometry("1200x800")
        
        # Variables para almacenar datos
        self.csv_path = None
        self.model_path = None
        self.df = None
        self.model = None
        self.feature_columns = None
        self.scaler = None
        self.predictions_df = None
        
        # Variables para dump processing
        self.dump_files = []
        self.processed_dump_df = None
        self.dump_model = None
        self.dump_model_path = None
        self.dump_predictions = None
        
        # Referencias a widgets
        self.predict_btn = None
        self.save_btn = None
        self.csv_label = None
        self.model_label = None
        
        # Herramientas de procesamiento
        self.parser = LAMMPSFileParser()
        self.feature_extractor = DumpFeatureExtractor()
        
        self.create_widgets()
        self.setup_style()
    
    def setup_style(self):
        """Configurar estilos visuales"""
        style = ttk.Style()
        style.theme_use('clam')
        
        # Configurar colores para los labels de estado
        style.configure("Success.TLabel", foreground="green")
        style.configure("Error.TLabel", foreground="red")
        style.configure("Warning.TLabel", foreground="orange")
    
    def create_widgets(self):
        # Frame principal con scroll
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill="both", expand=True)
        
        # T√≠tulo
        title_label = ttk.Label(main_frame, text="üîÆ Predictor de Vacancies ML - Extendido", 
                               font=("Arial", 18, "bold"))
        title_label.pack(pady=(0, 20))
        
        # Frame de controles principal (CSV/Model cl√°sico)
        control_frame = ttk.LabelFrame(main_frame, text="üìÇ Controles Cl√°sicos (CSV + Modelo)", padding="15")
        control_frame.pack(fill="x", pady=(0, 15))
        
        # Botones de carga cl√°sicos
        btn_frame = ttk.Frame(control_frame)
        btn_frame.pack(fill="x", pady=5)
        
        ttk.Button(btn_frame, text="üìÑ Cargar CSV", 
                  command=self.load_csv, width=18).pack(side="left", padx=(0, 10))
        ttk.Button(btn_frame, text="ü§ñ Cargar Modelo", 
                  command=self.load_model, width=18).pack(side="left", padx=(0, 10))
        
        # Bot√≥n de predicci√≥n cl√°sico
        self.predict_btn = ttk.Button(btn_frame, text="üîÆ Predecir CSV", 
                                     command=self.predict, width=15, state="disabled")
        self.predict_btn.pack(side="left", padx=(0, 10))
        
        # Bot√≥n de guardar cl√°sico
        self.save_btn = ttk.Button(btn_frame, text="üíæ Guardar", 
                                  command=self.save_results, width=15, state="disabled")
        self.save_btn.pack(side="left")
        
        # Info de archivos cargados
        info_frame = ttk.Frame(control_frame)
        info_frame.pack(fill="x", pady=(10, 0))
        
        self.csv_label = ttk.Label(info_frame, text="üìÑ CSV: No cargado")
        self.csv_label.pack(anchor="w", pady=2)
        
        self.model_label = ttk.Label(info_frame, text="ü§ñ Modelo: No cargado")
        self.model_label.pack(anchor="w", pady=2)
        
        # Progress bar
        self.progress_frame = ttk.Frame(main_frame)
        self.progress_frame.pack(fill="x", pady=(0, 10))
        
        self.progress = ttk.Progressbar(self.progress_frame, mode='indeterminate')
        self.progress_label = ttk.Label(self.progress_frame, text="")
        
        # Pesta√±as para resultados
        self.notebook = ttk.Notebook(main_frame)
        self.notebook.pack(fill="both", expand=True)
        
        # Pesta√±as existentes
        self.create_info_tab()
        self.create_results_tab()
        self.create_viz_tab()
        self.create_stats_tab()
        
        # Nueva pesta√±a para dump processing
        self.create_dump_processor_tab()
    
    def create_dump_processor_tab(self):
        """Crear pesta√±a para procesamiento de archivos dump"""
        dump_frame = ttk.Frame(self.notebook, padding="10")
        self.notebook.add(dump_frame, text="üî• Dump Processor")
        
        # Frame superior de controles
        control_frame = ttk.LabelFrame(dump_frame, text="üî• Procesamiento de Archivos LAMMPS Dump", padding="10")
        control_frame.pack(fill="x", pady=(0, 10))
        
        # Primera fila de botones
        btn_row1 = ttk.Frame(control_frame)
        btn_row1.pack(fill="x", pady=5)
        
        ttk.Button(btn_row1, text="üìÅ Cargar Dumps", 
                  command=self.load_dump_files, width=15).pack(side="left", padx=(0, 10))
        ttk.Button(btn_row1, text="üîÑ Procesar a CSV", 
                  command=self.process_dumps_to_csv, width=15).pack(side="left", padx=(0, 10))
        ttk.Button(btn_row1, text="ü§ñ Cargar Modelo ML", 
                  command=self.load_dump_model, width=15).pack(side="left", padx=(0, 10))
        
        # Segunda fila de botones
        btn_row2 = ttk.Frame(control_frame)
        btn_row2.pack(fill="x", pady=5)
        
        self.dump_predict_btn = ttk.Button(btn_row2, text="üîÆ Predecir Dump", 
                                          command=self.predict_from_dumps, width=15, state="disabled")
        self.dump_predict_btn.pack(side="left", padx=(0, 10))
        
        self.dump_save_btn = ttk.Button(btn_row2, text="üíæ Guardar Resultados", 
                                       command=self.save_dump_results, width=18, state="disabled")
        self.dump_save_btn.pack(side="left", padx=(0, 10))
        
        ttk.Button(btn_row2, text="üìä Exportar CSV Features", 
                  command=self.export_dump_csv, width=18).pack(side="left")
        
        # Labels de estado
        status_frame = ttk.Frame(control_frame)
        status_frame.pack(fill="x", pady=(10, 0))
        
        self.dump_files_label = ttk.Label(status_frame, text="üìÅ Archivos dump: Ninguno cargado")
        self.dump_files_label.pack(anchor="w", pady=2)
        
        self.dump_model_label = ttk.Label(status_frame, text="ü§ñ Modelo dump: No cargado")
        self.dump_model_label.pack(anchor="w", pady=2)
        
        self.dump_status_label = ttk.Label(status_frame, text="‚è≥ Estado: Listo para cargar archivos")
        self.dump_status_label.pack(anchor="w", pady=2)
        
        # √Årea de resultados
        results_frame = ttk.LabelFrame(dump_frame, text="üìä Resultados del Procesamiento", padding="10")
        results_frame.pack(fill="both", expand=True)
        
        self.dump_results_text = scrolledtext.ScrolledText(results_frame, height=15, wrap='word',
                                                          font=("Consolas", 10))
        self.dump_results_text.pack(fill="both", expand=True)
        
        # Mensaje inicial
        welcome_dump = """üî• DUMP PROCESSOR - PREDICCI√ìN DE VACANCIES
=====================================================

üí° FUNCIONALIDAD:
Este m√≥dulo procesa archivos LAMMPS dump (.dump, .dump.gz) para:

üîπ Extraer features autom√°ticamente de archivos dump
üîπ Convertir datos at√≥micos a formato CSV con features ML
üîπ Predecir vacancies usando modelos entrenados
üîπ Exportar resultados y datasets generados

üìã PASOS PARA USAR:
1Ô∏è‚É£ Cargar archivos dump (puedes seleccionar m√∫ltiples)
2Ô∏è‚É£ Procesar dumps a CSV (extrae features autom√°ticamente)
3Ô∏è‚É£ Cargar modelo ML entrenado (.joblib)
4Ô∏è‚É£ Predecir vacancies desde los dumps
5Ô∏è‚É£ Exportar resultados y datasets

üîß FEATURES EXTRA√çDAS AUTOM√ÅTICAMENTE:
‚Ä¢ Estad√≠sticas de energ√≠a (media, std, quantiles, distribuci√≥n)
‚Ä¢ Estad√≠sticas espaciales (x, y, z: media, std, rango)
‚Ä¢ Conteos por tipo de √°tomo
‚Ä¢ Features de velocidad (si disponibles)
‚Ä¢ Estimaci√≥n inicial de vacancies

‚ö° FORMATOS SOPORTADOS:
‚Ä¢ .dump, .dump.gz
‚Ä¢ .lammpstrj, .lammpstrj.gz

üöÄ ¬°Comienza cargando tus archivos dump!
"""
        self.dump_results_text.insert(1.0, welcome_dump)
    
    def load_dump_files(self):
        """Cargar archivos dump"""
        file_paths = filedialog.askopenfilenames(
            title="Seleccionar archivos LAMMPS dump",
            filetypes=[
                ("LAMMPS dump files", "*.dump *.dump.gz *.lammpstrj *.lammpstrj.gz"),
                ("Todos los archivos", "*.*")
            ]
        )
        
        if file_paths:
            self.dump_files = list(file_paths)
            self.dump_files_label.config(
                text=f"üìÅ Archivos dump: {len(self.dump_files)} archivos cargados", 
                style="Success.TLabel"
            )
            
            self.dump_status_label.config(
                text="‚è≥ Estado: Archivos cargados, listo para procesar", 
                style="Success.TLabel"
            )
            
            # Mostrar lista de archivos en el √°rea de resultados
            files_info = "üî• ARCHIVOS DUMP CARGADOS\n" + "="*50 + "\n\n"
            files_info += f"üìä Total de archivos: {len(self.dump_files)}\n\n"
            files_info += "üìÅ ARCHIVOS:\n"
            
            for i, filepath in enumerate(self.dump_files, 1):
                filename = Path(filepath).name
                filesize = Path(filepath).stat().st_size / (1024*1024)  # MB
                files_info += f"   {i:2d}. {filename} ({filesize:.1f} MB)\n"
            
            files_info += f"\n‚úÖ Archivos listos para procesar\n"
            files_info += "üîÑ Haz clic en 'Procesar a CSV' para extraer features"
            
            self.dump_results_text.delete(1.0, tk.END)
            self.dump_results_text.insert(1.0, files_info)
    
    def process_dumps_to_csv(self):
        """Procesar archivos dump y convertir a CSV con features"""
        if not self.dump_files:
            messagebox.showwarning("Advertencia", "Primero carga archivos dump")
            return
        
        try:
            self.show_progress("Procesando archivos dump...")
            self.dump_status_label.config(text="‚è≥ Estado: Procesando archivos dump...", 
                                        style="Warning.TLabel")
            
            all_features = []
            failed_files = []
            
            for i, dump_file in enumerate(self.dump_files):
                try:
                    # Actualizar progreso
                    self.progress_label.config(text=f"Procesando archivo {i+1}/{len(self.dump_files)}: {Path(dump_file).name}")
                    self.root.update()
                    
                    # Parsear archivo dump
                    atomic_df, n_atoms, metadata = self.parser.parse_last_frame(dump_file)
                    
                    # Extraer features
                    features = self.feature_extractor.extract_features(atomic_df, Path(dump_file).name)
                    all_features.append(features)
                    
                    logger.info(f"Procesado exitosamente: {Path(dump_file).name} ({n_atoms} √°tomos)")
                    
                except Exception as e:
                    failed_files.append((dump_file, str(e)))
                    logger.error(f"Error procesando {dump_file}: {str(e)}")
                    continue
            
            if not all_features:
                raise RuntimeError("No se pudieron procesar archivos dump")
            
            # Crear DataFrame con todas las features
            self.processed_dump_df = pd.DataFrame(all_features)
            
            # Reordenar columnas (file primero, luego features)
            cols = ['file'] + [col for col in self.processed_dump_df.columns if col != 'file']
            self.processed_dump_df = self.processed_dump_df[cols]
            
            # Actualizar estado
            self.dump_status_label.config(
                text=f"‚úÖ Estado: {len(all_features)} archivos procesados exitosamente", 
                style="Success.TLabel"
            )
            
            # Mostrar resultados
            self.show_dump_processing_results(failed_files)
            
            # Verificar si podemos predecir
            self.check_dump_prediction_ready()
            
        except Exception as e:
            self.dump_status_label.config(text="‚ùå Estado: Error en procesamiento", style="Error.TLabel")
            messagebox.showerror("Error", f"Error procesando dumps:\n{str(e)}\n\n{traceback.format_exc()}")
        finally:
            self.hide_progress()
    
    def show_dump_processing_results(self, failed_files):
        """Mostrar resultados del procesamiento"""
        if self.processed_dump_df is None:
            return
        
        results_text = "üî• PROCESAMIENTO DE DUMPS COMPLETADO\n" + "="*50 + "\n\n"
        
        # Estad√≠sticas generales
        results_text += f"üìä RESUMEN:\n"
        results_text += f"   ‚Ä¢ Archivos procesados exitosamente: {len(self.processed_dump_df)}\n"
        results_text += f"   ‚Ä¢ Features extra√≠das por archivo: {len(self.processed_dump_df.columns)-1}\n"  # -1 por 'file'
        results_text += f"   ‚Ä¢ Archivos fallidos: {len(failed_files)}\n\n"
        
        # Features disponibles
        feature_cols = [col for col in self.processed_dump_df.columns if col != 'file']
        results_text += f"üîß FEATURES EXTRA√çDAS ({len(feature_cols)}):\n"
        
        # Agrupar features por categor√≠a
        energy_features = [f for f in feature_cols if 'energy' in f]
        spatial_features = [f for f in feature_cols if any(coord in f for coord in ['x_', 'y_', 'z_'])]
        type_features = [f for f in feature_cols if 'type_' in f]
        velocity_features = [f for f in feature_cols if any(vel in f for vel in ['vx', 'vy', 'vz', 'speed'])]
        other_features = [f for f in feature_cols if f not in energy_features + spatial_features + type_features + velocity_features]
        
        if energy_features:
            results_text += f"   üîã Energ√≠a ({len(energy_features)}): {', '.join(energy_features[:5])}"
            if len(energy_features) > 5:
                results_text += f" ... y {len(energy_features)-5} m√°s"
            results_text += "\n"
        
        if spatial_features:
            results_text += f"   üìê Espaciales ({len(spatial_features)}): {', '.join(spatial_features[:5])}"
            if len(spatial_features) > 5:
                results_text += f" ... y {len(spatial_features)-5} m√°s"
            results_text += "\n"
        
        if type_features:
            results_text += f"   üß¨ Tipos de √°tomo ({len(type_features)}): {', '.join(type_features[:5])}"
            if len(type_features) > 5:
                results_text += f" ... y {len(type_features)-5} m√°s"
            results_text += "\n"
        
        if velocity_features:
            results_text += f"   üöÄ Velocidad ({len(velocity_features)}): {', '.join(velocity_features[:3])}"
            if len(velocity_features) > 3:
                results_text += f" ... y {len(velocity_features)-3} m√°s"
            results_text += "\n"
        
        if other_features:
            results_text += f"   üìä Otros ({len(other_features)}): {', '.join(other_features[:3])}"
            if len(other_features) > 3:
                results_text += f" ... y {len(other_features)-3} m√°s"
            results_text += "\n"
        
        # Estad√≠sticas de vacancies estimadas
        if 'estimated_vacancies' in self.processed_dump_df.columns:
            vac_stats = self.processed_dump_df['estimated_vacancies'].describe()
            results_text += f"\nüîç ESTAD√çSTICAS DE VACANCIES ESTIMADAS:\n"
            results_text += f"   ‚Ä¢ M√≠nimo: {vac_stats['min']:.0f}\n"
            results_text += f"   ‚Ä¢ M√°ximo: {vac_stats['max']:.0f}\n"
            results_text += f"   ‚Ä¢ Media: {vac_stats['mean']:.1f}\n"
            results_text += f"   ‚Ä¢ Desviaci√≥n: {vac_stats['std']:.1f}\n"
        
        # Primeras filas como ejemplo
        results_text += f"\nüìã PRIMERAS 3 FILAS DEL DATASET:\n"
        display_cols = ['file', 'n_atoms', 'estimated_vacancies']
        if 'energy_mean' in self.processed_dump_df.columns:
            display_cols.append('energy_mean')
        if 'energy_std' in self.processed_dump_df.columns:
            display_cols.append('energy_std')
        
        available_cols = [col for col in display_cols if col in self.processed_dump_df.columns]
        results_text += self.processed_dump_df[available_cols].head(3).to_string()
        
        # Archivos fallidos
        if failed_files:
            results_text += f"\n\n‚ö†Ô∏è ARCHIVOS FALLIDOS ({len(failed_files)}):\n"
            for failed_file, error in failed_files[:5]:  # Mostrar solo los primeros 5
                results_text += f"   ‚Ä¢ {Path(failed_file).name}: {error[:100]}...\n"
            if len(failed_files) > 5:
                results_text += f"   ... y {len(failed_files)-5} errores m√°s\n"
        
        results_text += f"\n‚úÖ Dataset listo para predicci√≥n ML"
        results_text += f"\nüíæ Usa 'Exportar CSV Features' para guardar el dataset"
        
        self.dump_results_text.delete(1.0, tk.END)
        self.dump_results_text.insert(1.0, results_text)
    
    def load_dump_model(self):
        """Cargar modelo para predicciones desde dump"""
        file_path = filedialog.askopenfilename(
            title="Seleccionar modelo .joblib para dumps",
            filetypes=[("Joblib files", "*.joblib"), ("Todos los archivos", "*.*")]
        )
        
        if file_path:
            try:
                self.show_progress("Cargando modelo para dumps...")
                
                self.dump_model_path = file_path
                model_data = joblib.load(file_path)
                
                # Manejar diferentes formatos de modelo
                if isinstance(model_data, dict):
                    self.dump_model = model_data["model"]
                    self.dump_feature_columns = model_data.get("feature_columns", None)
                    self.dump_scaler = model_data.get("scaler", None)
                else:
                    self.dump_model = model_data
                    self.dump_feature_columns = None
                    self.dump_scaler = None
                
                self.dump_model_label.config(
                    text=f"ü§ñ Modelo dump: {Path(file_path).name}", 
                    style="Success.TLabel"
                )
                
                # Mostrar info del modelo cargado
                model_info = f"\n\nü§ñ MODELO ML CARGADO PARA DUMPS\n" + "="*40 + "\n"
                model_info += f"üìÅ Archivo: {Path(file_path).name}\n"
                model_info += f"üß† Tipo: {type(self.dump_model).__name__}\n"
                
                if self.dump_feature_columns:
                    model_info += f"üîß Features requeridas: {len(self.dump_feature_columns)}\n"
                    model_info += f"üìã Features: {', '.join(self.dump_feature_columns[:10])}"
                    if len(self.dump_feature_columns) > 10:
                        model_info += f" ... y {len(self.dump_feature_columns)-10} m√°s"
                    model_info += "\n"
                
                if self.dump_scaler:
                    model_info += f"üìä Escalador: {type(self.dump_scaler).__name__}\n"
                
                model_info += "‚úÖ Modelo cargado exitosamente\n"
                
                # Agregar al √°rea de resultados
                current_text = self.dump_results_text.get(1.0, tk.END)
                self.dump_results_text.insert(tk.END, model_info)
                
                self.check_dump_prediction_ready()
                
            except Exception as e:
                self.dump_model_label.config(text="ü§ñ Modelo dump: Error al cargar", style="Error.TLabel")
                messagebox.showerror("Error", f"Error cargando modelo:\n{str(e)}\n\n{traceback.format_exc()}")
            finally:
                self.hide_progress()
    
    def check_dump_prediction_ready(self):
        """Verificar si se puede predecir desde dumps"""
        if (hasattr(self, 'processed_dump_df') and self.processed_dump_df is not None and
            hasattr(self, 'dump_model') and self.dump_model is not None):
            
            if (hasattr(self, 'dump_feature_columns') and 
                self.dump_feature_columns is not None and 
                len(self.dump_feature_columns) > 0):
                # Verificar que las features requeridas est√©n disponibles
                available_features = set(self.processed_dump_df.columns)
                required_features = set(self.dump_feature_columns)
                missing_features = required_features - available_features
                
                if missing_features:
                    self.dump_predict_btn.config(state="disabled")
                    self.dump_status_label.config(
                        text=f"‚ö†Ô∏è Estado: Features faltantes: {', '.join(list(missing_features)[:3])}...", 
                        style="Warning.TLabel"
                    )
                    return
            
            # Todo listo para predecir
            self.dump_predict_btn.config(state="normal")
            self.dump_status_label.config(
                text="üîÆ Estado: Listo para predecir vacancies", 
                style="Success.TLabel"
            )
        else:
            self.dump_predict_btn.config(state="disabled")
    
    def predict_from_dumps(self):
        """Realizar predicciones desde dumps procesados"""
        if self.processed_dump_df is None or self.dump_model is None:
            messagebox.showwarning("Advertencia", "Necesitas datos procesados y modelo cargado")
            return
        
        try:
            self.show_progress("Prediciendo vacancies desde dumps...")
            
            # Preparar datos
            if (hasattr(self, 'dump_feature_columns') and 
                self.dump_feature_columns is not None and 
                len(self.dump_feature_columns) > 0):
                # Usar features espec√≠ficas del modelo
                feature_cols = [col for col in self.dump_feature_columns 
                              if col in self.processed_dump_df.columns]
                if not feature_cols:
                    raise RuntimeError("No se encontraron features del modelo en los datos procesados")
                X_new = self.processed_dump_df[feature_cols].copy()
            else:
                # Usar todas las features num√©ricas excepto las prohibidas
                exclude_cols = ['file', 'estimated_vacancies']  # Excluir file y la estimaci√≥n inicial
                feature_cols = [col for col in self.processed_dump_df.columns 
                              if col not in exclude_cols and self.processed_dump_df[col].dtype in ['int64', 'float64']]
                
                if not feature_cols:
                    raise RuntimeError("No se encontraron features num√©ricas para la predicci√≥n")
                X_new = self.processed_dump_df[feature_cols].copy()
            
            # Aplicar escalado si existe
            if hasattr(self, 'dump_scaler') and self.dump_scaler is not None:
                X_new = pd.DataFrame(
                    self.dump_scaler.transform(X_new),
                    columns=X_new.columns,
                    index=X_new.index
                )
            
            # Hacer predicciones
            predictions = self.dump_model.predict(X_new)
            predictions = np.round(predictions).astype(int)
            
            # Crear DataFrame de resultados
            self.dump_predictions = self.processed_dump_df.copy()
            self.dump_predictions["vacancies_predicted"] = predictions
            
            # Calcular estad√≠sticas
            self.show_dump_prediction_results(predictions)
            
            # Habilitar bot√≥n de guardar
            self.dump_save_btn.config(state="normal")
            
            self.dump_status_label.config(
                text="‚úÖ Estado: Predicciones completadas exitosamente", 
                style="Success.TLabel"
            )
            
        except Exception as e:
            self.dump_status_label.config(text="‚ùå Estado: Error en predicci√≥n", style="Error.TLabel")
            messagebox.showerror("Error", f"Error en predicci√≥n:\n{str(e)}\n\n{traceback.format_exc()}")
        finally:
            self.hide_progress()
    
    def show_dump_prediction_results(self, predictions):
        """Mostrar resultados de predicciones desde dump"""
        results_text = "\n\nüîÆ PREDICCIONES DE VACANCIES COMPLETADAS\n" + "="*50 + "\n\n"
        
        # Estad√≠sticas b√°sicas de predicciones
        results_text += f"üìä ESTAD√çSTICAS DE PREDICCIONES:\n"
        results_text += f"   ‚Ä¢ Total archivos procesados: {len(predictions)}\n"
        results_text += f"   ‚Ä¢ Rango predicciones: {int(predictions.min())} - {int(predictions.max())}\n"
        results_text += f"   ‚Ä¢ Media: {predictions.mean():.2f}\n"
        results_text += f"   ‚Ä¢ Mediana: {np.median(predictions):.0f}\n"
        results_text += f"   ‚Ä¢ Desviaci√≥n est√°ndar: {predictions.std():.2f}\n\n"
        
        # Distribuci√≥n de predicciones
        value_counts = pd.Series(predictions).value_counts().sort_index()
        results_text += f"üìà DISTRIBUCI√ìN DE PREDICCIONES:\n"
        for i, (value, count) in enumerate(value_counts.items()):
            if i >= 10:  # Mostrar solo los primeros 10
                results_text += f"   ... y {len(value_counts)-10} valores m√°s\n"
                break
            results_text += f"   ‚Ä¢ {int(value)} vacancies: {count} archivos\n"
        
        # Comparaci√≥n con estimaci√≥n inicial (si existe)
        if 'estimated_vacancies' in self.dump_predictions.columns:
            initial_estimates = self.dump_predictions['estimated_vacancies'].values
            mae_vs_initial = np.mean(np.abs(predictions - initial_estimates))
            correlation = np.corrcoef(predictions, initial_estimates)[0,1] if len(set(predictions)) > 1 and len(set(initial_estimates)) > 1 else 0
            
            results_text += f"\nüîç COMPARACI√ìN CON ESTIMACI√ìN INICIAL:\n"
            results_text += f"   ‚Ä¢ MAE vs estimaci√≥n inicial: {mae_vs_initial:.2f}\n"
            results_text += f"   ‚Ä¢ Correlaci√≥n: {correlation:.3f}\n"
        
        # Tabla de primeros resultados
        results_text += f"\nüìã PRIMERAS 10 PREDICCIONES:\n"
        display_cols = ['file', 'vacancies_predicted']
        if 'estimated_vacancies' in self.dump_predictions.columns:
            display_cols.append('estimated_vacancies')
        if 'n_atoms' in self.dump_predictions.columns:
            display_cols.append('n_atoms')
        
        results_text += self.dump_predictions[display_cols].head(10).to_string(index=False)
        
        # Archivos con mayor y menor n√∫mero de vacancies
        sorted_preds = self.dump_predictions.sort_values('vacancies_predicted')
        results_text += f"\n\nüîª ARCHIVOS CON MENOS VACANCIES:\n"
        results_text += sorted_preds[['file', 'vacancies_predicted']].head(3).to_string(index=False)
        
        results_text += f"\n\nüî∫ ARCHIVOS CON M√ÅS VACANCIES:\n"
        results_text += sorted_preds[['file', 'vacancies_predicted']].tail(3).to_string(index=False)
        
        results_text += f"\n\n‚úÖ Predicciones completadas exitosamente"
        results_text += f"\nüíæ Usa 'Guardar Resultados' para exportar las predicciones"
        
        # Agregar al √°rea de resultados
        self.dump_results_text.insert(tk.END, results_text)
    
    def export_dump_csv(self):
        """Exportar dataset de features procesadas a CSV"""
        if self.processed_dump_df is None:
            messagebox.showwarning("Advertencia", "No hay datos procesados para exportar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Guardar dataset de features",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if file_path:
            try:
                self.show_progress("Exportando dataset de features...")
                
                if file_path.endswith('.xlsx'):
                    self.processed_dump_df.to_excel(file_path, index=False)
                else:
                    self.processed_dump_df.to_csv(file_path, index=False)
                
                messagebox.showinfo("√âxito", 
                                   f"‚úÖ Dataset exportado exitosamente:\n\n"
                                   f"üìÅ {file_path}\n"
                                   f"üìä {len(self.processed_dump_df)} filas\n"
                                   f"üîß {len(self.processed_dump_df.columns)} columnas")
            except Exception as e:
                messagebox.showerror("Error", f"Error exportando dataset:\n{str(e)}")
            finally:
                self.hide_progress()
    
    def save_dump_results(self):
        """Guardar resultados de predicciones desde dump"""
        if self.dump_predictions is None:
            messagebox.showwarning("Advertencia", "No hay predicciones para guardar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Guardar resultados de predicciones dump",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if file_path:
            try:
                self.show_progress("Guardando resultados de predicciones...")
                
                if file_path.endswith('.xlsx'):
                    self.dump_predictions.to_excel(file_path, index=False)
                else:
                    self.dump_predictions.to_csv(file_path, index=False)
                
                messagebox.showinfo("√âxito", 
                                   f"‚úÖ Predicciones guardadas exitosamente:\n\n"
                                   f"üìÅ {file_path}\n"
                                   f"üìä {len(self.dump_predictions)} archivos procesados\n"
                                   f"üîÆ Columna 'vacancies_predicted' a√±adida")
            except Exception as e:
                messagebox.showerror("Error", f"Error guardando predicciones:\n{str(e)}")
            finally:
                self.hide_progress()
    
    # ===============================================================
    # M√âTODOS EXISTENTES (funcionalidad CSV/Modelo cl√°sica)
    # ===============================================================
    
    def create_info_tab(self):
        """Crear pesta√±a de informaci√≥n"""
        info_frame = ttk.Frame(self.notebook, padding="10")
        self.notebook.add(info_frame, text="üìã Informaci√≥n")
        
        self.info_text = scrolledtext.ScrolledText(info_frame, height=20, wrap='word',
                                                  font=("Consolas", 10))
        self.info_text.pack(fill="both", expand=True)
        
        # Mensaje inicial
        welcome_msg = """üîÆ PREDICTOR DE VACANCIES ML - VERSI√ìN EXTENDIDA
=====================================================

üëã ¬°Bienvenido! Esta aplicaci√≥n te permite:

üîπ MODO CL√ÅSICO (CSV + Modelo):
   ‚Ä¢ Cargar archivos CSV con datos de materiales
   ‚Ä¢ Usar modelos entrenados (.joblib) para predecir vacancies
   ‚Ä¢ Visualizar resultados y estad√≠sticas

üîπ MODO DUMP PROCESSOR (NUEVO):
   ‚Ä¢ Procesar archivos LAMMPS dump directamente
   ‚Ä¢ Extraer features autom√°ticamente
   ‚Ä¢ Predecir vacancies sin necesidad de CSV previo

üìã PESTA√ëAS DISPONIBLES:
üîÆ Dump Processor - Procesa archivos .dump/.lammpstrj
üìä Resultados - Predicciones del modo cl√°sico
üìà Gr√°ficos - Visualizaciones del modo cl√°sico
üìà Estad√≠sticas - An√°lisis detallado del modo cl√°sico

üöÄ ¬°Explora las diferentes pesta√±as para comenzar!
"""
        self.info_text.insert(1.0, welcome_msg)
    
    def create_results_tab(self):
        """Crear pesta√±a de resultados"""
        results_frame = ttk.Frame(self.notebook, padding="10")
        self.notebook.add(results_frame, text="üìä Resultados CSV")
        
        self.results_text = scrolledtext.ScrolledText(results_frame, height=20, wrap='word',
                                                     font=("Consolas", 10))
        self.results_text.pack(fill="both", expand=True)
    
    def create_viz_tab(self):
        """Crear pesta√±a de visualizaci√≥n"""
        viz_frame = ttk.Frame(self.notebook, padding="10")
        self.notebook.add(viz_frame, text="üìà Gr√°ficos CSV")
        
        # Frame para controles de gr√°ficos
        viz_control_frame = ttk.Frame(viz_frame)
        viz_control_frame.pack(fill="x", pady=(0, 10))
        
        ttk.Button(viz_control_frame, text="üìä Distribuci√≥n", 
                  command=self.plot_distribution).pack(side="left", padx=(0, 5))
        ttk.Button(viz_control_frame, text="üéØ Precisi√≥n", 
                  command=self.plot_accuracy).pack(side="left", padx=(0, 5))
        ttk.Button(viz_control_frame, text="üîÑ Actualizar", 
                  command=self.update_visualization).pack(side="left")
        
        # Canvas para matplotlib
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.fig.tight_layout(pad=3.0)
        
        self.canvas = FigureCanvasTkAgg(self.fig, viz_frame)
        self.canvas.get_tk_widget().pack(fill="both", expand=True)
    
    def create_stats_tab(self):
        """Crear pesta√±a de estad√≠sticas"""
        stats_frame = ttk.Frame(self.notebook, padding="10")
        self.notebook.add(stats_frame, text="üìà Estad√≠sticas CSV")
        
        self.stats_text = scrolledtext.ScrolledText(stats_frame, height=20, wrap='word',
                                                   font=("Consolas", 10))
        self.stats_text.pack(fill="both", expand=True)
    
    def show_progress(self, message):
        """Mostrar barra de progreso"""
        self.progress_label.config(text=message)
        self.progress_label.pack(pady=5)
        self.progress.pack(fill="x", pady=5)
        self.progress.start()
        self.root.update()
    
    def hide_progress(self):
        """Ocultar barra de progreso"""
        self.progress.stop()
        self.progress.pack_forget()
        self.progress_label.pack_forget()
    
    def load_csv(self):
        """Cargar archivo CSV"""
        file_path = filedialog.askopenfilename(
            title="Seleccionar archivo CSV",
            filetypes=[("CSV files", "*.csv"), ("Todos los archivos", "*.*")]
        )
        
        if file_path:
            try:
                self.show_progress("Cargando CSV...")
                
                self.csv_path = file_path
                self.df = pd.read_csv(file_path)
                
                self.csv_label.config(text=f"üìÑ CSV: {Path(file_path).name} ({len(self.df):,} filas)", 
                                    style="Success.TLabel")
                
                # Mostrar informaci√≥n del CSV
                info = f"""üìÑ ARCHIVO CSV CARGADO
========================

üìÅ Archivo: {Path(file_path).name}
üìÅ Ubicaci√≥n: {file_path}
üìä Dimensiones: {len(self.df):,} filas √ó {len(self.df.columns)} columnas

üîß COLUMNAS DISPONIBLES ({len(self.df.columns)}):
{self._format_columns_list(self.df.columns.tolist())}

üìã INFORMACI√ìN GENERAL:
‚Ä¢ Memoria utilizada: {self.df.memory_usage(deep=True).sum() / 1024**2:.1f} MB
‚Ä¢ Valores nulos: {self.df.isnull().sum().sum():,}
‚Ä¢ Tipos de datos: {dict(self.df.dtypes.value_counts())}

üìÅ PRIMERAS 5 FILAS:
{self.df.head().to_string()}

‚úÖ CSV cargado correctamente
"""
                self.info_text.delete(1.0, tk.END)
                self.info_text.insert(1.0, info)
                
                self.check_ready()
                
            except Exception as e:
                self.csv_label.config(text="üìÑ CSV: Error al cargar", style="Error.TLabel")
                messagebox.showerror("Error", f"Error cargando CSV:\n{str(e)}\n\n{traceback.format_exc()}")
            finally:
                self.hide_progress()
    
    def _format_columns_list(self, columns, max_per_line=4):
        """Formatear lista de columnas para mostrar"""
        formatted = []
        for i in range(0, len(columns), max_per_line):
            line_cols = columns[i:i+max_per_line]
            formatted.append("  ‚Ä¢ " + " | ".join(line_cols))
        return "\n".join(formatted)
    
    def load_model(self):
        """Cargar modelo .joblib"""
        file_path = filedialog.askopenfilename(
            title="Seleccionar modelo .joblib",
            filetypes=[("Joblib files", "*.joblib"), ("Todos los archivos", "*.*")]
        )
        
        if file_path:
            try:
                self.show_progress("Cargando modelo...")
                
                self.model_path = file_path
                model_data = joblib.load(file_path)
                
                # Manejar diferentes formatos de modelo
                if isinstance(model_data, dict):
                    # Formato con metadatos
                    self.model = model_data["model"]
                    self.feature_columns = model_data["feature_columns"]
                    self.scaler = model_data.get("scaler", None)
                    training_params = model_data.get("training_params", {})
                else:
                    # Formato simple (solo modelo)
                    self.model = model_data
                    self.feature_columns = None
                    self.scaler = None
                    training_params = {}
                
                self.model_label.config(text=f"ü§ñ Modelo: {Path(file_path).name}", 
                                      style="Success.TLabel")
                
                # Mostrar informaci√≥n del modelo
                model_info = f"""ü§ñ MODELO CARGADO
================

üìÅ Archivo: {Path(file_path).name}
üìÅ Ubicaci√≥n: {file_path}
üß† Tipo: {type(self.model).__name__}

"""
                
                if self.feature_columns:
                    model_info += f"""üîß FEATURES REQUERIDAS ({len(self.feature_columns)}):
{self._format_columns_list(self.feature_columns)}

"""
                
                if self.scaler:
                    model_info += f"üìä Escalador: {type(self.scaler).__name__}\n"
                
                if training_params:
                    model_info += f"""‚öôÔ∏è PAR√ÅMETROS DE ENTRENAMIENTO:
{self._format_params(training_params)}

"""
                
                if hasattr(self.model, 'get_params'):
                    params = self.model.get_params()
                    model_info += f"""üîß PAR√ÅMETROS DEL MODELO:
{self._format_params(params, max_items=8)}

"""
                
                model_info += "‚úÖ Modelo cargado correctamente"
                
                # Agregar a la informaci√≥n existente
                current_info = self.info_text.get(1.0, tk.END)
                self.info_text.insert(tk.END, "\n" + "="*50 + "\n" + model_info)
                
                self.check_ready()
                
            except Exception as e:
                self.model_label.config(text="ü§ñ Modelo: Error al cargar", style="Error.TLabel")
                messagebox.showerror("Error", f"Error cargando modelo:\n{str(e)}\n\n{traceback.format_exc()}")
            finally:
                self.hide_progress()
    
    def _format_params(self, params_dict, max_items=None):
        """Formatear diccionario de par√°metros"""
        items = list(params_dict.items())
        if max_items:
            items = items[:max_items]
        
        formatted = []
        for key, value in items:
            if isinstance(value, float):
                formatted.append(f"  ‚Ä¢ {key}: {value:.4f}")
            else:
                formatted.append(f"  ‚Ä¢ {key}: {value}")
        
        if max_items and len(params_dict) > max_items:
            formatted.append(f"  ‚Ä¢ ... y {len(params_dict) - max_items} par√°metros m√°s")
        
        return "\n".join(formatted)
    
    def check_ready(self):
        """Verificar si se puede predecir"""
        if (self.csv_path and self.model_path and 
            hasattr(self, 'df') and self.df is not None and
            hasattr(self, 'model') and self.model is not None):
            
            if self.feature_columns:
                # Verificar que todas las features est√©n en el CSV
                missing_features = [feat for feat in self.feature_columns if feat not in self.df.columns]
                if missing_features:
                    self.predict_btn.config(state="disabled")
                    error_msg = f"‚ö†Ô∏è Features faltantes en CSV:\n{', '.join(missing_features)}"
                    messagebox.showwarning("Features Faltantes", error_msg)
                    return
            
            # Todo listo para predecir
            self.predict_btn.config(state="normal")
        else:
            self.predict_btn.config(state="disabled")
    
    def predict(self):
        """Realizar predicciones"""
        try:
            self.show_progress("Realizando predicciones...")
            
            # Preparar datos
            if self.feature_columns:
                X_new = self.df[self.feature_columns].copy()
            else:
                # Si no hay feature_columns, usar todas excepto 'vacancies' y columnas de texto
                exclude_cols = ['vacancies', 'file', 'filename', 'file_processed']
                feature_cols = [col for col in self.df.columns 
                              if col not in exclude_cols and self.df[col].dtype in ['int64', 'float64']]
                X_new = self.df[feature_cols].copy()
                self.feature_columns = feature_cols
            
            # Aplicar escalado si existe
            if self.scaler is not None:
                X_new = pd.DataFrame(
                    self.scaler.transform(X_new),
                    columns=self.feature_columns,
                    index=self.df.index
                )
            
            # Hacer predicciones
            predictions = self.model.predict(X_new)
            predictions = np.round(predictions).astype(int)
            
            # Agregar predicciones al DataFrame
            self.predictions_df = self.df.copy()
            self.predictions_df["vacancies_predicted"] = predictions
            
            # Calcular estad√≠sticas
            self.calculate_and_show_stats(predictions)
            
            # Crear visualizaci√≥n
            self.update_visualization()
            
            # Habilitar bot√≥n de guardar
            self.save_btn.config(state="normal")
            
            # Cambiar a pesta√±a de resultados
            self.notebook.select(1)
            
        except Exception as e:
            messagebox.showerror("Error", f"Error en predicci√≥n:\n{str(e)}\n\n{traceback.format_exc()}")
        finally:
            self.hide_progress()
    
    def calculate_and_show_stats(self, predictions):
        """Calcular y mostrar estad√≠sticas"""
        has_real_vacancies = "vacancies" in self.df.columns
        
        # Estad√≠sticas b√°sicas
        stats_text = f"""üîÆ RESULTADOS DE PREDICCI√ìN
==========================

üìä ESTAD√çSTICAS GENERALES:
‚Ä¢ Total de predicciones: {len(predictions):,}
‚Ä¢ Rango de predicciones: {int(predictions.min())} - {int(predictions.max())}
‚Ä¢ Media: {predictions.mean():.2f}
‚Ä¢ Mediana: {np.median(predictions):.0f}
‚Ä¢ Desviaci√≥n est√°ndar: {predictions.std():.2f}

üìà DISTRIBUCI√ìN DE PREDICCIONES:
{self._format_value_counts(predictions)}

"""
        
        if has_real_vacancies:
            actual = self.df["vacancies"].values
            mae = np.mean(np.abs(predictions - actual))
            rmse = np.sqrt(np.mean((predictions - actual)**2))
            exact_accuracy = np.mean(predictions == actual)
            
            # Precisi√≥n por rangos
            tolerance_1 = np.mean(np.abs(predictions - actual) <= 1)
            tolerance_2 = np.mean(np.abs(predictions - actual) <= 2)
            
            stats_text += f"""üéØ M√âTRICAS DE EVALUACI√ìN:
‚Ä¢ MAE (Error Absoluto Medio): {mae:.2f}
‚Ä¢ RMSE (Ra√≠z Error Cuadr√°tico Medio): {rmse:.2f}
‚Ä¢ Precisi√≥n exacta: {exact_accuracy:.1%}
‚Ä¢ Precisi√≥n ¬±1: {tolerance_1:.1%}
‚Ä¢ Precisi√≥n ¬±2: {tolerance_2:.1%}

üìä COMPARACI√ìN REAL vs PREDICHO:
‚Ä¢ Correlaci√≥n: {np.corrcoef(actual, predictions)[0,1]:.3f}
‚Ä¢ R¬≤: {1 - np.sum((actual - predictions)**2) / np.sum((actual - actual.mean())**2):.3f}

"""
        
        # Mostrar primeras predicciones
        display_cols = []
        if 'file' in self.predictions_df.columns:
            display_cols.append('file')
        display_cols.append('vacancies_predicted')
        if has_real_vacancies:
            display_cols.append('vacancies')
        
        stats_text += f"""üìÅ PRIMERAS 10 PREDICCIONES:
{self.predictions_df[display_cols].head(10).to_string()}

‚úÖ Predicciones completadas exitosamente
"""
        
        self.results_text.delete(1.0, tk.END)
        self.results_text.insert(1.0, stats_text)
        
        # Tambi√©n mostrar en pesta√±a de estad√≠sticas
        self.stats_text.delete(1.0, tk.END)
        self.stats_text.insert(1.0, stats_text)
    
    def _format_value_counts(self, values, max_show=10):
        """Formatear conteo de valores"""
        value_counts = pd.Series(values).value_counts().sort_index()
        
        formatted = []
        for i, (value, count) in enumerate(value_counts.items()):
            if i >= max_show:
                formatted.append(f"  ... y {len(value_counts) - max_show} valores m√°s")
                break
            formatted.append(f"  ‚Ä¢ {int(value)}: {count:,} casos")
        
        return "\n".join(formatted)
    
    def update_visualization(self):
        """Actualizar visualizaci√≥n"""
        if self.predictions_df is None:
            return
        
        # Limpiar axes
        for ax in self.axes.flat:
            ax.clear()
        
        predictions = self.predictions_df["vacancies_predicted"]
        has_real = "vacancies" in self.predictions_df.columns
        
        # Plot 1: Distribuci√≥n de predicciones
        self.axes[0,0].hist(predictions, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        self.axes[0,0].set_title('Distribuci√≥n de Predicciones')
        self.axes[0,0].set_xlabel('Vacancies Predichas')
        self.axes[0,0].set_ylabel('Frecuencia')
        self.axes[0,0].grid(True, alpha=0.3)
        
        if has_real:
            actual = self.predictions_df["vacancies"]
            
            # Plot 2: Scatter plot predicciones vs reales
            self.axes[0,1].scatter(actual, predictions, alpha=0.6, color='coral')
            max_val = max(actual.max(), predictions.max())
            self.axes[0,1].plot([0, max_val], [0, max_val], 'r--', label='Predicci√≥n perfecta')
            self.axes[0,1].set_xlabel('Valores Reales')
            self.axes[0,1].set_ylabel('Predicciones')
            self.axes[0,1].set_title('Predicciones vs Valores Reales')
            self.axes[0,1].legend()
            self.axes[0,1].grid(True, alpha=0.3)
            
            # Plot 3: Distribuci√≥n de errores
            errors = predictions - actual
            self.axes[1,0].hist(errors, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
            self.axes[1,0].set_title('Distribuci√≥n de Errores')
            self.axes[1,0].set_xlabel('Error (Predicho - Real)')
            self.axes[1,0].set_ylabel('Frecuencia')
            self.axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7)
            self.axes[1,0].grid(True, alpha=0.3)
            
            # Plot 4: Box plot de errores por rango de vacancies
            try:
                # Crear bins para rangos de vacancies
                bins = pd.cut(actual, bins=5)
                error_data = [errors[bins == bin_val].values for bin_val in bins.categories]
                self.axes[1,1].boxplot(error_data, labels=[f'{int(cat.left)}-{int(cat.right)}' for cat in bins.categories])
                self.axes[1,1].set_title('Errores por Rango de Vacancies')
                self.axes[1,1].set_xlabel('Rango de Vacancies Reales')
                self.axes[1,1].set_ylabel('Error')
                self.axes[1,1].grid(True, alpha=0.3)
            except:
                # Si falla el boxplot, hacer un gr√°fico simple
                self.axes[1,1].text(0.5, 0.5, 'Gr√°fico no disponible', 
                                   transform=self.axes[1,1].transAxes, ha='center')
        else:
            # Sin valores reales, mostrar solo an√°lisis de predicciones
            self.axes[0,1].text(0.5, 0.5, 'Sin valores reales\npara comparar', 
                               transform=self.axes[0,1].transAxes, ha='center', va='center')
            self.axes[1,0].text(0.5, 0.5, 'Sin valores reales\npara calcular errores', 
                               transform=self.axes[1,0].transAxes, ha='center', va='center')
            self.axes[1,1].text(0.5, 0.5, 'Sin valores reales\npara an√°lisis', 
                               transform=self.axes[1,1].transAxes, ha='center', va='center')
        
        self.fig.tight_layout()
        self.canvas.draw()
    
    def plot_distribution(self):
        """Actualizar gr√°fico de distribuci√≥n"""
        self.update_visualization()
    
    def plot_accuracy(self):
        """Mostrar gr√°fico de precisi√≥n"""
        self.update_visualization()
    
    def save_results(self):
        """Guardar resultados en CSV"""
        if self.predictions_df is None:
            messagebox.showwarning("Advertencia", "No hay resultados para guardar")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Guardar resultados",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv")]
        )
        
        if file_path:
            try:
                self.show_progress("Guardando resultados...")
                
                self.predictions_df.to_csv(file_path, index=False)
                messagebox.showinfo("√âxito", 
                                   f"‚úÖ Resultados guardados exitosamente:\n\n"
                                   f"üìÅ {file_path}\n"
                                   f"üìä {len(self.predictions_df):,} filas guardadas")
            except Exception as e:
                messagebox.showerror("Error", f"Error guardando:\n{str(e)}")
            finally:
                self.hide_progress()

# Ejecutar la aplicaci√≥n
if __name__ == "__main__":
    root = tk.Tk()
    app = PredictorGUI(root)
    root.mainloop()