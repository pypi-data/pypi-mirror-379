{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Fall Detection Example ðŸš¨\n",
    "\n",
    "YOLOv8-based fall detection system demonstration\n",
    "\n",
    "## ðŸ“‹ Features\n",
    "- Support for both PyTorch (.pt) and ONNX (.onnx) models\n",
    "- FP16 quantization support for faster inference\n",
    "- Real-time video processing\n",
    "- Easy-to-use API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import fall_detection\nfrom IPython.display import Image, display\n\n# Detect single image (using ONNX model by default)\nresult = fall_detection.process_image(\n    \"test_src/test.jpg\",\n    \"results/output.jpg\",\n    model_type=\"onnx\",  # Choose: 'onnx' (FP16, faster) or 'pt' (PyTorch)\n    confidence=0.36,\n)\n\n# Show results\nprint(f\"Fall detected: {result['fall_detected']}\")\nprint(f\"Fall count: {result['fall_count']}\")\nprint(f\"Normal count: {result['normal_count']}\")\n\n# Display image\nif result['image_saved']:\n    display(Image(filename=result['image_saved']))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import fall_detection\n\n# Process video (using ONNX model by default)\nresult = fall_detection.process_video(\n    \"test_src/test.mp4\",\n    \"results/fall_detection/output.mp4\",\n    model_type=\"onnx\",  # Choose: 'onnx' (FP16, faster) or 'pt' (PyTorch)\n    confidence=0.36\n)\n\nprint(f\"Total frames: {result['total_frames']}\")\nprint(f\"Fall frames: {result['fall_frames']}\")\nprint(f\"Total fall detections: {result['statistics']['fall_count']}\")\nprint(f\"Total normal detections: {result['statistics']['normal_count']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Detector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fall_detection import FallDetector\nimport cv2\nfrom IPython.display import Image, display\n\n# Create detector with ONNX model\ndetector = FallDetector(\n    model_type=\"onnx\",  # Choose: 'onnx' (FP16, faster) or 'pt' (PyTorch)\n    confidence=0.6\n)\n\n# Read image\nimg = cv2.imread(\"test_src/test.jpg\")\n\n# Detect\nresult = detector.detect_image(img)\n\n# Save result\ncv2.imwrite(\"results/output.jpg\", result['annotated_image'])\n\nprint(f\"Detection complete!\")\nprint(f\"Falls: {result['fall_count']}\")\nprint(f\"Normal: {result['normal_count']}\")\n\n# Display result\ndisplay(Image(filename=\"results/output.jpg\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-time Camera Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fall_detection import FallDetector\nimport cv2\n\n# Create detector with ONNX model\ndetector = FallDetector(\n    model_type=\"onnx\",  # Choose: 'onnx' (FP16, faster) or 'pt' (PyTorch)\n    confidence=0.5\n)\n\n# Open camera\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Detect\n    result = detector.detect_image(frame)\n    \n    # Show\n    cv2.imshow('Fall Detection', result['annotated_image'])\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\nprint(\"Camera closed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using ONNX Model for Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fall_detection import FallDetector\nimport cv2\nfrom IPython.display import Image, display\n\n# Create detector using ONNX model\ndetector = FallDetector(\n    model_type=\"onnx\",  # FP16 ONNX model for faster inference\n    confidence=0.5,\n    device=\"cuda\"  # Use GPU acceleration\n)\n\n# Read image\nimg = cv2.imread(\"test_src/test.jpg\")\n\n# Run detection\nresult = detector.detect_image(img)\n\n# Save result\ncv2.imwrite(\"results/onnx_output.jpg\", result['annotated_image'])\n\nprint(f\"âœ… ONNX model detection complete!\")\nprint(f\"Fall count: {result['fall_count']}\")\nprint(f\"Normal count: {result['normal_count']}\")\nprint(f\"Inference speed: 2-3x faster than PyTorch model\")\n\n# Display result\ndisplay(Image(filename=\"results/onnx_output.jpg\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using ONNX Model for Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fall_detection import FallDetector\n\n# Process video using ONNX model\ndetector = FallDetector(\n    model_type=\"onnx\",  # FP16 ONNX model\n    confidence=0.36,\n    device=\"cuda\"\n)\n\n# Process video file\nresult = detector.process_video_file(\n    input_path=\"test_src/test.mp4\",\n    output_path=\"results/fall_detection/output_onnx.mp4\",\n    show_progress=True\n)\n\nprint(f\"âœ… ONNX model video processing complete!\")\nprint(f\"Total frames: {result['total_frames']}\")\nprint(f\"Fall frames: {result['fall_frames']}\")\nprint(f\"Total fall detections: {result['statistics']['fall_count']}\")\nprint(f\"Total normal detections: {result['statistics']['normal_count']}\")\nprint(f\"\\nPerformance boost: FP16 ONNX model processes ~2-3x faster\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-fall-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}