{
  "id": "242a6140-7341-49ca-876b-c01366b39b84",
  "revision": 0,
  "last_node_id": 41,
  "last_link_id": 47,
  "nodes": [
    {
      "id": 10,
      "type": "CLIPLoader",
      "pos": [
        -485,
        164
      ],
      "size": [
        370,
        106
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            14
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "CLIPLoader",
        "models": [
          {
            "name": "t5xxl_fp16.safetensors",
            "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors",
            "directory": "text_encoders"
          }
        ]
      },
      "widgets_values": [
        "t5xxl_fp16.safetensors",
        "chroma",
        "default"
      ]
    },
    {
      "id": 13,
      "type": "UNETLoader",
      "pos": [
        -485,
        40
      ],
      "size": [
        370,
        82
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            30
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "UNETLoader",
        "models": [
          {
            "name": "Chroma1-Radiance-v0.1.safetensors",
            "url": "https://huggingface.co/lodestones/Chroma1-Radiance/resolve/main/Chroma1-Radiance-v0.1.safetensors",
            "directory": "diffusion_models"
          }
        ]
      },
      "widgets_values": [
        "Chroma1-Radiance-v0.1.safetensors",
        "default"
      ]
    },
    {
      "id": 7,
      "type": "CLIPTextEncode",
      "pos": [
        230,
        250
      ],
      "size": [
        425.27801513671875,
        180.6060791015625
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 12
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            6
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "low quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs, hands, fingers"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 11,
      "type": "T5TokenizerOptions",
      "pos": [
        -80,
        170
      ],
      "size": [
        270,
        82
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 14
        }
      ],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            12,
            20
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "T5TokenizerOptions"
      },
      "widgets_values": [
        0,
        3
      ]
    },
    {
      "id": 3,
      "type": "KSampler",
      "pos": [
        700,
        140
      ],
      "size": [
        315,
        262
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 31
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 4
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 6
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 47
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            7
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "KSampler"
      },
      "widgets_values": [
        461052028503273,
        "randomize",
        30,
        4,
        "euler",
        "simple",
        1
      ]
    },
    {
      "id": 15,
      "type": "VAELoader",
      "pos": [
        260,
        500
      ],
      "size": [
        370,
        58
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "links": [
            41
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "VAELoader"
      },
      "widgets_values": [
        "pixel_space"
      ]
    },
    {
      "id": 34,
      "type": "EmptyChromaRadianceLatentImage",
      "pos": [
        -460,
        370
      ],
      "size": [
        326.3540954589844,
        106
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            47
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "EmptyChromaRadianceLatentImage"
      },
      "widgets_values": [
        1024,
        1024,
        1
      ]
    },
    {
      "id": 22,
      "type": "ModelSamplingAuraFlow",
      "pos": [
        700,
        20
      ],
      "size": [
        315,
        60
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 30
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            31
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "ModelSamplingAuraFlow"
      },
      "widgets_values": [
        3.0000000000000004
      ]
    },
    {
      "id": 8,
      "type": "VAEDecode",
      "pos": [
        700,
        460
      ],
      "size": [
        315,
        46
      ],
      "flags": {
        "collapsed": false
      },
      "order": 12,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 7
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 41
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            35
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "VAEDecode"
      },
      "widgets_values": []
    },
    {
      "id": 14,
      "type": "Note",
      "pos": [
        -80,
        20
      ],
      "size": [
        270,
        90
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "min_padding 1 is supposed to be the official way to inference chroma but I think the results are better with min_padding 0\n"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 6,
      "type": "CLIPTextEncode",
      "pos": [
        230,
        40
      ],
      "size": [
        422.84503173828125,
        164.31304931640625
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 20
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            4
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "Hyperrealistic macro photograph of a team of tiny bakers—each precisely 2 inches tall—collaborating on an enormous, golden-brown croissant with flaky, layered textures. The bakers are engaged in dynamic, detailed actions: one uses a miniature wooden bucket to spread rich, creamy butter between the croissant’s layers, another climbs a thin rope ladder to evenly pipe smooth, glossy chocolate filling onto the top, and a third brushes a light egg wash with a tiny pastry brush. The scene is bathed in warm, soft kitchen lighting with cinematic depth—subtle highlights on the croissant’s golden crust, gentle shadows that emphasize texture, and a soft glow from overhead pendant lights. Floating flour dust particles catch the light, adding a sense of movement and realism, while tiny details like the bakers’ stitched cloth aprons, smudged flour on their faces, the rough wood of the worktable, and the slight sheen of melted butter on the croissant are rendered with ultra-precision. Ultra-detailed, 8K resolution, photorealistic textures, sharp focus on the bakers and croissant, shallow depth of field to blur the background slightly, rich warm color palette, lifelike proportions, and a cozy, whimsical atmosphere that balances realism with charm.\n"
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 26,
      "type": "SaveImage",
      "pos": [
        1030,
        20
      ],
      "size": [
        1018.58642578125,
        1102.2607421875
      ],
      "flags": {},
      "order": 13,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 35
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.59"
      },
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 38,
      "type": "Note",
      "pos": [
        260,
        600
      ],
      "size": [
        370,
        110
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "About pixel_space",
      "properties": {},
      "widgets_values": [
        "This Chroma1 Radiance image generation is in pixel space. No additional VAE is needed."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 40,
      "type": "MarkdownNote",
      "pos": [
        -1000,
        0
      ],
      "size": [
        490,
        420
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "## Model links\n\nThis model is still iterating, you can visit [lodestones/Chroma1-Radiance](https://huggingface.co/lodestones/Chroma1-Radiance) to get the latest version\n\n**diffusion_models**\n- [Chroma1-Radiance-v0.1.safetensors](https://huggingface.co/lodestones/Chroma1-Radiance/resolve/main/Chroma1-Radiance-v0.1.safetensors)\n\n**text_encoders**\n- [t5xxl_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors)\n\n```\n📂 ComfyUI/\n├─ 📂 models/\n│   ├─ 📂 diffusion_models/\n│   │     └── Chroma1-Radiance-v0.1.safetensors\n│   ├─ 📂 text_encoders/\n│   │     └── t5xxl_fp16.safetensors \n```\n\n\n## About Chroma1-Radiance\n\nChroma1-Radiance is a text-to-image model that generates pictures from words. What makes it special is that it works directly with the image's pixels, instead of using a compressed version. This helps it create higher-quality images with fewer visual mistakes like blurriness or distorted details. It's a new and promising model that's trying a different way to improve how images are made.\n\nIf you want to keep update with Chroma1-Radiance you can follow [lodestones](https://x.com/LodestoneE621)\n"
      ],
      "color": "#432",
      "bgcolor": "#653"
    }
  ],
  "links": [
    [
      4,
      6,
      0,
      3,
      1,
      "CONDITIONING"
    ],
    [
      6,
      7,
      0,
      3,
      2,
      "CONDITIONING"
    ],
    [
      7,
      3,
      0,
      8,
      0,
      "LATENT"
    ],
    [
      12,
      11,
      0,
      7,
      0,
      "CLIP"
    ],
    [
      14,
      10,
      0,
      11,
      0,
      "CLIP"
    ],
    [
      20,
      11,
      0,
      6,
      0,
      "CLIP"
    ],
    [
      30,
      13,
      0,
      22,
      0,
      "MODEL"
    ],
    [
      31,
      22,
      0,
      3,
      0,
      "MODEL"
    ],
    [
      35,
      8,
      0,
      26,
      0,
      "IMAGE"
    ],
    [
      41,
      15,
      0,
      8,
      1,
      "VAE"
    ],
    [
      47,
      34,
      0,
      3,
      3,
      "LATENT"
    ]
  ],
  "groups": [
    {
      "id": 1,
      "title": "Step 1 - Load models",
      "bounding": [
        -490,
        -30,
        390,
        313.6000061035156
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 2,
      "title": "Step3 - Prompt",
      "bounding": [
        220,
        -30,
        445.27801513671875,
        474.2060852050781
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 3,
      "title": "Step 2 - Image size",
      "bounding": [
        -490,
        300,
        390,
        190
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    }
  ],
  "config": {},
  "extra": {
    "ds": {
      "scale": 0.4914108634365025,
      "offset": [
        1632.196674315222,
        291.0511957579497
      ]
    },
    "frontendVersion": "1.26.11",
    "VHS_latentpreview": false,
    "VHS_latentpreviewrate": 0,
    "VHS_MetadataImage": true,
    "VHS_KeepIntermediate": true
  },
  "version": 0.4
}