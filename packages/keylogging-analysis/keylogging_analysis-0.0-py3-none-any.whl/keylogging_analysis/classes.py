#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Filename: keylogging_analysis.py
Author: Kobe Thonissen
Date: 2025-09-03
Version: 0.0.1
Description: definition of class KeyLoggingDataFrame for preprocessing and analysing keylogging data collected in the platforms Language Hero and Language Lab
"""

# structure of class methods
# 1. preprocessing methods

import pandas as pd
import numpy as np
import warnings
from pathlib import Path
import help_functions as hf

class KeyLoggingDataFrame(pd.DataFrame):
    _metadata = ['system']

    def __init__(self, *args, **kwargs):
        # Initialize as an empty DataFrame
        super().__init__(*args, **kwargs)
        if self.empty:
            self._metadata = []
        self.system = None

    @property
    def _constructor(self): # return KeyLoggingDataFrame when making copy
        return KeyLoggingDataFrame

    def _drop_anonymized(self):
        before = len(self)
        self = self[self['message_content'].str.contains('â– ', na=False)].index.tolist()
        after = len(self)
        print(f"Dropped {before - after} events occurring in anonymized messages.")
          
    def _drop_nonsense(self):
        message_ids = [57357, 57363, 73747, 65564, 57386, 57398, 57403, 57406, 57408, 24642, 49219, 24644, 49221, 24646, 57410, 24648, 57413, 57417, 57420, 24656, 57426, 57428, 41045, 57430, 24664, 65625, 57434, 49244, 57436, 57440, 24674, 41059, 57442, 57444, 24678, 57447, 24680, 24682, 24684, 57452, 24686, 57454, 24688, 57456, 24690, 73838, 49268, 24693, 24695, 73848, 24697, 57465, 24699, 24701, 24703, 24705, 57474, 24707, 73857, 24709, 41093, 24711, 24713, 49290, 24715, 24717, 24719, 24721, 24723, 41107, 24725, 65686, 24727, 24729, 49306, 24731, 73881, 24733, 49310, 24735, 24737, 57505, 24739, 24741, 24743, 24745, 24747, 73900, 24749, 73902, 24751, 24753, 24755, 32947, 24757, 24759, 24761, 24763, 73916, 24765, 24767, 73919, 24769, 24771, 24773, 24775, 24777, 24779, 24781, 24783, 24785, 24787, 24789, 65749, 24791, 24793, 24795, 24797, 24799, 24801, 24803, 24805, 24807, 24809, 24811, 24813, 24815, 24817, 24819, 24821, 24823, 57592, 24825, 24827, 57595, 24829, 24831, 57599, 24833, 24835, 24837, 24839, 24841, 24847, 24849, 57618, 24851, 24853, 24855, 24857, 24859, 24861, 24863, 57631, 24865, 24867, 24869, 24871, 24873, 41257, 24875, 24877, 33069, 24879, 24881, 24883, 57652, 24885, 33077, 24887, 74038, 24889, 24893, 41281, 24937, 49524, 65948, 65962, 41389, 41391, 41399, 41403, 41405, 41409, 41411, 41413, 41415, 41417, 41421, 41423, 41425, 49618, 41427, 74195, 33238, 25055, 74209, 25061, 33263, 33274, 57851, 49664, 25089, 57859, 25099, 25101, 33300, 25111, 33304, 25113, 57882, 25115, 66076, 25117, 25119, 25121, 25123, 25125, 25129, 25131, 25133, 57901, 25135, 25137, 25141, 49720, 25145, 57912, 25149, 25153, 25157, 25162, 52220, 57932, 25166, 25169, 57937, 16981, 16982, 25173, 16985, 25177, 74333, 33374, 57952, 16994, 33378, 25189, 25192, 57961, 49770, 57964, 25197, 17006, 25201, 49778, 57969, 49780, 25205, 57973, 25208, 49784, 49786, 49788, 25213, 49790, 57980, 17024, 25217, 33410, 17027, 49792, 25221, 49794, 49796, 25224, 49798, 25228, 49804, 49806, 57998, 57999, 25233, 49810, 25235, 49812, 25237, 25239, 17048, 25241, 33432, 25243, 58012, 25245, 17055, 25247, 25249, 25251, 25253, 25255, 25258, 25260, 33452, 25262, 33455, 25264, 58028, 25266, 25268, 74420, 25270, 41655, 25272, 25274, 58043, 25276, 25278, 25280, 58048, 25282, 25284, 25286, 58054, 25288, 49864, 17098, 25290, 25292, 33482, 25294, 74444, 17104, 25296, 25298, 58064, 25300, 25302, 25304, 25306, 25308, 25310, 25312, 25314, 25316, 33511, 25324, 25328, 17147, 33531, 58110, 58118, 58121, 33546, 74507, 33555, 25366, 33559, 49949, 58142, 17184, 25378, 25380, 33572, 25382, 74535, 25384, 25386, 25388, 58156, 25390, 33582, 25392, 33584, 25394, 25396, 25398, 25400, 17209, 25402, 17211, 25404, 25406, 25408, 25410, 33602, 25412, 58179, 25414, 25416, 25418, 25420, 25422, 58190, 25424, 58191, 25426, 33619, 25428, 58196, 25430, 25432, 25434, 25436, 58204, 25438, 25440, 25442, 33635, 25444, 58213, 25446, 74597, 25448, 74599, 25450, 25452, 33645, 25458, 17268, 25460, 25462, 58228, 25464, 25466, 25468, 25470, 50047, 25472, 25474, 33666, 25476, 41858, 25478, 25480, 25482, 25484, 33677, 25486, 25488, 25490, 25492, 25494, 33686, 25496, 25498, 25500, 25502, 25504, 25506, 25508, 25510, 33703, 25512, 58280, 25514, 25516, 25518, 66481, 58290, 25526, 66487, 25528, 66489, 25530, 25532, 25534, 58303, 17345, 58307, 58310, 58320, 58322, 17363, 58334, 74737, 33781, 33786, 33792, 41987, 33798, 74759, 33804, 58380, 33806, 58384, 58389, 33819, 58397, 58405, 74801, 33846, 17476, 33864, 33868, 25678, 25680, 25682, 25684, 25686, 58455, 25688, 25690, 25692, 58461, 25694, 25696, 33888, 25698, 42083, 25700, 25702, 25704, 25706, 25708, 25710, 25712, 58480, 25714, 25716, 25718, 25720, 25722, 58491, 25724, 25726, 25728, 25730, 25732, 25734, 25736, 25738, 58506, 25740, 25742, 25744, 25746, 25752, 25754, 25756, 25758, 58526, 25760, 25762, 58530, 25768, 25770, 25772, 25774, 25776, 33969, 25778, 25780, 25782, 25784, 25786, 33981, 58559, 58560, 33985, 58563, 33993, 33995, 74956, 50385, 50389, 17636, 58606, 58609, 25846, 58614, 25848, 25850, 25852, 25854, 25856, 25858, 25860, 25862, 25864, 25866, 25868, 25870, 25874, 25876, 25878, 25880, 58648, 25882, 25884, 25886, 25888, 25890, 25892, 25894, 25896, 25898, 58678, 42296, 34108, 17725, 58699, 42318, 42320, 17745, 58709, 17757, 34143, 34152, 58753, 58759, 42385, 42387, 58771, 17823, 42420, 58808, 17851, 17852, 17854, 58816, 26053, 26055, 26057, 67017, 26059, 58828, 26061, 26065, 26067, 17876, 26069, 34260, 26071, 26075, 26076, 34268, 58844, 26087, 26088, 26091, 26092, 26095, 26096, 42482, 26099, 26100, 58870, 26103, 26104, 58874, 26107, 26108, 75261, 26111, 26112, 26115, 26116, 42501, 26119, 26120, 26123, 26124, 42509, 58894, 26127, 26128, 42511, 26131, 26132, 42517, 26135, 26136, 17947, 26139, 17949, 26140, 26143, 17952, 17953, 26144, 26147, 42523, 26149, 42525, 26151, 42529, 26153, 42531, 26155, 42535, 58920, 26159, 26161, 42546, 26163, 42548, 26165, 26167, 42552, 26169, 26171, 26173, 34365, 26175, 26177, 26179, 58948, 17989, 26181, 26183, 26185, 26187, 26189, 26191, 26193, 26195, 26197, 75349, 26200, 26204, 26206, 26208, 34401, 26212, 26216, 26220, 58992, 26225, 75380, 42613, 26233, 18042, 26235, 18047, 26239, 26241, 26243, 26247, 18056, 26249, 34439, 26251, 42631, 26253, 18062, 26255, 26257, 26259, 34452, 26261, 26263, 18072, 26265, 26267, 18076, 26269, 59038, 26271, 26273, 26275, 34467, 26277, 34470, 26279, 34472, 26281, 26283, 26285, 26287, 34479, 26289, 75442, 26291, 26295, 26297, 26299, 26301, 26303, 75456, 26305, 26307, 26309, 26311, 26313, 59081, 26315, 26317, 26319, 26321, 34513, 26323, 26325, 26327, 26329, 34521, 26331, 26333, 26335, 26337, 26339, 26341, 26343, 26345, 59114, 26347, 18157, 26349, 26351, 26353, 34546, 26355, 26357, 42742, 26359, 26363, 34555, 26365, 26367, 26369, 26371, 26373, 26375, 26381, 26383, 26385, 26387, 34579, 26389, 26391, 42776, 26393, 26395, 26397, 26399, 26401, 26403, 42788, 26405, 26407, 26409, 26411, 26413, 26415, 59183, 26417, 26419, 26421, 26423, 26425, 26427, 26429, 59197, 34623, 26433, 26435, 59203, 26437, 26439, 26441, 26443, 75596, 26447, 42831, 26449, 26451, 26453, 26455, 26457, 34651, 34652, 59229, 75620, 59240, 26481, 59255, 59259, 75650, 26508, 26510, 26512, 26514, 26516, 26518, 26520, 26522, 26524, 26526, 26528, 26530, 26532, 26534, 26536, 26538, 26540, 34732, 26542, 26544, 26546, 26548, 75701, 26550, 26552, 75291, 26554, 26556, 18365, 18366, 26558, 42942, 34758, 26572, 75295, 26574, 26576, 26578, 75731, 26580, 26582, 26584, 18394, 18395, 26586, 26588, 26590, 18399, 26592, 42970, 26594, 26596, 26598, 18407, 26600, 18409, 18410, 42983, 34796, 18413, 18414, 34807, 34810, 67603, 43028, 75795, 75799, 34852, 75818, 18475, 43053, 34865, 34869, 18494, 18497, 59473, 59486, 26728, 34921, 26730, 26732, 26734, 26736, 26738, 26740, 26742, 26744, 26746, 26748, 26750, 59519, 26752, 18561, 26754, 43139, 26756, 26758, 26760, 43145, 26762, 26764, 26766, 26768, 26770, 26772, 26774, 26776, 26778, 26780, 59549, 26782, 18591, 26784, 18594, 18595, 26786, 26788, 26790, 59557, 26792, 59561, 26794, 75944, 26796, 59565, 26798, 26800, 26802, 26804, 26806, 26808, 46036, 43216, 35033, 18653, 35041, 26893, 26895, 26897, 26899, 43284, 26901, 35094, 26903, 26905, 35097, 26907, 26909, 26911, 26913, 18723, 26915, 26917, 26919, 43303, 26921, 26923, 26925, 26927, 59695, 26929, 26931, 26933, 26935, 26937, 26939, 26941, 26943, 67903, 26945, 26947, 76100, 26949, 26951, 18770, 43348, 76120, 59738, 59746, 59769, 18821, 59781, 67974, 18828, 18831, 35218, 35226, 43418, 59808, 59812, 76196, 76200, 68019, 35260, 35269, 35278, 43471, 35282, 59859, 51668, 27093, 27095, 51672, 27097, 27099, 35291, 59869, 68065, 27107, 43492, 59877, 27115, 27117, 35309, 27119, 35311, 27121, 27123, 27125, 27127, 27129, 51706, 27131, 27133, 27135, 27137, 27139, 35331, 27141, 27143, 27145, 35338, 27147, 27149, 27151, 27153, 27155, 27157, 51733, 27159, 27161, 27163, 27165, 35357, 27167, 27169, 27171, 27173, 27175, 43559, 27177, 59945, 27179, 27181, 27183, 27185, 27187, 27189, 35381, 27191, 27193, 27195, 27197, 27199, 27201, 76354, 27203, 27205, 27207, 27209, 27211, 27213, 27215, 35407, 27217, 27219, 27221, 59990, 27223, 27225, 27227, 27229, 27231, 60025, 35462, 51847, 76422, 35468, 35470, 76432, 68243, 27289, 27291, 68251, 27293, 27295, 27297, 27299, 27301, 27303, 27305, 27307, 27309, 27311, 27313, 27315, 27317, 27319, 35512, 27321, 68279, 27323, 68281, 27325, 27327, 27329, 60098, 27331, 27333, 27335, 27337, 27339, 27341, 27343, 27345, 68305, 27347, 68307, 27349, 68309, 27351, 35544, 27353, 35546, 27355, 68315, 27357, 27359, 68319, 27361, 27363, 35555, 27365, 27367, 27369, 27371, 27373, 27375, 35568, 27377, 27379, 27381, 27383, 68343, 27385, 68345, 27387, 68347, 27389, 68349, 27391, 68351, 27393, 68353, 27395, 68355, 27397, 43782, 43784, 43786, 35614, 76578, 60198, 76582, 35629, 76590, 35634, 68412, 76606, 76608, 35660, 76628, 76632, 35673, 35681, 60267, 68481, 52109, 35726, 19353, 19367, 19369, 43945, 19373, 60336, 35763, 19381, 19383, 52154, 19387, 19389, 19393, 19395, 19400, 68559, 52179, 52183, 68586, 27633, 27635, 27637, 27639, 27641, 27643, 19452, 27645, 35835, 27647, 35836, 27649, 35837, 27651, 35843, 27653, 27655, 27657, 27659, 27661, 27663, 27665, 27667, 35859, 27669, 35861, 27671, 35863, 27673, 27675, 27677, 76832, 27691, 35893, 35897, 35906, 35911, 60488, 35918, 76878, 35924, 60543, 35969, 19605, 19607, 35994, 36005, 19635, 27835, 19645, 27837, 36030, 19651, 60612, 60616, 19660, 36045, 36047, 19664, 19666, 19668, 36052, 19670, 19672, 36065, 19682, 19694, 19696, 19698, 19702, 19704, 44282, 19710, 36095, 19714, 36098, 19724, 60685, 19729, 36121, 36133, 44330, 36144, 77105, 68915, 44342, 36151, 36199, 68977, 36211, 44431, 69025, 36264, 69037, 60848, 11697, 11706, 11707, 36288, 11717, 36294, 11723, 11728, 36313, 36314, 11741, 36324, 44516, 11764, 11767, 60921, 11773, 36350, 60925, 11779, 11791, 11793, 11794, 11796, 11797, 20008, 11817, 11819, 11821, 36398, 11824, 69188, 11845, 11847, 11851, 11855, 11858, 69202, 11861, 20053, 20069, 11884, 11888, 11890, 69238, 69240, 11900, 20095, 11904, 20097, 11906, 20099, 61055, 20101, 20103, 20105, 11915, 20107, 20109, 20111, 36495, 20113, 11923, 20115, 20117, 11926, 61075, 20120, 20122, 20124, 11933, 20126, 20128, 20130, 20132, 20134, 20136, 20138, 11948, 20140, 20142, 69294, 20144, 11954, 20146, 20148, 20150, 20152, 20154, 11966, 11970, 11972, 11976, 20169, 20175, 11984, 11988, 11994, 12002, 12013, 20205, 12016, 20209, 20211, 12024, 12026, 20218, 12032, 20226, 12036, 61193, 12053, 36637, 12063, 12066, 20270, 36657, 12086, 20279, 36666, 12091, 20285, 12098, 20291, 12101, 12104, 12107, 61262, 20321, 36713, 20340, 12149, 36729, 12161, 20362, 36746, 12172, 61331, 20384, 12194, 20386, 20388, 20390, 20392, 12202, 20394, 20396, 20398, 36785, 44978, 69555, 20404, 20408, 12217, 36801, 12230, 12247, 61400, 12258, 69603, 12265, 12271, 36855, 12281, 36857, 36867, 12299, 69653, 20509, 12320, 12323, 12324, 12327, 12329, 12335, 20527, 12337, 36912, 12342, 12343, 12345, 36933, 12361, 12362, 20555, 20557, 20559, 20561, 20563, 20565, 12375, 20567, 12378, 12384, 61536, 12407, 12411, 12413, 12415, 12419, 20611, 69765, 12423, 69767, 12425, 69769, 69773, 69775, 69777, 69779, 20629, 69781, 69783, 20633, 69785, 69787, 12444, 20637, 69789, 61599, 69793, 69795, 20645, 69797, 20647, 69799, 20649, 69801, 12459, 28844, 20653, 37038, 12463, 20655, 20657, 28848, 20659, 28849, 20661, 28850, 20663, 28852, 20665, 28854, 20667, 28856, 12483, 20683, 20687, 12497, 12500, 12519, 12521, 12539, 37137, 37139, 12569, 37146, 37148, 12579, 12582, 20789, 12615, 37191, 12625, 37201, 12636, 12639, 12643, 20835, 20837, 12647, 12654, 12668, 12677, 12679, 37256, 37258, 12683, 37262, 12688, 12697, 37278, 12705, 37287, 37289, 20916, 20922, 20924, 20934, 12744, 61904, 61906, 61923, 20964, 20966, 61927, 12776, 20968, 20970, 37362, 20980, 37366, 61948, 12801, 61953, 21002, 61967, 12816, 21008, 37393, 12821, 21016, 61978, 12839, 61998, 21044, 12861, 21068, 21070, 21072, 62033, 12887, 21080, 12893, 21092, 21094, 21096, 21102, 21106, 21110, 21112, 21114, 21116, 21118, 21120, 21122, 12931, 21124, 21128, 62089, 21132, 21140, 62104, 12953, 62109, 12958, 21150, 12960, 12962, 12963, 12964, 12965, 12967, 12968, 12969, 12970, 21160, 12972, 12973, 21166, 21168, 21170, 12979, 12980, 21172, 12982, 21178, 12987, 12988, 12989, 12992, 12996, 62160, 21206, 62167, 21208, 21214, 62178, 13028, 21222, 62183, 21224, 62214, 21256, 13066, 21262, 21264, 21266, 21268, 21270, 21272, 21274, 21276, 37661, 21278, 70428, 21280, 62245, 13098, 13125, 13133, 62297, 62302, 29537, 29539, 37734, 13162, 13163, 37738, 13166, 13167, 62327, 13192, 21390, 21392, 21394, 21396, 21398, 62358, 21400, 21402, 54170, 21404, 21408, 21410, 13219, 21412, 70564, 13222, 21414, 70565, 70566, 70568, 70569, 21420, 37805, 21422, 62382, 21424, 70573, 21426, 70574, 70575, 70576, 21430, 70578, 70582, 54201, 21440, 21442, 70596, 21450, 21452, 21454, 21456, 70608, 21458, 13268, 21460, 21462, 13271, 21464, 29653, 21466, 29656, 21468, 29658, 29660, 29662, 29663, 29665, 62428, 29667, 13284, 29669, 29671, 29673, 29675, 29677, 13294, 29679, 29681, 13298, 29683, 29685, 29687, 13317, 13323, 37905, 13332, 37911, 21533, 70694, 70700, 70702, 21551, 21559, 70715, 70716, 70719, 70720, 70724, 70726, 46152, 70732, 70733, 70736, 37972, 37978, 70747, 37980, 21601, 37986, 21603, 21605, 70757, 37994, 21611, 70762, 21613, 70763, 21615, 62575, 21617, 13426, 21619, 70767, 21621, 70772, 21623, 70773, 21625, 62585, 21627, 70775, 21629, 70781, 21631, 70782, 21633, 21635, 21637, 54406, 21639, 70791, 21641, 70792, 21643, 70794, 21645, 70795, 21647, 62608, 70810, 70811, 70815, 38051, 38053, 70824, 70828, 70831, 62643, 70848, 70849, 70857, 70859, 70860, 21711, 70864, 21713, 21715, 21717, 70869, 21719, 70872, 21721, 70873, 21723, 70875, 21725, 70877, 21727, 38112, 21729, 70880, 21731, 70883, 21733, 21735, 21737, 70889, 21739, 70890, 21741, 70894, 21743, 21745, 21747, 21749, 21751, 21753, 70905, 21755, 21757, 21759, 70911, 21761, 21763, 21765, 70917, 21767, 70920, 21769, 21771, 70923, 21773, 21775, 21777, 21779, 70931, 21781, 70933, 21783, 70935, 21785, 21787, 70939, 21789, 70941, 21791, 70942, 70951, 70963, 70964, 70966, 70967, 70969, 21827, 21829, 21833, 54603, 70998, 38250, 62839, 30073, 30075, 30087, 74367, 54692, 21929, 38321, 38325, 71104, 62919, 62937, 71171, 62983, 71180, 30232, 38449, 63032, 63045, 71244, 71259, 71264, 71266, 71267, 71268, 71269, 71271, 71275, 54996, 71401, 71441, 30483, 71475, 63356, 30590, 55183, 63376, 63437, 63477, 30713, 55301, 30779, 30785, 71770, 30816, 30820, 30834, 63641, 30886, 55501, 71889, 30964, 63735, 55604, 72013, 72015, 31071, 72032, 72044, 72045, 72049, 72052, 72053, 72059, 72065, 31108, 55686, 31112, 55694, 72083, 72084, 72087, 72091, 72096, 72097, 72100, 55717, 72102, 39336, 55724, 72109, 72110, 72113, 72114, 55731, 72115, 72117, 55735, 72119, 72120, 72121, 72122, 72123, 72124, 72125, 72126, 72127, 72129, 55749, 72133, 72138, 72139, 72140, 72143, 72144, 72145, 55763, 55765, 55769, 55771, 72155, 72159, 72161, 72162, 31206, 72168, 39405, 31219, 55810, 31330, 55907, 72318, 72322, 55945, 72332, 72336, 72347, 55965, 55971, 72356, 55973, 31402, 72362, 72372, 72385, 56003, 72387, 72393, 39631, 56015, 72403, 72404, 64230, 56054, 64255, 72454, 39698, 72470, 39705, 39707, 39714, 39716, 39718, 39724, 39728, 72498, 39732, 31556, 72528, 39790, 56176, 31603, 39802, 64380, 56203, 64397, 31646, 23470, 72624, 72626, 72635, 39870, 72638, 72645, 72652, 72654, 72659, 69803, 72670, 31715, 48100, 69805, 64497, 69809, 48130, 39944, 48138, 72718, 39952, 48144, 39979, 56367, 31820, 40016, 40020, 72795, 40032, 56422, 56438, 64646, 64649, 56462, 64657, 72850, 64659, 72852, 72857, 72859, 64668, 56478, 72862, 64672, 72865, 72866, 64676, 72870, 72872, 64682, 64688, 40117, 64699, 72909, 56535, 48345, 72922, 64736, 40161, 48357, 56556, 48365, 48368, 72944, 23798, 48376, 48380, 48384, 72966, 56584, 48393, 48397, 40211, 48405, 48413, 48416, 48424, 48428, 48437, 48439, 56669, 56683, 40306, 32129, 56720, 64916, 40342, 40346, 23964, 23966, 23968, 23972, 23975, 23977, 23979, 40364, 23981, 23983, 23985, 56753, 23987, 23989, 23991, 23993, 23995, 23997, 23999, 24001, 24003, 24005, 24007, 73160, 24009, 24011, 24013, 24015, 24017, 24019, 24021, 24023, 24025, 24027, 48604, 24029, 24031, 24033, 56801, 24035, 24040, 40425, 24063, 48645, 40460, 65054, 40488, 24108, 24112, 73264, 24116, 24120, 24124, 48703, 24128, 40512, 40514, 65089, 48711, 73294, 24145, 24147, 24149, 24151, 24153, 24161, 40552, 40559, 40571, 40577, 73350, 40589, 24221, 24223, 24225, 24227, 24229, 24231, 24233, 48810, 24235, 40619, 24237, 48816, 73407, 57050, 24294, 24296, 24298, 24300, 24302, 24304, 24306, 65282, 73474, 40713, 40719, 40721, 65298, 24340, 32543, 40763, 24380, 40764, 24382, 73531, 40771, 40773, 24390, 24392, 24394, 24400, 73555, 40793, 24416, 73577, 24428, 24434, 24438, 73591, 24440, 24446, 24448, 24450, 24452, 24454, 24456, 24458, 24460, 24462, 24464, 40863, 24482, 24484, 49085, 57279, 24541, 24543, 24545, 24547, 24549, 40934, 24551, 24553, 24555, 24557, 65517, 24559, 24561, 24563, 24565, 40950, 24567, 24569, 24571, 24573, 24575]
        before = len(self)
        self = self[~self["message_id"].isin(message_ids)]
        after = len(self)
        print(f"Dropped {before - after} events occurring in messages which contain nonsense bursts")

    def _drop_native(self):
        #message_ids = [57357, 57363, 73747, 65564, 57386, 57398, 57403, 57406, 57408, 24642, 49219, 24644, 49221, 24646, 57410, 24648, 57413, 57417, 57420, 24656, 57426, 57428, 41045, 57430, 24664, 65625, 57434, 49244, 57436, 57440, 24674, 41059, 57442, 57444, 24678, 57447, 24680, 24682, 24684, 57452, 24686, 57454, 24688, 57456, 24690, 73838, 49268, 24693, 24695, 73848, 24697, 57465, 24699, 24701, 24703, 24705, 57474, 24707, 73857, 24709, 41093, 24711, 24713, 49290, 24715, 24717, 24719, 24721, 24723, 41107, 24725, 65686, 24727, 24729, 49306, 24731, 73881, 24733, 49310, 24735, 24737, 57505, 24739, 24741, 24743, 24745, 24747, 73900, 24749, 73902, 24751, 24753, 24755, 32947, 24757, 24759, 24761, 24763, 73916, 24765, 24767, 73919, 24769, 24771, 24773, 24775, 24777, 24779, 24781, 24783, 24785, 24787, 24789, 65749, 24791, 24793, 24795, 24797, 24799, 24801, 24803, 24805, 24807, 24809, 24811, 24813, 24815, 24817, 24819, 24821, 24823, 57592, 24825, 24827, 57595, 24829, 24831, 57599, 24833, 24835, 24837, 24839, 24841, 24847, 24849, 57618, 24851, 24853, 24855, 24857, 24859, 24861, 24863, 57631, 24865, 24867, 24869, 24871, 24873, 41257, 24875, 24877, 33069, 24879, 24881, 24883, 57652, 24885, 33077, 24887, 74038, 24889, 24893, 41281, 24937, 49524, 65948, 65962, 41389, 41391, 41399, 41403, 41405, 41409, 41411, 41413, 41415, 41417, 41421, 41423, 41425, 49618, 41427, 74195, 33238, 25055, 74209, 25061, 33263, 33274, 57851, 49664, 25089, 57859, 25099, 25101, 33300, 25111, 33304, 25113, 57882, 25115, 66076, 25117, 25119, 25121, 25123, 25125, 25129, 25131, 25133, 57901, 25135, 25137, 25141, 49720, 25145, 57912, 25149, 25153, 25157, 25162, 52220, 57932, 25166, 25169, 57937, 16981, 16982, 25173, 16985, 25177, 74333, 33374, 57952, 16994, 33378, 25189, 25192, 57961, 49770, 57964, 25197, 17006, 25201, 49778, 57969, 49780, 25205, 57973, 25208, 49784, 49786, 49788, 25213, 49790, 57980, 17024, 25217, 33410, 17027, 49792, 25221, 49794, 49796, 25224, 49798, 25228, 49804, 49806, 57998, 57999, 25233, 49810, 25235, 49812, 25237, 25239, 17048, 25241, 33432, 25243, 58012, 25245, 17055, 25247, 25249, 25251, 25253, 25255, 25258, 25260, 33452, 25262, 33455, 25264, 58028, 25266, 25268, 74420, 25270, 41655, 25272, 25274, 58043, 25276, 25278, 25280, 58048, 25282, 25284, 25286, 58054, 25288, 49864, 17098, 25290, 25292, 33482, 25294, 74444, 17104, 25296, 25298, 58064, 25300, 25302, 25304, 25306, 25308, 25310, 25312, 25314, 25316, 33511, 25324, 25328, 17147, 33531, 58110, 58118, 58121, 33546, 74507, 33555, 25366, 33559, 49949, 58142, 17184, 25378, 25380, 33572, 25382, 74535, 25384, 25386, 25388, 58156, 25390, 33582, 25392, 33584, 25394, 25396, 25398, 25400, 17209, 25402, 17211, 25404, 25406, 25408, 25410, 33602, 25412, 58179, 25414, 25416, 25418, 25420, 25422, 58190, 25424, 58191, 25426, 33619, 25428, 58196, 25430, 25432, 25434, 25436, 58204, 25438, 25440, 25442, 33635, 25444, 58213, 25446, 74597, 25448, 74599, 25450, 25452, 33645, 25458, 17268, 25460, 25462, 58228, 25464, 25466, 25468, 25470, 50047, 25472, 25474, 33666, 25476, 41858, 25478, 25480, 25482, 25484, 33677, 25486, 25488, 25490, 25492, 25494, 33686, 25496, 25498, 25500, 25502, 25504, 25506, 25508, 25510, 33703, 25512, 58280, 25514, 25516, 25518, 66481, 58290, 25526, 66487, 25528, 66489, 25530, 25532, 25534, 58303, 17345, 58307, 58310, 58320, 58322, 17363, 58334, 74737, 33781, 33786, 33792, 41987, 33798, 74759, 33804, 58380, 33806, 58384, 58389, 33819, 58397, 58405, 74801, 33846, 17476, 33864, 33868, 25678, 25680, 25682, 25684, 25686, 58455, 25688, 25690, 25692, 58461, 25694, 25696, 33888, 25698, 42083, 25700, 25702, 25704, 25706, 25708, 25710, 25712, 58480, 25714, 25716, 25718, 25720, 25722, 58491, 25724, 25726, 25728, 25730, 25732, 25734, 25736, 25738, 58506, 25740, 25742, 25744, 25746, 25752, 25754, 25756, 25758, 58526, 25760, 25762, 58530, 25768, 25770, 25772, 25774, 25776, 33969, 25778, 25780, 25782, 25784, 25786, 33981, 58559, 58560, 33985, 58563, 33993, 33995, 74956, 50385, 50389, 17636, 58606, 58609, 25846, 58614, 25848, 25850, 25852, 25854, 25856, 25858, 25860, 25862, 25864, 25866, 25868, 25870, 25874, 25876, 25878, 25880, 58648, 25882, 25884, 25886, 25888, 25890, 25892, 25894, 25896, 25898, 58678, 42296, 34108, 17725, 58699, 42318, 42320, 17745, 58709, 17757, 34143, 34152, 58753, 58759, 42385, 42387, 58771, 17823, 42420, 58808, 17851, 17852, 17854, 58816, 26053, 26055, 26057, 67017, 26059, 58828, 26061, 26065, 26067, 17876, 26069, 34260, 26071, 26075, 26076, 34268, 58844, 26087, 26088, 26091, 26092, 26095, 26096, 42482, 26099, 26100, 58870, 26103, 26104, 58874, 26107, 26108, 75261, 26111, 26112, 26115, 26116, 42501, 26119, 26120, 26123, 26124, 42509, 58894, 26127, 26128, 42511, 26131, 26132, 42517, 26135, 26136, 17947, 26139, 17949, 26140, 26143, 17952, 17953, 26144, 26147, 42523, 26149, 42525, 26151, 42529, 26153, 42531, 26155, 42535, 58920, 26159, 26161, 42546, 26163, 42548, 26165, 26167, 42552, 26169, 26171, 26173, 34365, 26175, 26177, 26179, 58948, 17989, 26181, 26183, 26185, 26187, 26189, 26191, 26193, 26195, 26197, 75349, 26200, 26204, 26206, 26208, 34401, 26212, 26216, 26220, 58992, 26225, 75380, 42613, 26233, 18042, 26235, 18047, 26239, 26241, 26243, 26247, 18056, 26249, 34439, 26251, 42631, 26253, 18062, 26255, 26257, 26259, 34452, 26261, 26263, 18072, 26265, 26267, 18076, 26269, 59038, 26271, 26273, 26275, 34467, 26277, 34470, 26279, 34472, 26281, 26283, 26285, 26287, 34479, 26289, 75442, 26291, 26295, 26297, 26299, 26301, 26303, 75456, 26305, 26307, 26309, 26311, 26313, 59081, 26315, 26317, 26319, 26321, 34513, 26323, 26325, 26327, 26329, 34521, 26331, 26333, 26335, 26337, 26339, 26341, 26343, 26345, 59114, 26347, 18157, 26349, 26351, 26353, 34546, 26355, 26357, 42742, 26359, 26363, 34555, 26365, 26367, 26369, 26371, 26373, 26375, 26381, 26383, 26385, 26387, 34579, 26389, 26391, 42776, 26393, 26395, 26397, 26399, 26401, 26403, 42788, 26405, 26407, 26409, 26411, 26413, 26415, 59183, 26417, 26419, 26421, 26423, 26425, 26427, 26429, 59197, 34623, 26433, 26435, 59203, 26437, 26439, 26441, 26443, 75596, 26447, 42831, 26449, 26451, 26453, 26455, 26457, 34651, 34652, 59229, 75620, 59240, 26481, 59255, 59259, 75650, 26508, 26510, 26512, 26514, 26516, 26518, 26520, 26522, 26524, 26526, 26528, 26530, 26532, 26534, 26536, 26538, 26540, 34732, 26542, 26544, 26546, 26548, 75701, 26550, 26552, 75291, 26554, 26556, 18365, 18366, 26558, 42942, 34758, 26572, 75295, 26574, 26576, 26578, 75731, 26580, 26582, 26584, 18394, 18395, 26586, 26588, 26590, 18399, 26592, 42970, 26594, 26596, 26598, 18407, 26600, 18409, 18410, 42983, 34796, 18413, 18414, 34807, 34810, 67603, 43028, 75795, 75799, 34852, 75818, 18475, 43053, 34865, 34869, 18494, 18497, 59473, 59486, 26728, 34921, 26730, 26732, 26734, 26736, 26738, 26740, 26742, 26744, 26746, 26748, 26750, 59519, 26752, 18561, 26754, 43139, 26756, 26758, 26760, 43145, 26762, 26764, 26766, 26768, 26770, 26772, 26774, 26776, 26778, 26780, 59549, 26782, 18591, 26784, 18594, 18595, 26786, 26788, 26790, 59557, 26792, 59561, 26794, 75944, 26796, 59565, 26798, 26800, 26802, 26804, 26806, 26808, 46036, 43216, 35033, 18653, 35041, 26893, 26895, 26897, 26899, 43284, 26901, 35094, 26903, 26905, 35097, 26907, 26909, 26911, 26913, 18723, 26915, 26917, 26919, 43303, 26921, 26923, 26925, 26927, 59695, 26929, 26931, 26933, 26935, 26937, 26939, 26941, 26943, 67903, 26945, 26947, 76100, 26949, 26951, 18770, 43348, 76120, 59738, 59746, 59769, 18821, 59781, 67974, 18828, 18831, 35218, 35226, 43418, 59808, 59812, 76196, 76200, 68019, 35260, 35269, 35278, 43471, 35282, 59859, 51668, 27093, 27095, 51672, 27097, 27099, 35291, 59869, 68065, 27107, 43492, 59877, 27115, 27117, 35309, 27119, 35311, 27121, 27123, 27125, 27127, 27129, 51706, 27131, 27133, 27135, 27137, 27139, 35331, 27141, 27143, 27145, 35338, 27147, 27149, 27151, 27153, 27155, 27157, 51733, 27159, 27161, 27163, 27165, 35357, 27167, 27169, 27171, 27173, 27175, 43559, 27177, 59945, 27179, 27181, 27183, 27185, 27187, 27189, 35381, 27191, 27193, 27195, 27197, 27199, 27201, 76354, 27203, 27205, 27207, 27209, 27211, 27213, 27215, 35407, 27217, 27219, 27221, 59990, 27223, 27225, 27227, 27229, 27231, 60025, 35462, 51847, 76422, 35468, 35470, 76432, 68243, 27289, 27291, 68251, 27293, 27295, 27297, 27299, 27301, 27303, 27305, 27307, 27309, 27311, 27313, 27315, 27317, 27319, 35512, 27321, 68279, 27323, 68281, 27325, 27327, 27329, 60098, 27331, 27333, 27335, 27337, 27339, 27341, 27343, 27345, 68305, 27347, 68307, 27349, 68309, 27351, 35544, 27353, 35546, 27355, 68315, 27357, 27359, 68319, 27361, 27363, 35555, 27365, 27367, 27369, 27371, 27373, 27375, 35568, 27377, 27379, 27381, 27383, 68343, 27385, 68345, 27387, 68347, 27389, 68349, 27391, 68351, 27393, 68353, 27395, 68355, 27397, 43782, 43784, 43786, 35614, 76578, 60198, 76582, 35629, 76590, 35634, 68412, 76606, 76608, 35660, 76628, 76632, 35673, 35681, 60267, 68481, 52109, 35726, 19353, 19367, 19369, 43945, 19373, 60336, 35763, 19381, 19383, 52154, 19387, 19389, 19393, 19395, 19400, 68559, 52179, 52183, 68586, 27633, 27635, 27637, 27639, 27641, 27643, 19452, 27645, 35835, 27647, 35836, 27649, 35837, 27651, 35843, 27653, 27655, 27657, 27659, 27661, 27663, 27665, 27667, 35859, 27669, 35861, 27671, 35863, 27673, 27675, 27677, 76832, 27691, 35893, 35897, 35906, 35911, 60488, 35918, 76878, 35924, 60543, 35969, 19605, 19607, 35994, 36005, 19635, 27835, 19645, 27837, 36030, 19651, 60612, 60616, 19660, 36045, 36047, 19664, 19666, 19668, 36052, 19670, 19672, 36065, 19682, 19694, 19696, 19698, 19702, 19704, 44282, 19710, 36095, 19714, 36098, 19724, 60685, 19729, 36121, 36133, 44330, 36144, 77105, 68915, 44342, 36151, 36199, 68977, 36211, 44431, 69025, 36264, 69037, 60848, 11697, 11706, 11707, 36288, 11717, 36294, 11723, 11728, 36313, 36314, 11741, 36324, 44516, 11764, 11767, 60921, 11773, 36350, 60925, 11779, 11791, 11793, 11794, 11796, 11797, 20008, 11817, 11819, 11821, 36398, 11824, 69188, 11845, 11847, 11851, 11855, 11858, 69202, 11861, 20053, 20069, 11884, 11888, 11890, 69238, 69240, 11900, 20095, 11904, 20097, 11906, 20099, 61055, 20101, 20103, 20105, 11915, 20107, 20109, 20111, 36495, 20113, 11923, 20115, 20117, 11926, 61075, 20120, 20122, 20124, 11933, 20126, 20128, 20130, 20132, 20134, 20136, 20138, 11948, 20140, 20142, 69294, 20144, 11954, 20146, 20148, 20150, 20152, 20154, 11966, 11970, 11972, 11976, 20169, 20175, 11984, 11988, 11994, 12002, 12013, 20205, 12016, 20209, 20211, 12024, 12026, 20218, 12032, 20226, 12036, 61193, 12053, 36637, 12063, 12066, 20270, 36657, 12086, 20279, 36666, 12091, 20285, 12098, 20291, 12101, 12104, 12107, 61262, 20321, 36713, 20340, 12149, 36729, 12161, 20362, 36746, 12172, 61331, 20384, 12194, 20386, 20388, 20390, 20392, 12202, 20394, 20396, 20398, 36785, 44978, 69555, 20404, 20408, 12217, 36801, 12230, 12247, 61400, 12258, 69603, 12265, 12271, 36855, 12281, 36857, 36867, 12299, 69653, 20509, 12320, 12323, 12324, 12327, 12329, 12335, 20527, 12337, 36912, 12342, 12343, 12345, 36933, 12361, 12362, 20555, 20557, 20559, 20561, 20563, 20565, 12375, 20567, 12378, 12384, 61536, 12407, 12411, 12413, 12415, 12419, 20611, 69765, 12423, 69767, 12425, 69769, 69773, 69775, 69777, 69779, 20629, 69781, 69783, 20633, 69785, 69787, 12444, 20637, 69789, 61599, 69793, 69795, 20645, 69797, 20647, 69799, 20649, 69801, 12459, 28844, 20653, 37038, 12463, 20655, 20657, 28848, 20659, 28849, 20661, 28850, 20663, 28852, 20665, 28854, 20667, 28856, 12483, 20683, 20687, 12497, 12500, 12519, 12521, 12539, 37137, 37139, 12569, 37146, 37148, 12579, 12582, 20789, 12615, 37191, 12625, 37201, 12636, 12639, 12643, 20835, 20837, 12647, 12654, 12668, 12677, 12679, 37256, 37258, 12683, 37262, 12688, 12697, 37278, 12705, 37287, 37289, 20916, 20922, 20924, 20934, 12744, 61904, 61906, 61923, 20964, 20966, 61927, 12776, 20968, 20970, 37362, 20980, 37366, 61948, 12801, 61953, 21002, 61967, 12816, 21008, 37393, 12821, 21016, 61978, 12839, 61998, 21044, 12861, 21068, 21070, 21072, 62033, 12887, 21080, 12893, 21092, 21094, 21096, 21102, 21106, 21110, 21112, 21114, 21116, 21118, 21120, 21122, 12931, 21124, 21128, 62089, 21132, 21140, 62104, 12953, 62109, 12958, 21150, 12960, 12962, 12963, 12964, 12965, 12967, 12968, 12969, 12970, 21160, 12972, 12973, 21166, 21168, 21170, 12979, 12980, 21172, 12982, 21178, 12987, 12988, 12989, 12992, 12996, 62160, 21206, 62167, 21208, 21214, 62178, 13028, 21222, 62183, 21224, 62214, 21256, 13066, 21262, 21264, 21266, 21268, 21270, 21272, 21274, 21276, 37661, 21278, 70428, 21280, 62245, 13098, 13125, 13133, 62297, 62302, 29537, 29539, 37734, 13162, 13163, 37738, 13166, 13167, 62327, 13192, 21390, 21392, 21394, 21396, 21398, 62358, 21400, 21402, 54170, 21404, 21408, 21410, 13219, 21412, 70564, 13222, 21414, 70565, 70566, 70568, 70569, 21420, 37805, 21422, 62382, 21424, 70573, 21426, 70574, 70575, 70576, 21430, 70578, 70582, 54201, 21440, 21442, 70596, 21450, 21452, 21454, 21456, 70608, 21458, 13268, 21460, 21462, 13271, 21464, 29653, 21466, 29656, 21468, 29658, 29660, 29662, 29663, 29665, 62428, 29667, 13284, 29669, 29671, 29673, 29675, 29677, 13294, 29679, 29681, 13298, 29683, 29685, 29687, 13317, 13323, 37905, 13332, 37911, 21533, 70694, 70700, 70702, 21551, 21559, 70715, 70716, 70719, 70720, 70724, 70726, 46152, 70732, 70733, 70736, 37972, 37978, 70747, 37980, 21601, 37986, 21603, 21605, 70757, 37994, 21611, 70762, 21613, 70763, 21615, 62575, 21617, 13426, 21619, 70767, 21621, 70772, 21623, 70773, 21625, 62585, 21627, 70775, 21629, 70781, 21631, 70782, 21633, 21635, 21637, 54406, 21639, 70791, 21641, 70792, 21643, 70794, 21645, 70795, 21647, 62608, 70810, 70811, 70815, 38051, 38053, 70824, 70828, 70831, 62643, 70848, 70849, 70857, 70859, 70860, 21711, 70864, 21713, 21715, 21717, 70869, 21719, 70872, 21721, 70873, 21723, 70875, 21725, 70877, 21727, 38112, 21729, 70880, 21731, 70883, 21733, 21735, 21737, 70889, 21739, 70890, 21741, 70894, 21743, 21745, 21747, 21749, 21751, 21753, 70905, 21755, 21757, 21759, 70911, 21761, 21763, 21765, 70917, 21767, 70920, 21769, 21771, 70923, 21773, 21775, 21777, 21779, 70931, 21781, 70933, 21783, 70935, 21785, 21787, 70939, 21789, 70941, 21791, 70942, 70951, 70963, 70964, 70966, 70967, 70969, 21827, 21829, 21833, 54603, 70998, 38250, 62839, 30073, 30075, 30087, 74367, 54692, 21929, 38321, 38325, 71104, 62919, 62937, 71171, 62983, 71180, 30232, 38449, 63032, 63045, 71244, 71259, 71264, 71266, 71267, 71268, 71269, 71271, 71275, 54996, 71401, 71441, 30483, 71475, 63356, 30590, 55183, 63376, 63437, 63477, 30713, 55301, 30779, 30785, 71770, 30816, 30820, 30834, 63641, 30886, 55501, 71889, 30964, 63735, 55604, 72013, 72015, 31071, 72032, 72044, 72045, 72049, 72052, 72053, 72059, 72065, 31108, 55686, 31112, 55694, 72083, 72084, 72087, 72091, 72096, 72097, 72100, 55717, 72102, 39336, 55724, 72109, 72110, 72113, 72114, 55731, 72115, 72117, 55735, 72119, 72120, 72121, 72122, 72123, 72124, 72125, 72126, 72127, 72129, 55749, 72133, 72138, 72139, 72140, 72143, 72144, 72145, 55763, 55765, 55769, 55771, 72155, 72159, 72161, 72162, 31206, 72168, 39405, 31219, 55810, 31330, 55907, 72318, 72322, 55945, 72332, 72336, 72347, 55965, 55971, 72356, 55973, 31402, 72362, 72372, 72385, 56003, 72387, 72393, 39631, 56015, 72403, 72404, 64230, 56054, 64255, 72454, 39698, 72470, 39705, 39707, 39714, 39716, 39718, 39724, 39728, 72498, 39732, 31556, 72528, 39790, 56176, 31603, 39802, 64380, 56203, 64397, 31646, 23470, 72624, 72626, 72635, 39870, 72638, 72645, 72652, 72654, 72659, 69803, 72670, 31715, 48100, 69805, 64497, 69809, 48130, 39944, 48138, 72718, 39952, 48144, 39979, 56367, 31820, 40016, 40020, 72795, 40032, 56422, 56438, 64646, 64649, 56462, 64657, 72850, 64659, 72852, 72857, 72859, 64668, 56478, 72862, 64672, 72865, 72866, 64676, 72870, 72872, 64682, 64688, 40117, 64699, 72909, 56535, 48345, 72922, 64736, 40161, 48357, 56556, 48365, 48368, 72944, 23798, 48376, 48380, 48384, 72966, 56584, 48393, 48397, 40211, 48405, 48413, 48416, 48424, 48428, 48437, 48439, 56669, 56683, 40306, 32129, 56720, 64916, 40342, 40346, 23964, 23966, 23968, 23972, 23975, 23977, 23979, 40364, 23981, 23983, 23985, 56753, 23987, 23989, 23991, 23993, 23995, 23997, 23999, 24001, 24003, 24005, 24007, 73160, 24009, 24011, 24013, 24015, 24017, 24019, 24021, 24023, 24025, 24027, 48604, 24029, 24031, 24033, 56801, 24035, 24040, 40425, 24063, 48645, 40460, 65054, 40488, 24108, 24112, 73264, 24116, 24120, 24124, 48703, 24128, 40512, 40514, 65089, 48711, 73294, 24145, 24147, 24149, 24151, 24153, 24161, 40552, 40559, 40571, 40577, 73350, 40589, 24221, 24223, 24225, 24227, 24229, 24231, 24233, 48810, 24235, 40619, 24237, 48816, 73407, 57050, 24294, 24296, 24298, 24300, 24302, 24304, 24306, 65282, 73474, 40713, 40719, 40721, 65298, 24340, 32543, 40763, 24380, 40764, 24382, 73531, 40771, 40773, 24390, 24392, 24394, 24400, 73555, 40793, 24416, 73577, 24428, 24434, 24438, 73591, 24440, 24446, 24448, 24450, 24452, 24454, 24456, 24458, 24460, 24462, 24464, 40863, 24482, 24484, 49085, 57279, 24541, 24543, 24545, 24547, 24549, 40934, 24551, 24553, 24555, 24557, 65517, 24559, 24561, 24563, 24565, 40950, 24567, 24569, 24571, 24573, 24575]
        before = len(self)
        #self = self[~self["message_id"].isin(message_ids)]
        after = len(self)
        print(f"Dropped {before - after} events of native speakers")


    def load_default_data(self, system, include_anonymized:bool|None = None, include_nonsense:bool|None = None, include_native:bool|None = None) -> "KeyLoggingDataFrame":
        # 1. verify parameters
        if system not in ("lh", "ll"):
            raise ValueError(f"Invalid system '{system}'. Must be 'lh' (Language Hero) or 'll' (Language Lab).")

        if system != "lh" and include_anonymized is not None:
            raise ValueError(
                "include_anonymized can only be specified when system='lh'."
            )
        if system != "lh" and include_nonsense is not None:
            raise ValueError(
                "include_nonsense can only be specified when system='lh'."
            )
        if system != "lh" and include_native is not None:
            raise ValueError(
                "include_native can only be specified when system='lh'."
            )
        self.system = system
        
        # 2. load data
        base_dir = Path(__file__).resolve().parent
        if system == "lh":
            dtype_dict = {"key_id": int, "content": str, "event_for_message_type": str, "key_time": str, "message_id": int, "user_status": str, "message_content": str, "message_created_at": str, "session_id": int, "PERSONA_ID": float, "USER_ID": float, "TASK_ID": float, "application": str, "creationDate_conversation": str, "targetLang": int, "tutorLang": int, "hasTasks": bool, "CREATOR_ID": int, "SCENARIO_ID": int, "preCreated": bool, "simulated": bool, "archivedForProcessing": bool, "username": str}
            df = pd.read_csv(base_dir / "data/lh_default.csv", dtype=dtype_dict)
            df["key_time"] = pd.to_datetime(df["key_time"], format="%Y-%m-%d %H:%M:%S.%f", errors="coerce")
            df["message_created_at"] = pd.to_datetime(df["message_created_at"],format="%Y-%m-%d %H:%M:%S.%f", errors="coerce") 
            self.__init__(df)

        elif system == "ll":
            dtype_dict = {"key_id": int, "message_id": int, "content": str, "key_time": str, "message_uid": str, "message_content": str, "user_id": int, "session_id": int, "reply_to_message_id": str, "message_created_at": str}
            df = pd.read_csv(base_dir / "data/ll_default.csv", dtype=dtype_dict)
            df["key_time"] = pd.to_datetime(df["key_time"], format="%Y-%m-%d %H:%M:%S.%f", errors="coerce")
            df["message_created_at"] = pd.to_datetime(df["message_created_at"], errors="coerce")
            self.__init__(df)

        print(f"Loaded {len(self)} events")

        # 3. Drop unwanted data
        if system == "lh":
            if not include_anonymized:
                self._drop_anonymized()
            if not include_nonsense:
                self._drop_nonsense()
            if not include_native:
                self._drop_native()
        print("KeyLoggingDataFrame of shape", self.shape)

        return self

    def load_data_from_path(self, system:str, path_keys:str, path_messages:str, include_anonymized:bool|None = None, include_nonsense:bool|None = None, include_native:bool|None = None) -> "KeyLoggingDataFrame":
        """Initialize the KeyLoggingDataFrame with key and message data.
        input:
            path_keys: str, path to the keys/events logging data CSV file
            path_messages: str, path to the message data CSV file
            system: "lh" for Language Hero data or "ll" for Language Lab data
        output: a KeyLoggingDataFrame object containing the merged csv files
        """
        # 1. verify parameters
        if system not in ("lh", "ll"):
            raise ValueError(f"Invalid system '{system}'. Must be 'lh' (Language Hero) or 'll' (Language Lab).")
        if system != "lh" and include_anonymized is not None:
            raise ValueError(
                "include_anonymized can only be specified when system='lh'."
            )
        if system != "lh" and include_nonsense is not None:
            raise ValueError(
                "include_nonsense can only be specified when system='lh'."
            )
        if system != "lh" and include_native is not None:
            raise ValueError(
                "include_native can only be specified when system='lh'."
            )
        self.system = system

        # 2. open files
        try:
            keys = pd.read_csv(path_keys)
            messages = pd.read_csv(path_messages)
        except Exception as e:
            raise FileNotFoundError(f"Error reading files: {e}")

        # 3. normalise column names
        try:
            if system == "lh":
                keys = keys.rename(columns={
                    "id": "key_id",
                    "content": "content",
                    "eventForMessageType": "event_for_message_type",
                    "moment": "key_time",
                    "MESSAGE_ID": "message_id"
                })
                messages = messages.rename(columns={
                    "id": "message_id",
                    "MESSAGE_TYPE": "user_status",
                    "transcript": "message_content",
                    "creationDate": "message_created_at",
                    "DIALOGUE_ID": "session_id"
                })
            elif system == "ll":
                keys = keys.rename(columns={
                    "id": "key_id",
                    "message": "content",
                    "date": "key_time",
                    "message_id": "message_id"
                })
                messages = messages.rename(columns={
                    "message_id": "message_uid",
                    "id": "message_id",
                    "content": "message_content",
                    "created_at": "message_created_at",
                    "session_id": "session_id"
                })
        except Exception as e:
            raise ValueError(f"Error renaming columns: {e}")

        # 4. add missing columns
        # Language Lab does not have event_for_message_type and user_status columns
        if system == "ll":
            if "event_for_message_type" not in keys.columns:
                keys["event_for_message_type"] = 0
                # set first event of each message to 5 if column content is empty
                first_events = keys.sort_values(by=["message_id", "key_time"]).groupby("message_id").head(1).index
                empty_content = keys[keys["content"].isnull()].index
                keys.loc[first_events.intersection(empty_content), "event_for_message_type"] = 5
            if "user_status" not in messages.columns:
                # TO DO: ideally user status already in messages
                messages["user_status"] = "L"
                messages.loc[messages["user_id"] == 8, "user_status"] = "T"



        # 3. read columns in correct format
        for col in ["key_id", "message_id", "session_id"]:
            if col in keys.columns:
                keys[col] = keys[col].astype(str)
            if col in messages.columns:
                messages[col] = messages[col].astype(str)

        if system == "ll":
            keys["key_time"] = pd.to_datetime(keys["key_time"], origin='unix', unit='ms', errors='coerce')
        for col in ["created_at", "key_time"]:
            if col in keys.columns:
                keys[col] = pd.to_datetime(keys[col], errors='coerce')
            if col in messages.columns:
                messages[col] = pd.to_datetime(messages[col], errors='coerce')

        # 4. Verify whether ids are unique
        if keys["key_id"].nunique() != len(keys):
            num_dropped = len(keys) - keys["key_id"].nunique()
            warnings.warn(f"Dropped {num_dropped} duplicate rows from keys data.")
            keys = keys.loc[keys["key_id"].drop_duplicates().index]

        if messages["message_id"].nunique() != len(messages):
            num_dropped = len(messages) - messages["message_id"].nunique()
            warnings.warn(f"Dropped {num_dropped} duplicate rows from messages data.")
            messages = messages.loc[messages["message_id"].drop_duplicates().index]

        # 5. Raise a warning if there are keys without a corresponding message
        if not keys["message_id"].isin(messages["message_id"]).all():
            num_orphaned = (~keys["message_id"].isin(messages["message_id"])).sum()
            warnings.warn(f"There are {num_orphaned} keys without a corresponding message. These will not be included in the final dataframe")

        # 4. Merge dataframes
        merged_df = pd.merge(keys, messages, on="message_id", how="inner")
        self.__init__(merged_df)

        print(f"Loaded {len(self)} events")

        # 5. Drop unwanted data
        if system == "lh":
            if not include_anonymized:
                self._drop_anonymized()
            if not include_nonsense:
                self._drop_nonsense()
            if not include_native:
                self._drop_native()
        print("Created KeyLoggingDataFrame of shape", self.shape)

        return self

    def add_iki(self, colname: str = "iki", include_first_key: bool = False, include_nontyping_events: bool = False) -> "KeyLoggingDataFrame":
        """Add inter-key interval (IKI) column to the dataframe. The IKI is the time difference in milliseconds between consecutive key presses within the same message.
        input: 
            colname: str, name of the new column to be added (default is 'iki'). Make sure it does not already exist in the dataframe.
        output: KeyLoggingDataFrame with new IKI column"""
        
        # 1. verify parameters
        if "key_time" not in self.columns:
            raise ValueError("DataFrame must contain 'key_time' column to calculate IKI.")
        if "message_id" not in self.columns:
            raise ValueError("DataFrame must contain 'message_id' column to calculate IKI.")
        if "event_for_message_type" not in self.columns:
            raise ValueError("ignore_nontyping_events can only be used if 'event_for_message_type' column is present")
        if colname in self.columns:
            raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
        if include_nontyping_events is True and include_first_key is False:
            raise ValueError("include_first_key can only be False if include_nontyping_events is False")
        
        # 2. Make copy of dataframe in self
        df = self.copy()
        df = df.sort_values(by=['message_id', 'key_time'])

        # 3. Select events specified in parameters and compute IKI
        df["event_for_message_type"] = pd.to_numeric(df["event_for_message_type"])
        if not include_nontyping_events:
            if not include_first_key:
                df = df[df['event_for_message_type'] == 0]
                df['prev_key_time'] = df['key_time'].shift(1)
                df['prev_message_id'] = df['message_id'].shift(1)
                df[colname] = np.where(
                    (df['message_id'] == df['prev_message_id']) & (df['event_for_message_type'] == 0),
                    (df['key_time'] - df['prev_key_time']) / 1000000,
                    pd.NaT
                    )
            else:
                df = df[df['event_for_message_type'].isin([0, 5])]
                df['prev_key_time'] = df['key_time'].shift(1)
                df['prev_message_id'] = df['message_id'].shift(1)
                df[colname] = np.where(
                    (df['message_id'] == df['prev_message_id']) & (df['event_for_message_type'] == 0),
                    (df['key_time'] - df['prev_key_time']) / 1000000,
                    pd.NaT
                    )
        else:
            df['prev_key_time'] = df['key_time'].shift(1)
            df['prev_message_id'] = df['message_id'].shift(1)
            df[colname] = np.where(
                (df['message_id'] == df['prev_message_id']),
                (df['key_time'] - df['prev_key_time']) / 1000000,
                pd.NaT
                )
        
        # 4. Merge IKI column back into self on key_id
        merged_df = self.merge(df[['key_id', colname]], on='key_id', how='left')

        # 5. Set empty iki values to pd.NaT
        merged_df[colname] = pd.to_numeric(merged_df[colname], errors='coerce')

        self.__init__(merged_df)
        return self

    def add_pause(self, colname:str, method:str, threshold:float=None, a:float=None, iki_colname:str=None, include_first_key:bool=False, include_nontyping_events:bool=False) -> "KeyLoggingDataFrame":
        """Add a boolean pause column to the dataframe, which contains True when the key event is preceded by a pause and False if it is not. The IKI is set to NaN if there is no IKI available for the event
        Pauses can be either computed based on a fixed threshold IKI (in milliseconds) or individualized based on the median IKI of each user
        input:
            method: str, method to determine pauses: can be "fixed" or "individualized".
            threshold: float, IKI threshold in milliseconds above which a pause is marked.
            colname: str, name of the new column to be added (default is 'pause'). Make sure it does not already exist in the dataframe.
            include_first_key: bool, whether to include the first key event of each message when calculating IKIs (default is False).
            include_nontyping_events: bool, whether to include non-typing events (event_for_message_type != 0) when calculating IKIs (default is False).
        output: KeyLoggingDataFrame with new pause column"""
        try:
            # 1. verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
            if not method in ["fixed", "individualized"]:
                raise ValueError("Only 'fixed' and 'individualized' methods are supported.")
            if method == "fixed":
                if a:
                    raise AttributeError("Parameter 'a' is only used with method='individualized'")
                if not threshold:
                    threshold = 1000
                    print("No threshold specified, using default of 1000 ms")
                if not isinstance(threshold, (int, float)) or threshold <= 0:
                    raise ValueError("Threshold must be a positive number.")
            elif method == "individualized":
                if a is None:
                    a = 2
                    print("No 'a' value specified, using default of 2")
                if not isinstance(a, (int, float)):
                    raise ValueError("Parameter 'a' must be a number.")
                if threshold:
                    raise AttributeError("Parameter 'threshold' is only used with method='fixed'")
            if "key_time" not in self.columns:
                raise ValueError("DataFrame must contain 'key_time' column to calculate pauses.")
            if "message_id" not in self.columns:
                raise ValueError("DataFrame must contain 'message_id' column to calculate pauses.")
            if "event_for_message_type" not in self.columns:
                raise ValueError("ignore_nontyping_events can only be used if 'event_for_message_type' column is present")
            if not colname:
                colname = hf.generate_colname("pause", self.columns)
            if colname in self.columns:
                raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
            if include_nontyping_events is True and include_first_key is False:
                raise ValueError("include_first_key can only be False if include_nontyping_events is False")
            if iki_colname and iki_colname not in self.columns:
                    raise ValueError(f"IKI column '{iki_colname}' not found in DataFrame.")
                
            # 2. Make copy of dataframe in self
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # 3. add IKI column if not already present
            # indicate pauses
            if method == "fixed":
                # select non nan IKI key_ids
                non_nan_iki = df[df[iki_colname].notna()].index
                df[colname] = np.nan
                df[colname] = df[colname].astype("boolean")
                df.loc[non_nan_iki, colname] = df.loc[non_nan_iki, iki_colname] > threshold
            
            elif method == "individualized":
                # get median IKI per user
                median_iki = df.groupby("user_id")[iki_colname].median()
                median_mad = df.groupby("user_id")[iki_colname].apply(lambda x: (x - x.median()).abs().median())
                # addcolumn with median IKI per user
                df = df.merge(median_iki.rename("median_iki"), on="user_id", how="left")
                df = df.merge(median_mad.rename("median_mad"), on="user_id", how="left")
                # select non nan IKI key_ids
                non_nan_iki = df[df[iki_colname].notna()].index
                df.loc[non_nan_iki, "individualized_threshold"] = df.loc[non_nan_iki, "median_iki"] + a * df.loc[non_nan_iki, "median_mad"]
                df[colname] = np.nan
                df[colname] = df[colname].astype("boolean")
                df.loc[non_nan_iki, colname] = df.loc[non_nan_iki, iki_colname] > df.loc[non_nan_iki, "individualized_threshold"]
                print("1.", df)

            # 4. Merge pause column back into self on key_id
            #print(df.head())
            #print(self.head())
            merged_df = self.merge(df[['key_id', colname]], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e

    def add_pburst(self, colname:str=None, pause_colname:str=None, pause_method:str=None, pause_threshold:float=None, pause_a:float=None, iki_colname:str=None) -> "KeyLoggingDataFrame":
        """Add a boolean pburst column to the dataframe, which contains True when the key event is part of a pause burst and False if it is not. A pause burst is defined as a sequence of at least two consecutive key events that are each preceded by a pause.
        input:
            colname: str, name of the new column to be added (default is 'pburst'). Make sure it does not already exist in the dataframe.
            pause_colname: str, name of the column containing pause information (default is 'pause').
        output: KeyLoggingDataFrame with new pburst column"""
        try:
            # 1. verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. Load data first throught load_default_data or load_data_from_path")
            if "key_time" not in self.columns:
                raise ValueError("DataFrame must contain 'key_time' column to calculate pbursts.")
            if "message_id" not in self.columns:
                raise ValueError("DataFrame must contain 'message_id' column to calculate pbursts.")
            if not pd.api.types.is_bool_dtype(self[pause_colname]) and not pd.api.types.is_integer_dtype(self[pause_colname]) and not pd.api.types.is_float_dtype(self[pause_colname]):
                raise ValueError(f"Pause column '{pause_colname}' must be of boolean or numeric dtype.")
            if not colname:
                colname = hf.generate_colname("pburst", self.columns)
            if colname in self.columns:
                raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
            
                
            # 2. Make copy of dataframe in self
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # 3. create pause column if not already present
            #if pause_method is not None:
            #    if pause_colname is None:
            #        pause_colname = hf.generate_colname("pause", df.columns)
            #    df.add_pause(colname=pause_colname, method=pause_method, threshold=pause_threshold, a=pause_a, iki_colname=iki_colname, include_first_key=True, include_nontyping_events=False)

            # Create pburst column
            first_characters = df[df["event_for_message_type"] == 0].sort_values(by=["message_id", "key_time"]).groupby("message_id").head(1).index
            df["pburst"] = "O"  # default to 'O' (other/NaN)
            df.loc[df[pause_colname] == False, "pburst"] = "I"
            df.loc[df[pause_colname] == True, "pburst"] = "B"
            df.loc[first_characters, "pburst"] = "B"

            # merge df to self
            merged_df = self.merge(df[['key_id', 'pburst']], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e

    def add_action(self, colname:str=None):
        """
        Add an action column to the dataframe, which contains the action performed with each key event. Actions can be "addition", "deletion", "modification" or "no change".
        """# 1. verify parameters
        try:
            if self.empty:
                raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
            if "event_for_message_type" not in self.columns:
                raise ValueError("DataFrame must contain 'event_for_message_type' column to calculate actions.")
            if "content" not in self.columns:
                raise ValueError("DataFrame must contain 'content' column to calculate actions.")
            if not colname:
                colname = hf.generate_colname("action", self.columns)
            elif colname in self.columns:
                raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
            
            # 2. copy df
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # 3. select edits
            df["event_for_message_type"] = pd.to_numeric(df["event_for_message_type"])
            df = df[df['event_for_message_type'] == 0]
    
            # add a columns "previous_content", which contains the content of the previous event of the message. Contains an empty string if the event is the first event of the message
            df["previous_content"] = df["content"].shift(1)
            df["previous_message_id"] = df["message_id"].shift(1)

            # add action using pandas apply
            df['action'] = df.apply(
                lambda row: hf.detect_action(row['content'], row['previous_content']),
                axis=1
            )

            # set action to NaN if different messages
            df.loc[df['message_id'] != df['previous_message_id'], 'action'] = None


            # 4. Merge action column back into self on key_id
            merged_df = self.merge(df[['key_id', 'action']], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e
        
    def add_span(self, colnames:list=[None, None, None, None], selection:list=[True,True,True,True]):
        """
        Add 4 columns to the dataframe which indicate the range of the event: start_deletion_span, end_deletion_span, start_addition_span, end_addition_span
        """
        try:
            # 1. verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
            if "event_for_message_type" not in self.columns:
                raise ValueError("DataFrame must contain 'action' column to calculate ranges. Use add_action() to add this column.")
            if "content" not in self.columns:
                raise ValueError("DataFrame must contain 'content' column to calculate ranges.")
            if not colnames or colnames == [None, None, None, None]:
                colnames = []
                for base in ["start_deletion_span", "end_deletion_span", "start_addition_span", "end_addition_span"]:
                    colname.append(hf.generate_colname(base, self.columns))
            elif not isinstance(colnames, list) or len(colnames) != 4:
                raise ValueError("colnames must be a list of 4 strings (or None).")
            else:
                for colname in colnames:
                    if colname in self.columns:
                        raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
                    

            # 2. copy df
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # 3. select edits only
            df["event_for_message_type"] = pd.to_numeric(df["event_for_message_type"])
            df = df[df['event_for_message_type'] == 0]
    
            # add a columns "previous_content", which contains the content of the previous event of the message. Contains an empty string if the event is the first event of the message
            df["previous_content"] = df["content"].shift(1)
            df["previous_message_id"] = df["message_id"].shift(1)

            # add range using pandas apply
            if selection[0] == True:
                if not colnames[0]:
                    colnames[0] = hf.generate_colname("start_deletion_span", df.columns)
                df[colnames[0]] = df.apply(
                    lambda row: hf.detect_start_deletion_span(row['content'], row['previous_content']),
                    axis=1
                )
            
            if selection[1] == True:
                if not colnames[1]:
                    colnames[1] = hf.generate_colname("end_deletion_span", df.columns)
                df[colnames[1]] = df.apply(
                    lambda row: hf.detect_end_deletion_span(row['content'], row['previous_content']),
                    axis=1
                )
            if selection[2] == True:
                if not colnames[2]:
                    colnames[2] = hf.generate_colname("start_addition_span", df.columns)
                df[colnames[2]] = df.apply(
                    lambda row: hf.detect_start_addition_span(row['content'], row['previous_content']),
                    axis=1
                )

            if selection[3] == True:
                if not colnames[3]:
                    colnames[3] = hf.generate_colname("end_addition_span", df.columns)
                df[colnames[3]] = df.apply(
                    lambda row: hf.detect_end_addition_span(row['content'], row['previous_content']),
                    axis=1
                )

            added_colnames = [col for col, sel in zip(colnames, selection) if sel]
            # set spans to NaN if different messages
            df.loc[df['message_id'] != df['previous_message_id'], added_colnames] = None

            # 4. Merge action column back into self on key_id
            merged_df = self.merge(df[['key_id']+added_colnames], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e

    def add_length(self, colnames:list=[None, None], selection=[True, True]) -> "KeyLoggingDataFrame":
        """
        Add 2 columns to the dataframe which indicate the length of the the deleted sequence and the length of the added sequence
        """
        # 1. verify parameters
        if self.empty:
            raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
        if "event_for_message_type" not in self.columns:
            raise ValueError("DataFrame must contain 'event_for_message_type' column to calculate lengths.")
        if "content" not in self.columns:
            raise ValueError("DataFrame must contain 'content' column to calculate lengths.")
        if not colnames:
            colnames = [None, None, None, None]
        elif not isinstance(colnames, list) or len(colnames) != 2:
            raise ValueError("colnames must be a list of 2 strings (or None).")
        else:
            for colname in colnames:
                if colname in self.columns:
                    raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
                
        # 2. copy df
        df = self.copy()
        df = df.sort_values(by=['message_id', 'key_time'])

        # 3. select edits only
        df["event_for_message_type"] = pd.to_numeric(df["event_for_message_type"])
        df = df[df['event_for_message_type'] == 0]

        #
        if selection[0]: # deletion length
            # get deletion span
            startcol = hf.generate_colname("start_deletion_span", df.columns)
            endcol = hf.generate_colname("end_deletion_span", df.columns)
            df.add_span(colnames=[startcol, endcol, None, None], selection=[True, True, False, False])
            # calculate length
            df[colnames[0]] = df[endcol] - df[startcol]
        
        if selection[1]: # addition length
            # get addition span
            startcol = hf.generate_colname("start_addition_span", df.columns)
            endcol = hf.generate_colname("end_addition_span", df.columns)
            df.add_span(colnames=[None, None, startcol, endcol], selection=[False, False, True, True])
            # calculate length
            df[colnames[1]] = df[endcol] - df[startcol]

        
        # 4. Merge action column back into self on key_id
        merged_df = self.merge(df[['key_id', 'length_deletion', 'length_addition']], on='key_id', how='left')
        self.__init__(merged_df)
        return self
    

    def add_rburst(self, colname:str=None, action_colname:str=None) -> "KeyLoggingDataFrame":
        """Add a column to the dataframe annotating in IOB format whether the key event is the beginning of a revision burst (B), inside a revision burst (I) or outside a revision burst (O)."""
        try:
            # 1. verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. Load data first throught load_default_data or load_data_from_path")
            if not action_colname:
                raise ValueError("Action column name must be specified.")
            if "message_id" not in self.columns:
                raise ValueError("DataFrame must contain 'message_id' column to calculate pbursts.")
            if not pd.api.types.is_bool_dtype(self[pause_colname]) and not pd.api.types.is_integer_dtype(self[pause_colname]) and not pd.api.types.is_float_dtype(self[pause_colname]):
                raise ValueError(f"Pause column '{pause_colname}' must be of boolean or numeric dtype.")
            if not colname:
                colname = hf.generate_colname("pburst", self.columns)
            if colname in self.columns:
                raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
            
                
            # 2. Make copy of dataframe in self
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # 3. create pause column if not already present
            #if pause_method is not None:
            #    if pause_colname is None:
            #        pause_colname = hf.generate_colname("pause", df.columns)
            #    df.add_pause(colname=pause_colname, method=pause_method, threshold=pause_threshold, a=pause_a, iki_colname=iki_colname, include_first_key=True, include_nontyping_events=False)

            # Create pburst column
            first_characters = df[df["event_for_message_type"] == 0].sort_values(by=["message_id", "key_time"]).groupby("message_id").head(1).index
            df["pburst"] = "O"  # default to 'O' (other/NaN)
            df.loc[df[pause_colname] == False, "pburst"] = "I"
            df.loc[df[pause_colname] == True, "pburst"] = "B"
            df.loc[first_characters, "pburst"] = "B"

            # merge df to self
            merged_df = self.merge(df[['key_id', 'pburst']], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e

    def add_distance_to_end(self, colname:str=None) -> "KeyLoggingDataFrame":
        try: 
            # verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
            if "content" not in self.columns:
                raise ValueError("DataFrame must contain 'content' column to calculate distance to end.")
            if "key_id" not in self.columns:
                raise ValueError("DataFrame must contain 'key_id' column to calculate distance to end.")
            
            df = self.copy()
            end_del_colname = hf.generate_colname("end_deletion_span", df.columns)
            end_add_colname  = hf.generate_colname("end_addition_span", df.columns)
            df = df.add_span(colnames=[None, end_del_colname, None, end_add_colname], selection=[False,True,False,True])
            df[colname] = df.apply(
                lambda row: hf.detect_distance_to_end(row['content'], row[end_del_colname], row[end_add_colname]),
                axis=1
            )
            merged_df = self.merge(df[['key_id', colname]], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e
    
    def add_revision(self, colname:str=None):
        """add a column indicating which events are part of a revision according to the IOB format"""
        try:
            # 1. verify parameters
            if self.empty:
                raise ValueError("DataFrame is empty. load data first throught load_default_data or load_data_from_path")
            if not action_colname:
                raise ValueError("Action column name must be specified.")
            if action_colname not in self.columns:
                raise ValueError(f"Action column '{action_colname}' not found in DataFrame.")
            if "message_id" not in self.columns:
                raise ValueError("DataFrame must contain 'message_id' column to calculate revisions.")
            if "key_id" not in self.columns:
                raise ValueError("DataFrame must contain 'key_id' column to calculate revisions.")
            if not pd.api.types.is_object_dtype(self[action_colname]) and not pd.api.types.is_string_dtype(self[action_colname]):
                raise ValueError(f"Action column '{action_colname}' must be of string dtype.")
            if not set(self[action_colname].dropna().unique()).issubset({"addition", "deletion", "modification", "no change"}):
                raise ValueError(f"Action column '{action_colname}' must only contain the values 'addition', 'deletion', 'modification' and 'no change'.")
            if colname in self.columns:
                raise ValueError(f"Column '{colname}' already exists in the DataFrame. Please choose a different name.")
            if not colname:
                colname = hf.generate_colname("revision", self.columns)

            # 2. Make copy of dataframe in self
            df = self.copy()
            df = df.sort_values(by=['message_id', 'key_time'])

            # add action and distance_to_end


            # 3. Create revision column
            df["revision"] = "O"

            # Mark deletions and within-text insertions and  as 'I'
            

            first_characters = df[df["event_for_message_type"] == 0].sort_values(by=["message_id", "key_time"]).groupby("message_id").head(1).index
            df["revision"] = "O"  # default to 'O' (other/NaN)
            df.loc[df[action_colname] != "no change", "revision"] = "B"
            df.loc[first_characters, "revision"] = "O"

            # merge df to self
            merged_df = self.merge(df[['key_id', 'revision']], on='key_id', how='left')
            self.__init__(merged_df)
            return self
        except Exception as e:
            raise e


    def pause_dataframe(self, pause_colname:str=None, iki_colname:str=None) -> pd.DataFrame:
        """returns a Pandas DataFrame in which each row represents a pause in the KeyLoggingDataFrame. Besides the normal keylogging data, the dataframe contains the following columns:
        - pause_location: "word_start", "word_middle", "word_end" (words are defined as sequences of alphannumeric characters)
        """


    def burst_dataframe(self, burst_colname:str=None, iki_colname:str=None, action_colname:str=None) -> pd.DataFrame:
        """Returns a dataframe containing the character length and duration of each burst in the KeyLoggingDataFrame.
        input: self is a KeyLoggingDataFrame 
            burst_colname: str, name of the column containing burst tags in IOB format (generated with add_pburst or add_rburst)
        output: pd.DataFrame with one row per burst and columns: burst_id, message_id, user_id, session_id, start_time, end_time, duration, length
        """
        # verify parameters
        if not burst_colname:
            raise ValueError("Burst column name must be specified.")
        if burst_colname not in self.columns:
            raise ValueError(f"Burst column '{burst_colname}' not found in DataFrame.")
        if not iki_colname:
            raise ValueError("IKI column name must be specified.")
        if iki_colname not in self.columns:
            raise ValueError(f"IKI column '{iki_colname}' not found in DataFrame.")
        if self.empty:
            return pd.DataFrame()  # return empty dataframe if self is empty

        events_df = self.copy()
        # group by message id and apply get burst_metrics to each group
        burst_df = events_df.groupby("message_id").apply(
            lambda x: hf.get_burst_metrics(x, burst_colname=burst_colname, iki_colname=iki_colname)
        ).reset_index(drop=True)
        return burst_df

    def revision_dataframe(self, rburst_colname:str=None, action_colname:str=None) -> pd.DataFrame:
        pass
    
    def pburst_analysis(self, pburst_colname:str=None) -> "KeyLoggingDataFrame" :
        # verify parameters
        if not pburst_colname:
            raise ValueError("Pburst column name must be specified.")
        if pburst_colname and pburst_colname not in self.columns:
            raise ValueError(f"Pburst column '{pburst_colname}' not found in DataFrame.")
        
        # add pburst column if necessary
        if not pburst_colname:
            pburst_colname = hf.generate_colname("pburst", self.columns)
            self.add_pburst(colname=pburst_colname, method=pause_method, threshold=pause_threshold, a=pause_a, iki_colname=iki_colname, include_first_key=True, include_nontyping_events=False)
            print(f"Added pburst column '{pburst_colname}' using method '{pause_method}'")

        # 

    def pause_analysis(self, pause_colname:str=None, iki_colname:str=None) -> pd.DataFrame:
        # verify parameters
        if not pause_colname:
            pause_colname = hf.generate_colname("pause", self.columns)
        if pause_colname not in self.columns:
            raise ValueError(f"Pause column '{pause_colname}' not found in DataFrame.")
        if not pd.api.types.is_bool_dtype(self[pause_colname]) and not pd.api.types.is_integer_dtype(self[pause_colname]) and not pd.api.types.is_float_dtype(self[pause_colname]):
            raise ValueError(f"Pause column '{pause_colname}' must be of boolean or numeric dtype.")

        df = self.copy()
        df["event_for_message_type"] = pd.to_numeric(df["event_for_message_type"])
        df["iki"] = pd.to_numeric(df["iki"], errors='coerce')
        df = df[df['event_for_message_type'] == 0]  # consider only typing events

        # calculate summary statistics
        nb_keys = len(df)
        nb_pauses = df[pause_colname].sum()
        percentage_pauses_per_keys = nb_pauses / nb_keys * 100 if nb_keys > 0 else None
        total_duration = df[iki_colname].sum()
        pause_duration = df.loc[df[pause_colname] == True, iki_colname].sum()
        percentage_pause_duration = pause_duration / total_duration * 100 if total_duration > 0 else None
        avg_pause_length = df.loc[df[pause_colname] == True, iki_colname].mean()
        std_pause_length = df.loc[df[pause_colname] == True, iki_colname].std()
        median_pause_length = df.loc[df[pause_colname] == True, iki_colname].median()
        mad_pause_length = (df.loc[df[pause_colname] == True, iki_colname] - df.loc[df[pause_colname] == True, iki_colname].median()).abs().median()
        max_pause_length = df.loc[df[pause_colname] == True, iki_colname].max()
        min_pause_length = df.loc[df[pause_colname] == True, iki_colname].min()

        # create summary dataframe
        metrics = {
            "nb_pauses": int(nb_pauses) if pd.notna(nb_pauses) else None,
            "percentage_pauses_per_keys": round(percentage_pauses_per_keys,3) if pd.notna(percentage_pauses_per_keys) else None,
            "total_pause_duration": int(pause_duration) if pd.notna(pause_duration) else None,
            "percentage_pause_duration": round(percentage_pause_duration,3) if pd.notna(percentage_pause_duration) else None,
            "mean_pause_length": round(avg_pause_length,3) if pd.notna(avg_pause_length) else None,
            "std_pause_length": round(std_pause_length,3) if pd.notna(std_pause_length) else None,
            "median_pause_length": int(median_pause_length) if pd.notna(median_pause_length) else None,
            "mad_pause_length": int(mad_pause_length) if pd.notna(mad_pause_length) else None,
            "max_pause_length": int(max_pause_length) if pd.notna(max_pause_length) else None,
            "min_pause_length": int(min_pause_length) if pd.notna(min_pause_length) else None
        }
        metrics = pd.DataFrame([metrics])
        return metrics
        
    