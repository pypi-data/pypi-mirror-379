---
queues: # Queue definitions (Partitions in SLURM)
  gpu.q: # Queue name
    time: 18000 # Maximum job rum time (minutes)
    max_size: 250 # Maximum memory per job (GB)
    slot_size: 64 # Maximum memory per CPU core (GB)
    max_slots: 20 # Maximum number of threads per job (e.g. number of CPU cores)
    copros: # Available coprocessors
      cuda: # Coprocessor name (same as configuration above)
        max_quantity: 4 # Number of devices available per job (e.g. number of devices on a single node)
        classes: # List of available classes as defined in the coproc_opts section 'class' keys
          - K
          - P
          - V
    parallel_envs: # List of available parallel environments - remove for SLURM clusters
      - openmp
    map_ram: true  # Split jobs over multiple slots when requesting more RAM than available in a single slot?
    priority: 1 # Priority of queue, higher numbers win
    group: 0 # Group of queue - groups variations of a queue, the 'priority' decides which variant to use
  short:
    time: 1440
    max_size: 160
    slot_size: 4
    max_slots: 16
    patallel_envs: # List of available parallel environments - remove for SLURM clusters
      - openmp
    map_ram: true
    priority: 1
    group: 1
    smt: False  # SLURM specific - see fsl_sub_plugin_slurm documentation
  long:
    time: 10080
    max_size: 368
    slot_size: 16
    max_slots: 24
    patallel_envs: # List of available parallel environments - remove for SLURM clusters
      - openmp
    map_ram: true
    priority: 1
    group: 2
    default: True