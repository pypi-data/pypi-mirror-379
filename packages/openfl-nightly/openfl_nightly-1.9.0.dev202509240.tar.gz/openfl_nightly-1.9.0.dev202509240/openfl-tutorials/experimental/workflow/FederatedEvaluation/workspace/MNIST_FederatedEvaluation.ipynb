{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Federated Evaluation with MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd059520",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Welcome to the first OpenFL Federated Evaluation Workflow Interface tutorial! This notebook demonstrates OpenFL capability of running your first horizontal federated evaluation workflow. This work has the following goals:\n",
    "\n",
    "- Template for federated evaluation exposing key metrics post evaluation run (e.g model accuracy)\n",
    "- Build on top of first example of learning via workflow API (refer [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) ) using MNIST dataset and perform fedeval (federated evaluation) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d98490",
   "metadata": {},
   "source": [
    "First we start by installing OpenFL as per [installation guide](https://openfl.readthedocs.io/en/latest/installation.html).\n",
    "\n",
    "The next step is to install the Workflow API dependencies, and PyTorch for the ML part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47734ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../../workflow_interface_requirements.txt\n",
    "# Uncomment this section if running in Google Colab\n",
    "#!pip install -r https://raw.githubusercontent.com/securefederatedai/openfl/refs/heads/develop/openfl-tutorials/experimental/workflow/workflow_interface_requirements.txt\n",
    "!pip install torch==2.7.0 torchvision==0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fd3b8-c531-4fd3-be07-ff43cfcdb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac0fac",
   "metadata": {},
   "source": [
    "One foundational pre-requisite for evaluation is to have a pre-trained model available and thats exactly what this notebook expects as a pre-requisite:\n",
    "- A pre-trained model that can be loaded for evaluation\n",
    "\n",
    "For this tutorial, let's use the final output model of [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) run, a sample of same is saved at [Pre-trained model](../pretrainedmodels/cnn_mnist.pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682720b",
   "metadata": {},
   "source": [
    "Sample of the output of training run model that was saved"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f557a6e-ef87-4149-aff2-e74103f6c0ce",
   "metadata": {
    "vscode": {
     "languageId": "code-text-binary"
    }
   },
   "source": [
    "Sample of the final model weights: tensor([[[ 0.1221, -0.0846, -0.0635,  0.0590, -0.2059],\n",
    "         [ 0.1558, -0.0202,  0.1005,  0.0272, -0.0148],\n",
    "         [ 0.1034,  0.0560,  0.1089, -0.0367,  0.0182],\n",
    "         [ 0.0086,  0.0602,  0.0315,  0.2058,  0.0909],\n",
    "         [-0.0778, -0.1234, -0.0414, -0.0904, -0.0548]]])\n",
    "\n",
    "Final aggregated model accuracy for 2 rounds of training: 0.8463999927043915"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "Let's first define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment, however \n",
    "notice the difference in this setup compared to a typical training/learning setup as detailed in [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) :\n",
    "- There is no need to download the training set as we will do only evaluation\n",
    "- No optimizer settings needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Set parameters\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load and transform the MNIST test dataset\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"./files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) for digit classification on the MNIST dataset.\n",
    "\n",
    "    Architecture:\n",
    "    - Conv2D layer with 10 filters, kernel size 5\n",
    "    - MaxPooling layer with size 2\n",
    "    - Conv2D layer with 20 filters, kernel size 5\n",
    "    - Dropout2D for regularization\n",
    "    - MaxPooling layer with size 2\n",
    "    - Fully connected layer (320 -> 50)\n",
    "    - Dropout\n",
    "    - Fully connected output layer (50 -> 10)\n",
    "    - LogSoftmax activation at output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize CNN layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Log-probabilities for each digit class\n",
    "        \"\"\"\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "def inference(network,test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        network (nn.Module): Trained neural network model.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test dataset.\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e406db6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we come to the flow definition. The OpenFL Workflow Interface adopts the conventions set by Metaflow, that every workflow begins with `start` and concludes with the `end` task. The aggregator begins with an optionally passed in model and optimizer. The aggregator begins the flow with the `start` task, where the list of collaborators is extracted from the runtime (`self.collaborators = self.runtime.collaborators`) and is then used as the list of participants to run the task listed in `self.next`, `evaluate`. The model, optimizer, and anything that is not explicitly excluded from the next function will be passed from the `start` function on the aggregator to the `evaluate` task on the collaborator. Where the tasks run is determined by the placement decorator that precedes each task definition (`@aggregator` or `@collaborator`). Once each of the collaborators (defined in the runtime) complete the `evaluate` task, they finally `join` at the aggregator doing just model evaluation/validation without any training. It is in `join` that an accuracy of model per collaborator is shown.\n",
    "\n",
    "![FedEval.png](../../../../../docs/images/FedEval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "class FederatedEvaluationFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, rounds=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            \n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(self.evaluate, foreach='collaborators', exclude=['private'])\n",
    "\n",
    "    @collaborator\n",
    "    def evaluate(self):\n",
    "        print(f'Performing model evaluation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        print(f'Average aggregated model accuracy values = {self.aggregated_model_accuracy}')\n",
    "\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(self.evaluate,\n",
    "                      foreach='collaborators', exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ab527",
   "metadata": {},
   "source": [
    "Now let's setup the participants in similar fashion as basic learning/training tutorial but notice the difference in the setup below since we are doing only evaluation there is no need to configure training related data, targets and data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "n_collaborators = 2\n",
    "collaborator_names = [\"Bengaluru\", \"Portland\"]\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_test, shuffle=True)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "278ad46b",
   "metadata": {},
   "source": [
    "Now that we have our evaluation flow and runtime defined, let's run the experiment! Since its evaluation we need to run it only for one round of validation and for that first we will load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file \n",
    "url = 'https://github.com/securefederatedai/openfl/raw/refs/heads/develop/openfl-tutorials/experimental/pretrainedmodels/cnn_mnist.pth'\n",
    "response = requests.get(url)\n",
    "with open('cnn_mnist.pth', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load the model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('cnn_mnist.pth'))\n",
    "best_model = model\n",
    "flflow = FederatedEvaluationFlow(model, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cc8f7",
   "metadata": {},
   "source": [
    "Now that the flow has completed, let's get the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nFinal model accuracy for {flflow.rounds} rounds of evaluation: {flflow.aggregated_model_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec521c90",
   "metadata": {},
   "source": [
    "It should ideally report +-0.05 as per the pre-trained models' accuracy that is used in this experiment which, as detailed above, was ~0.846"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a61a876d",
   "metadata": {},
   "source": [
    "Now that the flow is complete, let's dig into some of the information captured along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = flflow._run_id\n",
    "from metaflow import Metaflow, Flow, Task, Step\n",
    "m = Metaflow()\n",
    "s = Step(f'FederatedEvaluationFlow/{run_id}/evaluate')\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2958c64-0c26-484f-a18a-edbfc246c369",
   "metadata": {},
   "source": [
    "Now we see **4** steps: **2** collaborators each performed **1** rounds of model evaluation\n",
    "Let's look at one of those data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df28feb-724f-4c41-9f67-770929e227a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Task(f'FederatedEvaluationFlow/{run_id}/evaluate/2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef877a50",
   "metadata": {},
   "source": [
    "Now let's look at the data artifacts this task generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9826c45f",
   "metadata": {},
   "source": [
    "Now let's look at its log output (stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235d3d8",
   "metadata": {},
   "source": [
    "For more details on checkpointing and using Metaflow to dig into more details of a federation run please refer to previous tutorials on learning like [101 MNIST Notebook](https://github.com/securefederatedai/openfl/tree/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957faa4b-bb38-47b4-9643-b4d832b99c87",
   "metadata": {},
   "source": [
    "### Setup Federation: Director & Envoys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78cd58-5e87-4002-bdb5-603fda1e81c2",
   "metadata": {},
   "source": [
    "Before we can deploy the experiment, let us create participants in Federation: Director and Envoys. As the Tutorial uses two collaborators we shall launch three participants:\n",
    "\n",
    "1. Director: The central node in the Federation\n",
    "2. Bengaluru: The first envoy in the Federation\n",
    "3. Portland: The second envoy in the Federation\n",
    "   \n",
    "The participants can be launched by following steps mentioned in [README]((https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/FederatedRuntime/101_MNIST/README.md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a84130-ae1c-43f3-847b-d6ecd03e0b4e",
   "metadata": {},
   "source": [
    "### Deploy: FederatedRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0f1db-9378-4a1a-9916-5ea68a190f32",
   "metadata": {},
   "source": [
    "We now import and instantiate `FederatedRuntime` to enable deployment of experiment on distributed infrastructure. Initializing the `FederatedRuntime` requires following inputs to be provided by the user:\n",
    "\n",
    "- `director_info` – director information including fqdn of the director node, port, and certificate information\n",
    "- `collaborators` - names of the collaborators participating in experiment\n",
    "- `notebook_path`- path to this jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8630d-eb56-4e77-8363-c1d2631dc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.experimental.workflow.runtime import FederatedRuntime\n",
    "\n",
    "director_info = {\n",
    "    'director_node_fqdn': 'localhost',\n",
    "    'director_port': 50050,\n",
    "}\n",
    "\n",
    "federated_runtime = FederatedRuntime(\n",
    "    collaborators = collaborator_names,\n",
    "    director=director_info,\n",
    "    notebook_path='./MNIST_FederatedEvaluation.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd1fc8-6f7b-48ad-bdc5-dc79011b3c24",
   "metadata": {},
   "source": [
    "Let us connect to federation & check if the envoys are connected to the director by using the `get_envoys` method of `FederatedRuntime`. If the participants are launched successful in previous step the status of `Bengaluru` and `Portland` should be displayed as `Online`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5f9c5-3f34-41a1-8596-6c2b76c5be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_runtime.get_envoys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082c6e2-f37f-4e23-92c8-9c836df62aad",
   "metadata": {},
   "source": [
    "Now that we have our distributed infrastructure ready, let us modify the flow runtime to `FederatedRuntime` instance and deploy the experiment. \n",
    "\n",
    "Progress of the flow is available on \n",
    "1. Jupyter notebook: if `checkpoint` attribute of the flow object is set to `True`\n",
    "2. Director and Envoy terminals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012f4b4-63d1-495d-a730-c153540ec335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file \n",
    "url = 'https://github.com/securefederatedai/openfl/raw/refs/heads/develop/openfl-tutorials/experimental/pretrainedmodels/cnn_mnist.pth'\n",
    "response = requests.get(url)\n",
    "with open('cnn_mnist.pth', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"downloading cnn_mnist.pth file is completed\")\n",
    "# Load the model\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('cnn_mnist.pth'))\n",
    "print(\"cnn_mnist.pth \")\n",
    "best_model = model\n",
    "flflow = FederatedEvaluationFlow(model, checkpoint=True)\n",
    "flflow.runtime = federated_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedfddc3-4de5-41b7-b6d5-5e2013bab0fc",
   "metadata": {},
   "source": [
    "Now that the flow has completed, let's get the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378266ad-da61-4ef6-9c03-efe0d46b60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nFinal model accuracy for {flflow.rounds} rounds of evaluation: {flflow.aggregated_model_accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "426f2395",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You've successfully completed your first Federated Evaluation workflow interface quickstart notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
