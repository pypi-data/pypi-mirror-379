{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated PyTorch UNET Tutorial using Workflow API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "%pip install torch\n",
    "%pip install matplotlib\n",
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import installed modules\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from torchvision import transforms as tsf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "from openfl.utilities import validate_file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Kvasir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://datasets.simula.no/downloads/hyper-kvasir/hyper-kvasir-segmented-images.zip' -O kvasir.zip\n",
    "ZIP_SHA384 = ('66cd659d0e8afd8c83408174'\n",
    "            '1ade2b75dada8d4648b816f2533c8748b1658efa3d49e205415d4116faade2c5810e241e')\n",
    "validate_file_hash('./kvasir.zip', ZIP_SHA384)\n",
    "!unzip -n kvasir.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our dataset and model to perform federated learning on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/segmented-images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Read image and mask from disk.\n",
    "    \"\"\"\n",
    "    img = io.imread(image_path)\n",
    "    assert(img.shape[2] == 3)\n",
    "    mask = io.imread(mask_path)\n",
    "    return (img, mask[:, :, 0].astype(np.uint8))\n",
    "\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Kvasir dataset contains 1000 images for all collaborators.\n",
    "    Args:\n",
    "        data_path: path to dataset on disk\n",
    "        collaborator_count: total number of collaborators\n",
    "        collaborator_num: number of current collaborator\n",
    "        is_validation: validation option\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collaborator_count, collaborator_num, is_validation):\n",
    "        self.images_path = DATA_PATH + 'images/'\n",
    "        self.masks_path = DATA_PATH + 'masks/'\n",
    "        self.images_names = [\n",
    "            img_name\n",
    "            for img_name in sorted(listdir(self.images_path))\n",
    "            if len(img_name) > 3 and img_name[-3:] == 'jpg'\n",
    "        ]\n",
    "\n",
    "        self.images_names = self.images_names[collaborator_num:: collaborator_count]\n",
    "        self.is_validation = is_validation\n",
    "        assert(len(self.images_names) > 8)\n",
    "        validation_size = len(self.images_names) // 8\n",
    "        if is_validation:\n",
    "            self.images_names = self.images_names[-validation_size:]\n",
    "        else:\n",
    "            self.images_names = self.images_names[: -validation_size]\n",
    "\n",
    "        self.img_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332)),\n",
    "            tsf.ToTensor(),\n",
    "            tsf.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "        self.mask_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332), interpolation=PIL.Image.NEAREST),\n",
    "            tsf.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.images_names[index]\n",
    "        img, mask = read_data(self.images_path + name, self.masks_path + name)\n",
    "        img = self.img_trans(img).numpy()\n",
    "        mask = self.mask_trans(mask).numpy()\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(Up, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        if bilinear:\n",
    "            self.Up = nn.Upsample(\n",
    "                scale_factor=2,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=True\n",
    "            )\n",
    "        else:\n",
    "            self.Up = nn.ConvTranspose2d(in_ch, in_ch // 2, 2, stride=2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.Up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX //\n",
    "                        2, diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is setting up the participants, an `Aggregator` and a few `Collaborator`s which will train the model, partition the dataset between the collaborators, and pass them to the appropriate runtime environment (in our case, a `LocalRuntime`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup participants\n",
    "aggregator_ = Aggregator()\n",
    "aggregator_.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = [f'collaborator{i}' for i in range(2)]\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "\n",
    "for collaborator_idx, collaborator_ in enumerate(collaborators):\n",
    "    collaborator_.private_attributes = {\n",
    "            'train_loader': DataLoader(KvasirDataset(len(collaborators), collaborator_idx, is_validation=False),\n",
    "                                                         num_workers=8, batch_size=6, shuffle=True),\n",
    "            'test_loader': DataLoader(KvasirDataset(len(collaborators), collaborator_idx, is_validation=True),\n",
    "                                                        num_workers=8, batch_size=6)\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator_, collaborators=collaborators, backend='single_process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an aggregation algorithm, optimizer and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(models, weights=None):\n",
    "    \"\"\"\n",
    "    Federated averaging of model parameters.\n",
    "\n",
    "    Args:\n",
    "        models (list[torch.nn.Module]): List of PyTorch models to aggregate.\n",
    "        weights (list[float], optional): List of weights for each model.\n",
    "            Defaults to equal weights if not specified.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Module: New model with averaged parameters.\n",
    "    \"\"\"    \n",
    "    # Start with a new model based on the first model's architecture\n",
    "    new_model = type(models[0])()\n",
    "    new_model.load_state_dict(models[0].state_dict())\n",
    "\n",
    "    # Aggregate parameters\n",
    "    new_state_dict = {}\n",
    "    for key in models[0].state_dict().keys():\n",
    "        # Collect the corresponding parameters from all models\n",
    "        tensors = [model.state_dict()[key].cpu().numpy() for model in models]\n",
    "        avg = np.average(tensors, weights=weights, axis=0)\n",
    "        new_state_dict[key] = torch.tensor(avg, dtype=models[0].state_dict()[key].dtype).to(models[0].state_dict()[key].device)\n",
    "\n",
    "    new_model.load_state_dict(new_state_dict)\n",
    "    return new_model\n",
    "\n",
    "def get_optimizer(model):\n",
    "    return optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def soft_dice_loss(output, target):\n",
    "    num = target.size(0)\n",
    "    m1 = output.view(num, -1)\n",
    "    m2 = target.view(num, -1)\n",
    "    intersection = m1 * m2\n",
    "    score = 2.0 * (intersection.sum(1) + 1) / (m1.sum(1) + m2.sum(1) + 1)\n",
    "    score = 1 - score.sum() / num\n",
    "    return score\n",
    "\n",
    "def soft_dice_coef(output, target):\n",
    "    num = target.size(0)\n",
    "    m1 = output.view(num, -1)\n",
    "    m2 = target.view(num, -1)\n",
    "    intersection = m1 * m2\n",
    "    score = 2.0 * (intersection.sum(1) + 1) / (m1.sum(1) + m2.sum(1) + 1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up work to be executed by the aggregator and the collaborators by extending `FLSpec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedFlow(FLSpec):\n",
    "    def __init__(self, model=None, optimizer=None, rounds=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.n_rounds = rounds\n",
    "        self.loss = 0.\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.current_round = 0\n",
    "        self.next(self.aggregated_model_validation, foreach='collaborators')\n",
    "\n",
    "    def evaluate_segmentation_accuracy(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_dice_score = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_target in data_loader:\n",
    "                num_samples = batch_target.shape[0]\n",
    "                total_samples += num_samples                \n",
    "                output = self.model(batch_data)\n",
    "                batch_dice_score = soft_dice_coef(output, batch_target)\n",
    "                total_dice_score += batch_dice_score.sum().cpu().numpy()\n",
    "    \n",
    "        if total_samples == 0:\n",
    "            print(\"\\nValidation set is empty. Returning score as 0.0\\n\")\n",
    "            return 0.0\n",
    "        \n",
    "        avg_dice_score = total_dice_score / total_samples\n",
    "        print(f\"\\nValidation Results: Average Dice Coefficient: {avg_dice_score:.4f} (over {total_samples} samples)\\n\")\n",
    "        return avg_dice_score\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}, model: {id(self.model)}')\n",
    "        self.agg_validation_score = self.evaluate_segmentation_accuracy(self.test_loader)\n",
    "\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        # Log after processing a quarter of the samples\n",
    "        log_threshold = .25\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        self.optimizer = get_optimizer(self.model)\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = soft_dice_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (len(data) * batch_idx) / len(self.train_loader.dataset) >= log_threshold:\n",
    "                print('Train Epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "                self.loss = loss.item()\n",
    "                log_threshold += .25\n",
    "                torch.save(self.model.state_dict(), 'model.pth')\n",
    "                torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
    "            \n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        print(f'Performing local model validation for collaborator {self.input}')\n",
    "        self.local_validation_score = self.evaluate_segmentation_accuracy(self.test_loader)\n",
    "        print(\n",
    "            f'Done with local model validation for collaborator {self.input}, Accuracy: {self.local_validation_score}')\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        print(f'joining')\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = inputs[0].optimizer\n",
    "        self.current_round += 1\n",
    "\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs) / len(inputs)\n",
    "        print(f'Average aggregated model accuracy = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "\n",
    "        if self.current_round < self.n_rounds:\n",
    "            self.next(self.aggregated_model_validation, foreach='collaborators')\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'Flow ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the federation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "flflow = FederatedFlow(model, get_optimizer(model), rounds=30, checkpoint=False)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfl-quickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
