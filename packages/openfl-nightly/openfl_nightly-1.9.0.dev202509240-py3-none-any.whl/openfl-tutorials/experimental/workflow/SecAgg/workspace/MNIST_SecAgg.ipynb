{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc13070c",
   "metadata": {},
   "source": [
    "# Secure Aggregation with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0701e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In Federated Learning (FL), Secure Aggregation (SecAgg) is a technique that allows the participants to collaborate on the central model without revealing their individual contributions (local model updates). The goal is to allow participants to aggregate their model updates in a secure and privacy-preserving fashion. \n",
    "\n",
    "SecAgg is therefore used to:\n",
    "- Safeguard the updates sent from the clients from interception or manipulation from malicious actors. \n",
    "- Increase the trust in a federation as it guarantees that the client updates are private and not accessible by any other participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79eacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109332c",
   "metadata": {},
   "source": [
    "### Installing Pre-requisties\n",
    "We start by installing OpenFL and dependencies of the workflow interface. These dependencies are exported and become requirements for the Federated Learning Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7475cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "!pip install git+https://github.com/securefederatedai/openfl.git\n",
    "!pip install -r ../../workflow_interface_requirements.txt\n",
    "!pip install pycryptodome\n",
    "!pip install torch==2.4.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install -U ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85485b8",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc94801",
   "metadata": {},
   "source": [
    "We use the quintessential example of a pytorch CNN model trained on MNIST dataset to demonstrate the Secure Aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd8ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "batch_size = 32\n",
    "log_interval = 10\n",
    "\n",
    "# Model definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "# Helper function to validate the model\n",
    "def validate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Helper function to train the model\n",
    "def _train_model(model, optimizer, data_loader):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for _, (X, y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(X)\n",
    "\n",
    "    train_loss /= len(data_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# Helper function to initialize seed for reproducibility\n",
    "def initialize_seed(random_seed=42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475aa38c",
   "metadata": {},
   "source": [
    "### Dataset definition\n",
    "\n",
    "We now download the training and test datasets of MNIST which are used to run the experiment using LocalRuntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9836c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# Train and Test datasets\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"../files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"../files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b246",
   "metadata": {},
   "source": [
    "### Secure Aggregation definitions\n",
    "\n",
    "We will be using [Practical Secure Aggregation for Privacy-Preserving Machine Learning](https://eprint.iacr.org/2017/281.pdf) as a reference for the steps involved in secure aggregation.\n",
    "\n",
    "We will use the secure aggregation utility functions built-in to openfl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a984a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.utilities.secagg import (\n",
    "    create_secret_shares,\n",
    "    reconstruct_secret,\n",
    "    create_ciphertext,\n",
    "    decipher_ciphertext,\n",
    "    generate_agreed_key,\n",
    "    calculate_mask\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def masked_train_model(\n",
    "    model, optimizer, data_loader,\n",
    "    collaborator_index, agreed_keys, private_seed,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to perform training of the model giving masked input\n",
    "    vector instead of loss directly.\n",
    "    \"\"\"\n",
    "    loss = _train_model(model, optimizer, data_loader)\n",
    "    collaborator_mask = calculate_mask(collaborator_index, agreed_keys, private_seed)\n",
    "\n",
    "    return loss + collaborator_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770fe7c",
   "metadata": {},
   "source": [
    "### Workflow definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306f73d",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, placement decorators (`aggregator/collaborator`), and define the `FedAvg` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cf4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "\n",
    "# Helper function for federated averaging\n",
    "def FedAvg(agg_model, models, weights=None):\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    agg_state_dict = agg_model.state_dict()\n",
    "    for key in models[0].state_dict():\n",
    "        agg_state_dict[key] = torch.from_numpy(\n",
    "            np.average([state[key].numpy() for state in state_dicts], axis=0, weights=weights)\n",
    "        )\n",
    "\n",
    "    agg_model.load_state_dict(agg_state_dict)\n",
    "    return agg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d8a60",
   "metadata": {},
   "source": [
    "Let us now define the Workflow.\n",
    "\n",
    "![SecAggFlow](mermaid-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import struct\n",
    "\n",
    "from Crypto.PublicKey import ECC\n",
    "\n",
    "\n",
    "class SecureAggregation_MNIST(FLSpec):\n",
    "    \"\"\"\n",
    "    Federated Flow to train a CNN on MNIST dataset using Secure Aggregation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, learning_rate=1e-2, momentum=0.5, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            initialize_seed()\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.rounds = rounds\n",
    "        self.results = []\n",
    "        self.collaborator_secagg_data = {}\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        This is the start of the Flow.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Workflow .... \")\n",
    "\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.collaborator_count = len(self.collaborators)\n",
    "        self.current_round = 0\n",
    "\n",
    "        self.next(self.generate_keys, foreach=\"collaborators\")\n",
    "\n",
    "    @collaborator\n",
    "    def generate_keys(self):\n",
    "        \"\"\"\n",
    "        Receives the collaborator IDs for all the collaborators.\n",
    "        Shares the public keys of the key pairs generated as private\n",
    "        attributes with the aggregator.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> generating key pairs for secure aggregation...\")\n",
    "        # Generate key pairs for secure aggregation.\n",
    "        # The private attribute self.private_key is a list of 2 private keys of type\n",
    "        # Crypto.PublicKey.ECC.EccKey.\n",
    "        self.public_key = [\n",
    "            ECC.import_key(pvt_key).public_key().export_key(format=\"PEM\")\n",
    "            for pvt_key in self.private_key\n",
    "        ]\n",
    "\n",
    "        self.next(self.generate_collaborator_index)\n",
    "\n",
    "    @aggregator\n",
    "    def generate_collaborator_index(self, inputs):\n",
    "        \"\"\"\n",
    "        Shares the collaborators' public keys with all the collaborators\n",
    "        in the experiment.\n",
    "        \"\"\"\n",
    "        print(f\"<Agg>: Generating unique indices for collaborators...\")\n",
    "        index = 1\n",
    "        # This step is needed as the aggregator also needs to have the\n",
    "        # public keys for all collaborators.\n",
    "        for input in inputs:\n",
    "            self.collaborator_secagg_data[input.input] = {\n",
    "                # Set IDs for the collaborators.\n",
    "                \"index\": index,\n",
    "                \"public_key\": input.public_key\n",
    "            }\n",
    "            index += 1\n",
    "\n",
    "        self.next(self.generate_ciphertexts, foreach=\"collaborators\")\n",
    "\n",
    "    @collaborator\n",
    "    def generate_ciphertexts(self):\n",
    "        \"\"\"\n",
    "        Generates the ciphertexts for all the other collaborators which\n",
    "        includes source collaborator ID, recipient collaborator ID, source\n",
    "        collaborator's private seed share for recipient collaborator, source\n",
    "        collaborator's private key share for recipient collaborator.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> generating ciphertexts for secure aggregation...\")\n",
    "        # Find the index of the current collaborator.\n",
    "        for collab in self.collaborator_secagg_data:\n",
    "            index = self.collaborator_secagg_data[collab][\"index\"]\n",
    "            if self.collaborator_secagg_data[collab][\"public_key\"][0] == self.public_key[0]:\n",
    "                self.index = index\n",
    "                break\n",
    "\n",
    "        self.agreed_key = {}\n",
    "        self.cipher_verification = {}\n",
    "        self.ciphers = {}\n",
    "\n",
    "        # Using Shamir's secret sharing.\n",
    "        seed_shares = create_secret_shares(\n",
    "            # Using private attribute private_seed.\n",
    "            # Converts the floating-point number private_seed into an 8-byte\n",
    "            # binary representation.\n",
    "            struct.pack(\"d\", self.private_seed),\n",
    "            self.collaborator_count,\n",
    "            self.collaborator_count,\n",
    "        )\n",
    "\n",
    "        key_shares = create_secret_shares(\n",
    "            str.encode(ECC.import_key(self.private_key[0]).export_key(format=\"PEM\")),\n",
    "            self.collaborator_count,\n",
    "            self.collaborator_count,\n",
    "        )\n",
    "\n",
    "        for collab in self.collaborator_secagg_data:\n",
    "            self.collaborator_secagg_data[collab][\"ciphertext_from\"] = {}\n",
    "            collab_index = self.collaborator_secagg_data[collab][\"index\"]\n",
    "            # Use the private attribute (private_key) and public keys\n",
    "            # shared in `generate_keys` to generate agreed keys.\n",
    "            self.agreed_key[collab_index] = [\n",
    "                generate_agreed_key(\n",
    "                    self.private_key[0],\n",
    "                    self.collaborator_secagg_data[collab][\"public_key\"][0]\n",
    "                ),\n",
    "                generate_agreed_key(\n",
    "                    self.private_key[1],\n",
    "                    self.collaborator_secagg_data[collab][\"public_key\"][1]\n",
    "                )\n",
    "            ]\n",
    "            # Generate ciphertext for the collaborator.\n",
    "            ciphertext, mac, nonce = create_ciphertext(\n",
    "                self.agreed_key[collab_index][0],   # agreed key\n",
    "                self.index,                         # source collaborator index\n",
    "                collab_index,                       # destination collaborator index\n",
    "                seed_shares[collab_index],          # seed share from source to dest\n",
    "                key_shares[collab_index]            # key share from source to dest\n",
    "            )\n",
    "            # Adding the ciphertext from self.index to collab_index to class\n",
    "            # attribute collaborator_secagg_data such that it can be accessed by other\n",
    "            # collaborators.\n",
    "            self.collaborator_secagg_data[collab][\"ciphertext_from\"][self.index] = [ciphertext, mac, nonce]\n",
    "\n",
    "        self.next(self.filter_ciphertexts)\n",
    "\n",
    "    @aggregator\n",
    "    def filter_ciphertexts(self, inputs):\n",
    "        \"\"\"\n",
    "        Receives ciphertexts from all collaborators and filters them for each collaborator\n",
    "        that they are addressed to.\n",
    "        \"\"\"\n",
    "        for input in inputs:\n",
    "            for collab in input.collaborator_secagg_data:\n",
    "                if \"ciphertext_from\" not in self.collaborator_secagg_data[collab]:\n",
    "                    self.collaborator_secagg_data[collab][\"ciphertext_from\"] = input.collaborator_secagg_data[collab][\"ciphertext_from\"]\n",
    "                else:\n",
    "                    self.collaborator_secagg_data[collab][\"ciphertext_from\"].update(\n",
    "                        input.collaborator_secagg_data[collab][\"ciphertext_from\"]\n",
    "                    )\n",
    "\n",
    "        self.next(self.decrypt_ciphertext, foreach=\"collaborators\")\n",
    "\n",
    "    @collaborator\n",
    "    def decrypt_ciphertext(self):\n",
    "        \"\"\"\n",
    "        Receives the ciphertexts addressed to it and deciphers them.\n",
    "        Shares the deciphered values with the aggregator.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> decrypting ciphertexts for secure aggregation... \")\n",
    "        self.seed_shares = {}\n",
    "        self.key_shares = {}\n",
    "        for collab in self.collaborator_secagg_data:\n",
    "            if self.collaborator_secagg_data[collab][\"index\"] == self.index:\n",
    "                addressed_ciphertexts = self.collaborator_secagg_data[collab].get(\"ciphertext_from\", {})\n",
    "                for source_id in addressed_ciphertexts:\n",
    "                    source_public_key = [\n",
    "                        self.collaborator_secagg_data[collab][\"public_key\"][0] \n",
    "                        if self.collaborator_secagg_data[collab][\"index\"] == source_id \n",
    "                        else None\n",
    "                        for collab in self.collaborator_secagg_data\n",
    "                    ]\n",
    "                    source_public_key = list(filter(None, source_public_key))[0]\n",
    "                    _, _, seed_share, key_share = decipher_ciphertext(\n",
    "                        generate_agreed_key(\n",
    "                            self.private_key[0],\n",
    "                            source_public_key\n",
    "                        ),                                          # agreed_key_1\n",
    "                        addressed_ciphertexts[source_id][0],        # ciphertext\n",
    "                        addressed_ciphertexts[source_id][1],        # mac\n",
    "                        addressed_ciphertexts[source_id][2],        # nonce\n",
    "                    )\n",
    "                    self.seed_shares[source_id] = [self.index, str(seed_share)]\n",
    "                    self.key_shares[source_id] = [self.index, str(key_share)]\n",
    "\n",
    "        self.next(\n",
    "            self.reconstruct,\n",
    "            exclude=[\"agreed_key\", \"cipher_verification\"]\n",
    "        )\n",
    "\n",
    "    @aggregator\n",
    "    def reconstruct(self, inputs):\n",
    "        \"\"\"\n",
    "        Reconstructs the secrets for all the collaborators\n",
    "        which are required for unmasking during aggregation.\n",
    "        \"\"\"\n",
    "        print(f\"<Agg>: Reconstructing secrets for secure aggregation...\")\n",
    "        seed_shares = {}\n",
    "        key_shares = {}\n",
    "        for input in inputs:\n",
    "            # Create a dictionary of shares for each seed.\n",
    "            for source_id in input.seed_shares:\n",
    "                if source_id not in seed_shares:\n",
    "                    seed_shares[source_id] = {}\n",
    "                seed_shares[source_id][input.seed_shares[source_id][0]] = input.seed_shares[source_id][1][2:-1]\n",
    "            # Create a dictionary of shares for each key.\n",
    "            for source_id in input.key_shares:\n",
    "                if source_id not in key_shares:\n",
    "                    key_shares[source_id] = {}\n",
    "                key_shares[source_id][input.key_shares[source_id][0]] = input.key_shares[source_id][1][2:-1]\n",
    "\n",
    "        # Reconstruct the secrets (seeds and private keys) for all source\n",
    "        # collaborators.\n",
    "        self._private_seeds = {}\n",
    "        self._private_keys = {}\n",
    "        print(seed_shares, key_shares)\n",
    "        for source_id in seed_shares:\n",
    "            self._private_seeds[source_id] = struct.unpack(\"d\", reconstruct_secret(seed_shares[source_id]))[0]\n",
    "            self._private_keys[source_id] = reconstruct_secret(key_shares[source_id])\n",
    "\n",
    "        # Generate all agreed keys.\n",
    "        self.agreed_keys = {}\n",
    "        for source in self.collaborator_secagg_data:\n",
    "            source_index = self.collaborator_secagg_data[source][\"index\"]\n",
    "            source_private_key = self._private_keys[source_index]\n",
    "            self.agreed_keys[source_index] = {}\n",
    "            for dest in self.collaborator_secagg_data:\n",
    "                dest_index = self.collaborator_secagg_data[dest][\"index\"]\n",
    "                dest_public_key = self.collaborator_secagg_data[dest][\"public_key\"][0]\n",
    "                self.agreed_keys[source_index][dest_index] = [dest, generate_agreed_key(\n",
    "                    source_private_key,\n",
    "                    dest_public_key\n",
    "                )]\n",
    "\n",
    "        self.next(self.aggregated_model_validation, foreach=\"collaborators\")\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        \"\"\"\n",
    "        Perform validation of aggregated model on collaborators.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> Performing Validation on aggregated model ... \")\n",
    "        self.agg_validation_score = validate(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"<Collab: {self.input}> Aggregated Model validation score = {self.agg_validation_score:.4f}\"\n",
    "        )\n",
    "\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train model on Local collaborator dataset.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}>: Training Model on local dataset ... \")\n",
    "\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "\n",
    "        self.loss = masked_train_model(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            self.train_loader,\n",
    "            self.index,\n",
    "            self.agreed_keys[self.index],\n",
    "            self.private_seed,\n",
    "        )\n",
    "\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        \"\"\"\n",
    "        Validate locally trained model.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> Performing Validation on locally trained model ... \")\n",
    "        self.local_validation_score = validate(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"<Collab: {self.input}> Local model validation score = {self.local_validation_score:.4f}\"\n",
    "        )\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        \"\"\"\n",
    "        Model aggregation step.\n",
    "        \"\"\"\n",
    "        print(f\"<Agg>: Joining models from collaborators...\")\n",
    "\n",
    "        # Average Training loss, aggregated and locally trained model accuracy\n",
    "        total_loss = sum(input.loss for input in inputs)\n",
    "        # Calculate and remove the masks from the total loss.\n",
    "        for input in inputs:\n",
    "            total_loss -= calculate_mask(input.index, self.agreed_keys[input.index], self._private_seeds[input.index])\n",
    "\n",
    "        self.average_loss = total_loss / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs) / len(inputs)\n",
    "\n",
    "        print(f\"Avg. aggregated model validation score = {self.aggregated_model_accuracy:.4f}\")\n",
    "        print(f\"Avg. training loss = {self.average_loss:.4f}\")\n",
    "        print(f\"Avg. local model validation score = {self.local_model_accuracy:.4f}\")\n",
    "\n",
    "        # Average works as the masks on the loss get cancelled out during aggregation.\n",
    "        self.model = FedAvg(self.model, [input.model for input in inputs])\n",
    "\n",
    "        self.results.append(\n",
    "            [\n",
    "                self.current_round,\n",
    "                self.aggregated_model_accuracy,\n",
    "                self.average_loss,\n",
    "                self.local_model_accuracy,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next( self.aggregated_model_validation, foreach=\"collaborators\")\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        This is the last step in the Flow.\n",
    "        \"\"\"\n",
    "        print(f\"This is the end of the flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0757812",
   "metadata": {},
   "source": [
    "### Simulation: LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccffd7",
   "metadata": {},
   "source": [
    "We now import & define the `LocalRuntime`, participants (`Aggregator/Collaborator`), and initialize the private attributes for participants\n",
    "\n",
    "- `Runtime` – Defines where the flow runs. `LocalRuntime` simulates the flow on local node.\n",
    "- `Aggregator/Collaborator` - (Local) Participants in the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "\n",
    "# Setup Aggregator & initialize private attributes\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup Collaborators & initialize shards of MNIST dataset as private attributes\n",
    "n_collaborators = 2\n",
    "collaborator_names = [\"Bengaluru\", \"Portland\"]\n",
    "\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::n_collaborators]\n",
    "    local_train.targets = mnist_train.targets[idx::n_collaborators]\n",
    "    local_test.data = mnist_test.data[idx::n_collaborators]\n",
    "    local_test.targets = mnist_test.targets[idx::n_collaborators]\n",
    "\n",
    "    collaborator.private_attributes = {\n",
    "        \"train_loader\": torch.utils.data.DataLoader(\n",
    "            local_train, batch_size=batch_size, shuffle=False\n",
    "        ),\n",
    "        \"test_loader\": torch.utils.data.DataLoader(\n",
    "            local_test, batch_size=batch_size, shuffle=False\n",
    "        ),\n",
    "        \"private_seed\": random.random(),\n",
    "        \"private_key\": [\n",
    "            ECC.generate(curve=\"ed25519\").export_key(format=\"PEM\"),\n",
    "            ECC.generate(curve=\"ed25519\").export_key(format=\"PEM\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(\n",
    "    aggregator=aggregator, collaborators=collaborators, backend=\"single_process\"\n",
    ")\n",
    "print(f\"Local runtime collaborators = {local_runtime.collaborators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78819357",
   "metadata": {},
   "source": [
    "### Start Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2675ba",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the simulation ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f10d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "flflow = SecureAggregation_MNIST(model, optimizer, learning_rate, momentum, rounds=2, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50300fed",
   "metadata": {},
   "source": [
    "Let us check the simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d77540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "headers = [\"Rounds\", \"Agg Model Validation Score\", \"Local Train loss\", \"Local Model Validation score\"]\n",
    "print('********** Simulation results **********')\n",
    "simulation_results = flflow.results\n",
    "print(tabulate(simulation_results, headers=headers, tablefmt=\"outline\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5371b6d",
   "metadata": {},
   "source": [
    "### Setup Federation: Director & Envoys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270e385",
   "metadata": {},
   "source": [
    "Before we can deploy the experiment, let us create participants in Federation: Director and Envoys. As the Tutorial uses two collaborators we shall launch three participants:\n",
    "1. Director: The central node in the Federation\n",
    "2. Bengaluru: The first envoy in the Federation\n",
    "3. Portland: The second envoy in the Federation \n",
    "\n",
    "The participants can be launched by following steps mentioned in [README]((https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/FederatedRuntime/101_MNIST/README.md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d556d0",
   "metadata": {},
   "source": [
    "### Deploy: FederatedRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd73b6",
   "metadata": {},
   "source": [
    "We now import and instantiate `FederatedRuntime` to enable deployment of experiment on distributed infrastructure. Initializing the `FederatedRuntime` requires following inputs to be provided by the user:\n",
    "\n",
    "- `director_info` – director information including fqdn of the director node, port, and certificate information\n",
    "- `collaborators` - names of the collaborators participating in experiment\n",
    "- `notebook_path`- path to this jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1715a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.experimental.workflow.runtime import FederatedRuntime\n",
    "\n",
    "director_info = {\n",
    "    'director_node_fqdn': 'localhost',\n",
    "    'director_port': 50050,\n",
    "}\n",
    "\n",
    "federated_runtime = FederatedRuntime(\n",
    "    collaborators=collaborator_names,\n",
    "    director=director_info, \n",
    "    notebook_path='./MNIST_SecAgg.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d22bbb",
   "metadata": {},
   "source": [
    "Let us connect to federation & check if the envoys are connected to the director by using the `get_envoys` method of `FederatedRuntime`. If the participants are launched successful in previous step the status of `Bengaluru` and `Portland` should be displayed as `Online`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1be87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_runtime.get_envoys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c487cb",
   "metadata": {},
   "source": [
    "Now that we have our distributed infrastructure ready, let us modify the flow runtime to `FederatedRuntime` instance and deploy the experiment. \n",
    "\n",
    "Progress of the flow is available on \n",
    "1. Jupyter notebook: if `checkpoint` attribute of the flow object is set to `True`\n",
    "2. Director and Envoy terminals  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d19819",
   "metadata": {},
   "outputs": [],
   "source": [
    "flflow.results = [] # clear results from previous run\n",
    "flflow.runtime = federated_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ef3ea",
   "metadata": {},
   "source": [
    "Let us compare the simulation results from `LocalRuntime` and federation results from `FederatedRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Rounds\", \"Agg Model Validation Score\", \"Local Train loss\", \"Local Model Validation score\"]\n",
    "print('********** Simulation results **********')\n",
    "print(tabulate(simulation_results, headers=headers, tablefmt=\"outline\"))\n",
    "\n",
    "print('********** Federation results **********')\n",
    "federation_results = flflow.results\n",
    "print(tabulate(federation_results, headers=headers, tablefmt=\"outline\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
