"""Test script to verify embedding system functionality without making API calls."""

import asyncio
import os
from pathlib import Path
import sys

# Add parent directory to path to import chunkhound modules
sys.path.insert(0, str(Path(__file__).parent))

import json
import os
from pathlib import Path
from typing import Optional

import pytest

from chunkhound.embeddings import EmbeddingManager
from chunkhound.providers.embeddings.openai_provider import OpenAIEmbeddingProvider

from .test_utils import get_api_key_for_tests, should_run_live_api_tests


async def test_official_openai_validation():
    """Test official OpenAI API key validation logic."""
    # Should work: API key provided
    provider = OpenAIEmbeddingProvider(api_key="sk-fake-key")
    assert provider.api_key == "sk-fake-key"
    
    # Should fail: No API key for official OpenAI
    provider = OpenAIEmbeddingProvider()
    with pytest.raises(ValueError, match="OpenAI API key is required for official OpenAI API"):
        await provider._ensure_client()


async def test_custom_endpoint_validation():
    """Test custom endpoint mode allows optional API key."""
    # Should work: Custom endpoint, no API key
    provider = OpenAIEmbeddingProvider(
        base_url="http://localhost:11434", 
        model="nomic-embed-text"
    )
    assert provider.base_url == "http://localhost:11434"
    
    # Should work: Custom endpoint + API key
    provider = OpenAIEmbeddingProvider(
        base_url="http://localhost:1234",
        api_key="custom-key"
    )
    assert provider.api_key == "custom-key"


def test_url_detection_logic():
    """Test the logic that determines official vs custom endpoints."""
    # Official OpenAI URLs (should require API key)
    official_urls = [
        None,
        "https://api.openai.com",
        "https://api.openai.com/v1",
        "https://api.openai.com/v1/",
    ]
    
    for url in official_urls:
        provider = OpenAIEmbeddingProvider(base_url=url)
        is_official = not provider._base_url or (
            provider._base_url.startswith("https://api.openai.com") and 
            (provider._base_url == "https://api.openai.com" or provider._base_url.startswith("https://api.openai.com/"))
        )
        assert is_official, f"URL {url} should be detected as official OpenAI"
    
    # Custom URLs (should NOT require API key)
    custom_urls = [
        "http://localhost:11434",
        "https://api.example.com/v1/embeddings",
        "https://api.openai.com.evil.com/v1",
        "http://api.openai.com/v1",
    ]
    
    for url in custom_urls:
        provider = OpenAIEmbeddingProvider(base_url=url)
        is_official = not provider._base_url or (
            provider._base_url.startswith("https://api.openai.com") and 
            (provider._base_url == "https://api.openai.com" or provider._base_url.startswith("https://api.openai.com/"))
        )
        assert not is_official, f"URL {url} should be detected as custom endpoint"


@pytest.mark.skipif(not should_run_live_api_tests(), 
                   reason="No API key available (set CHUNKHOUND_EMBEDDING__API_KEY or add to .chunkhound.json)")
async def test_real_embedding_api():
    """Test real embedding API call with discovered provider and key."""
    api_key, provider_name = get_api_key_for_tests()
    
    # Create the appropriate provider based on what's configured
    if provider_name == "openai":
        from chunkhound.providers.embeddings.openai_provider import OpenAIEmbeddingProvider
        provider = OpenAIEmbeddingProvider(api_key=api_key, model="text-embedding-3-small")
        expected_dims = 1536
    elif provider_name == "voyageai":
        from chunkhound.providers.embeddings.voyageai_provider import VoyageAIEmbeddingProvider
        provider = VoyageAIEmbeddingProvider(api_key=api_key, model="voyage-3.5")
        expected_dims = 1024  # voyage-3.5 dimensions
    else:
        pytest.skip(f"Unknown provider: {provider_name}")
    
    result = await provider.embed(["Hello, world!"])
    
    assert len(result) == 1
    assert len(result[0]) == expected_dims
    assert all(isinstance(x, float) for x in result[0])


async def test_custom_endpoint_mock_behavior():
    """Test custom endpoint behavior without real server."""
    provider = OpenAIEmbeddingProvider(
        base_url="http://localhost:11434",
        model="nomic-embed-text"
    )
    
    try:
        await provider._ensure_client()
    except Exception as e:
        assert "API key" not in str(e), f"Should not require API key for custom endpoint: {e}"


async def test_ollama_with_reranking_configuration():
    """Test OpenAI provider configured for Ollama with dual-endpoint reranking."""
    # Test configuration using OpenAI provider for Ollama embeddings and separate reranking endpoint
    provider = OpenAIEmbeddingProvider(
        base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
        model="nomic-embed-text",
        api_key="dummy-key-for-custom-endpoint",  # Custom endpoints don't validate API keys
        rerank_model="test-reranker",
        rerank_url="http://localhost:8001/rerank"  # Separate rerank service
    )
    
    # Verify configuration
    assert provider.base_url == "http://localhost:11434/v1"
    assert provider.model == "nomic-embed-text"
    assert provider._rerank_model == "test-reranker"
    assert provider._rerank_url == "http://localhost:8001/rerank"
    
    # Test that reranking is supported when rerank_model is configured
    assert provider.supports_reranking() == True
    
    # Test reranking call (will fail due to no actual service, but tests structure)
    try:
        await provider.rerank("test query", ["doc1", "doc2"])
    except Exception as e:
        # Expected to fail since we don't have actual rerank service running
        # But should not be an API key error
        assert "API key" not in str(e), f"Should not require API key error for reranking: {e}"
        # Should be a connection error since the rerank service isn't running
        assert any(keyword in str(e).lower() for keyword in ["connection", "network", "reranking failed"]), \
            f"Expected connection error for rerank service, got: {e}"


@pytest.mark.skipif(
    not (
        # Check if Ollama is running
        os.system("curl -s http://localhost:11434/api/tags > /dev/null 2>&1") == 0 and
        # Check if rerank service is running  
        os.system("curl -s http://localhost:8001/health > /dev/null 2>&1") == 0
    ),
    reason="Ollama and/or rerank service not running"
)
async def test_ollama_with_live_reranking():
    """Test OpenAI provider configured for Ollama with actual reranking service.
    
    Note: This test requires a real reranking service (e.g., vLLM) and may not work
    with the simple mock server due to HTTP parsing limitations in the mock server.
    """
    # Check if we're using the mock server (which has HTTP parsing issues with httpx)
    import httpx
    try:
        # Use synchronous check since we can't use async in the test setup
        with httpx.Client(timeout=1.0) as client:
            response = client.get("http://localhost:8001/health")
            if response.json().get("service") == "mock-rerank-server":
                # Mock server has issues with httpx requests - skip this test
                pytest.skip("Mock server has HTTP parsing issues with httpx - use vLLM for this test")
    except Exception as e:
        # If we can't check, continue with test
        pass
    
    # This test uses OpenAI provider configured for Ollama embeddings and a separate service for reranking
    # Embeddings come from Ollama (port 11434)
    # Reranking goes to separate service (port 8001)
    provider = OpenAIEmbeddingProvider(
        base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
        model="nomic-embed-text",
        api_key="dummy-key",  # Ollama doesn't require real API key
        rerank_model="test-model",
        rerank_url="http://localhost:8001/rerank"  # Absolute URL to rerank service
    )
    
    # Test that reranking works end-to-end
    test_docs = [
        "def calculate_sum(a, b): return a + b",
        "import numpy as np",
        "class Calculator: pass",
        "function add(x, y) { return x + y; }"
    ]
    
    results = await provider.rerank("python function definition", test_docs, top_k=3)
    
    # Verify results structure
    assert len(results) <= 3, "Should respect top_k limit"
    assert all(hasattr(r, 'index') and hasattr(r, 'score') for r in results), "Results should have index and score"
    assert all(0 <= r.index < len(test_docs) for r in results), "Indices should be valid"
    assert all(isinstance(r.score, float) for r in results), "Scores should be floats"
    
    # Verify we got meaningful results (ranking may vary with embeddings)
    assert len(results) > 0, "Should return results"
    
    print(f"‚úÖ Live reranking test passed:")
    print(f"   ‚Ä¢ Reranked {len(test_docs)} documents")
    print(f"   ‚Ä¢ Top result: '{test_docs[results[0].index][:50]}...' (score: {results[0].score:.3f})")
    print(f"   ‚Ä¢ All results: {[(r.index, f'{r.score:.3f}') for r in results]}")
    print(f"   ‚Ä¢ Document mapping:")
    for i, doc in enumerate(test_docs):
        print(f"     [{i}]: {doc}")
    
    # Check that scores are in descending order
    for i in range(len(results) - 1):
        assert results[i].score >= results[i+1].score, "Results should be ordered by score"


def test_embedding_manager():
    """Test embedding manager functionality."""
    print("\nTesting embedding manager...")

    try:
        manager = EmbeddingManager()

        # Create a mock provider
        provider = OpenAIEmbeddingProvider(
            api_key="sk-test-key-for-testing", model="text-embedding-3-small"
        )

        # Register provider
        manager.register_provider(provider, set_default=True)

        # Test provider retrieval
        retrieved = manager.get_provider()
        assert retrieved.name == "openai"
        assert retrieved.model == "text-embedding-3-small"

        # Test provider listing
        providers = manager.list_providers()
        assert "openai" in providers

        print("‚úÖ Embedding manager tests passed:")
        print(f"   ‚Ä¢ Registered providers: {providers}")
        print(f"   ‚Ä¢ Default provider: {retrieved.name}/{retrieved.model}")

    except Exception as e:
        print(f"‚ùå Embedding manager test failed: {e}")
        assert False, f"Embedding manager test failed: {e}"


async def test_mock_embedding_generation():
    """Test embedding generation with mock data (no API call)."""
    print("\nTesting mock embedding generation...")

    try:
        # This will fail with API call, but we can test the structure
        provider = OpenAIEmbeddingProvider(
            api_key="sk-test-key-for-testing", model="text-embedding-3-small"
        )

        # Test input validation
        empty_result = await provider.embed([])
        assert empty_result == []
        print("‚úÖ Empty input handling works")

        # Test with actual text (this will fail due to fake API key, but that's expected)
        try:
            result = await provider.embed(["def hello(): pass"])
            print(f"‚ùå Unexpected success - should have failed with fake API key")
        except Exception as e:
            print(f"‚úÖ Expected API failure with fake key: {type(e).__name__}")

        return True

    except Exception as e:
        print(f"‚ùå Mock embedding test failed: {e}")
        return False






def test_provider_integration():
    """Test integration of all providers with EmbeddingManager."""
    print("\nTesting provider integration with EmbeddingManager...")

    try:
        manager = EmbeddingManager()

        # Register OpenAI provider
        openai_provider = OpenAIEmbeddingProvider(
            api_key="sk-test-key", model="text-embedding-3-small"
        )
        manager.register_provider(openai_provider)



        # Test provider listing
        providers = manager.list_providers()
        expected_providers = {"openai"}
        assert expected_providers.issubset(set(providers))


        # Test specific provider retrieval
        openai_retrieved = manager.get_provider("openai")
        assert openai_retrieved.name == "openai"


        print(f"‚úÖ Provider integration successful:")
        print(f"   ‚Ä¢ Registered providers: {providers}")
        print(f"   ‚Ä¢ Can retrieve by name: ‚úì")

    except Exception as e:
        print(f"‚ùå Provider integration test failed: {e}")
        assert False, f"Provider integration failed: {e}"


def test_environment_variable_handling():
    """Test environment variable handling."""
    print("\nTesting environment variable handling...")

    # Save original env vars
    original_key = os.getenv("OPENAI_API_KEY")
    original_url = os.getenv("OPENAI_BASE_URL")

    try:
        # Test with env vars
        os.environ["OPENAI_API_KEY"] = "sk-test-env-key"
        os.environ["OPENAI_BASE_URL"] = "https://test.example.com"

        provider = OpenAIEmbeddingProvider()
        print("‚úÖ Environment variable loading works")

        # Test missing API key
        del os.environ["OPENAI_API_KEY"]
        try:
            provider = OpenAIEmbeddingProvider()
            print("‚ùå Should have failed with missing API key")
        except ValueError as e:
            print("‚úÖ Correctly handles missing API key")

    except Exception as e:
        print(f"‚ùå Environment test failed: {e}")

    finally:
        # Restore original env vars
        if original_key:
            os.environ["OPENAI_API_KEY"] = original_key
        elif "OPENAI_API_KEY" in os.environ:
            del os.environ["OPENAI_API_KEY"]

        if original_url:
            os.environ["OPENAI_BASE_URL"] = original_url
        elif "OPENAI_BASE_URL" in os.environ:
            del os.environ["OPENAI_BASE_URL"]


async def main():
    """Run all tests."""
    print("ChunkHound Embedding System Tests")
    print("=" * 40)

    # Test provider creation
    provider = await test_openai_provider_creation()


    # Test embedding manager
    manager = test_embedding_manager()

    # Test provider integration
    test_provider_integration()

    # Test mock embedding generation
    await test_mock_embedding_generation()

    # Test environment variables
    test_environment_variable_handling()

    print("\n" + "=" * 40)
    print("Test summary:")
    print("‚úÖ OpenAI provider creation")
    print("‚úÖ Embedding manager functionality")
    print("‚úÖ Provider integration")
    print("‚úÖ Mock embedding generation")
    print("‚úÖ Environment variable handling")
    print("\nAll core embedding functionality verified!")
    print("\nTo test with real API calls, set OPENAI_API_KEY and run:")
    print(
        'python -c "import asyncio; from test_embeddings import test_real_api; asyncio.run(test_real_api())"'
    )


async def test_real_api():
    """Test with real embedding API (requires valid API key)."""
    # Get API key from generic test function
    api_key, provider_name = get_api_key_for_tests()

    if not api_key:
        print("‚è≠Ô∏è  Skipping real API tests - no API key found")
        print("To run real API tests: set CHUNKHOUND_EMBEDDING__API_KEY or configure .chunkhound.json")
        return True  # Return success to not break test suite

    print("\n" + "=" * 50)
    print(f"üöÄ COMPREHENSIVE REAL API TESTING ({provider_name.upper()})")
    print("=" * 50)

    try:
        # Test 1: Basic embedding generation
        print("\n1. Testing basic embedding generation...")
        
        # Create the appropriate provider
        if provider_name == "openai":
            from chunkhound.providers.embeddings.openai_provider import OpenAIEmbeddingProvider
            provider = OpenAIEmbeddingProvider(api_key=api_key)
        elif provider_name == "voyageai":
            from chunkhound.providers.embeddings.voyageai_provider import VoyageAIEmbeddingProvider
            provider = VoyageAIEmbeddingProvider(api_key=api_key, model="voyage-3.5")
        else:
            print(f"‚ùå Unknown provider: {provider_name}")
            return False

        test_texts = [
            "def hello(): return 'world'",
            "class Database: pass",
            "async def search(query: str) -> List[str]:",
        ]

        result = await provider.embed(test_texts)

        print(f"‚úÖ Basic embedding test successful:")
        print(f"   ‚Ä¢ Generated {len(result)} embeddings")
        print(f"   ‚Ä¢ Vector dimensions: {len(result[0])}")
        print(f"   ‚Ä¢ Model: {provider.model}")
        print(f"   ‚Ä¢ Provider: {provider.name}")

        # Test 2: Alternative model (if available)
        if provider_name == "openai":
            print("\n2. Testing with text-embedding-3-large...")
            alt_provider = OpenAIEmbeddingProvider(
                api_key=api_key, model="text-embedding-3-large"
            )
            alt_result = await alt_provider.embed(["def test(): pass"])
            print(f"‚úÖ Alternative model test successful:")
            print(f"   ‚Ä¢ Model: {alt_provider.model}")
            print(f"   ‚Ä¢ Dimensions: {len(alt_result[0])}")
        elif provider_name == "voyageai":
            print("\n2. Testing with voyage-3-large...")
            alt_provider = VoyageAIEmbeddingProvider(
                api_key=api_key, model="voyage-3-large"
            )
            alt_result = await alt_provider.embed(["def test(): pass"])
            print(f"‚úÖ Alternative model test successful:")
            print(f"   ‚Ä¢ Model: {alt_provider.model}")
            print(f"   ‚Ä¢ Dimensions: {len(alt_result[0])}")

        # Test 3: Batch processing
        print("\n3. Testing batch processing...")
        batch_texts = [f"def function_{i}(): return {i}" for i in range(10)]

        batch_result = await provider.embed(batch_texts)
        print(f"‚úÖ Batch processing test successful:")
        print(f"   ‚Ä¢ Processed {len(batch_result)} texts in batch")
        print(f"   ‚Ä¢ All vectors have {len(batch_result[0])} dimensions")

        # Test 4: Integration with EmbeddingManager
        print("\n4. Testing EmbeddingManager integration...")
        manager = EmbeddingManager()
        manager.register_provider(provider, set_default=True)

        manager_result = await manager.embed_texts(
            ["import asyncio", "from typing import List, Optional"]
        )

        print(f"‚úÖ EmbeddingManager integration successful:")
        print(f"   ‚Ä¢ Generated {len(manager_result.embeddings)} embeddings via manager")
        print(f"   ‚Ä¢ Each vector: {len(manager_result.embeddings[0])} dimensions")
        print(f"   ‚Ä¢ Using provider: {manager.get_provider().name}")
        print(f"   ‚Ä¢ Result model: {manager_result.model}")
        print(f"   ‚Ä¢ Result provider: {manager_result.provider}")

        # Test 5: Vector similarity check
        print("\n5. Testing vector similarity (semantic relationship)...")
        similar_texts = [
            "async def process_file():",
            "async def handle_file():",
            "def synchronous_function():",
        ]

        similar_results = await provider.embed(similar_texts)

        # Calculate cosine similarity between first two (should be higher)
        import math

        def cosine_similarity(a, b):
            dot_product = sum(x * y for x, y in zip(a, b))
            magnitude_a = math.sqrt(sum(x * x for x in a))
            magnitude_b = math.sqrt(sum(x * x for x in b))
            return dot_product / (magnitude_a * magnitude_b)

        sim_async = cosine_similarity(similar_results[0], similar_results[1])
        sim_mixed = cosine_similarity(similar_results[0], similar_results[2])

        print(f"‚úÖ Semantic similarity test:")
        print(f"   ‚Ä¢ Async function similarity: {sim_async:.4f}")
        print(f"   ‚Ä¢ Mixed function similarity: {sim_mixed:.4f}")
        print(f"   ‚Ä¢ Semantic relationship detected: {sim_async > sim_mixed}")

        print("\n" + "üéâ" * 15)
        print("ALL REAL API TESTS PASSED!")
        print("üéâ" * 15)
        print(f"\nSummary:")
        print(f"‚úÖ Basic embedding generation working")
        print(f"‚úÖ Multiple model support")
        print(f"‚úÖ Batch processing functional")
        print(f"‚úÖ EmbeddingManager integration complete")
        print(f"‚úÖ Semantic relationships captured in vectors")
        print(f"‚úÖ Ready for production use with real embeddings!")

        return True

    except Exception as e:
        print(f"‚ùå Real API test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    asyncio.run(main())
