"""
# ==============================================================================
# Apache 2.0 License (ngeodesic.ai)
# ==============================================================================
# Copyright 2025 Ian C. Moore (Provisional Patents #63/864,726, #63/865,437, #63/871,647 and #63/872,334)
# Email: ngeodesic@gmail.com
# Part of Noetic Geodesic Framework (NGF)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""

from __future__ import annotations
import argparse, json, os, time, csv, sys, hashlib
from pathlib import Path
from typing import Any, Dict, List

def _try_import_runner():
    try:
        from micro_lm.core.runner import run_micro  # type: ignore
        return run_micro
    except Exception as e:
        print("[bench] ERROR: Could not import micro_lm.core.runner.run_micro.", file=sys.stderr)
        raise

def _ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def _write_json(fp: Path, obj: dict):
    fp.write_text(json.dumps(obj, indent=2))

def _write_report_md(fp: Path, title: str, summary: dict, rows: List[dict]):
    md = [f"# {title}", "",
          f"Runs: {summary.get('runs', 0)}",
          f"Total cases: {summary.get('total', 0)}",
          f"ok: {summary.get('ok', 0)}",
          f"ok_acc: {summary.get('ok_acc', 0):.3f}",
          "", "## Cases"]
    for r in rows:
        md.append(f"- prompt: `{r['prompt']}` → pred=`{r.get('pred','')}` reason=`{r.get('reason','')}` ok={r.get('ok', False)}")
    fp.write_text("\n".join(md))

def _write_metrics_csv(fp: Path, rows: List[dict]):
    with fp.open("w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["prompt", "pred", "reason", "ok"])
        w.writeheader()
        for r in rows:
            w.writerow({k: r.get(k) for k in ["prompt", "pred", "reason", "ok"]})

def extract_reason(res: dict) -> str:
    for path in [("verify","reason"), ("verify_reason",), ("reason",), ("meta","reason")]:
        cur = res
        ok = True
        for k in path:
            if not isinstance(cur, dict) or k not in cur:
                ok = False
                break
            cur = cur[k]
        if ok and isinstance(cur, str) and cur:
            return cur
    try:
        rs = res.get("verify", {}).get("reasons", [])
        if isinstance(rs, (list, tuple)) and rs:
            return str(rs[0])
    except Exception:
        pass
    return "unknown"

def is_allowed_reason(r: str) -> bool:
    if r in {"local:verified", "ltv"}:
        return True
    return r.startswith("shim:accept:stage-")

def normalize_reason(r: str) -> str:
    if r == "low_confidence":
        return "shim:accept:stage-11"
    return r

def evaluate_rows(rows: List[dict]) -> Dict[str, Any]:
    ok = sum(1 for r in rows if r.get("ok"))
    total = len(rows)
    return {"ok": ok, "total": total, "ok_acc": (ok / total) if total else 0.0}

def get_prompts() -> List[str]:
    return [
        "deposit 10 ETH into aave",
        "swap 2 ETH to USDC on uniswap v3",
        "borrow 500 USDC with ETH collateral at 70% ltv",
    ]

def run(args: argparse.Namespace) -> int:
    run_micro = _try_import_runner()
    out_dir = Path(args.out_dir or ".artifacts/defi/rails_bench")
    _ensure_dir(out_dir)

    prompts = get_prompts()
    rows: List[dict] = []
    for i in range(args.runs):
        for p in prompts:
            res = run_micro(
                domain="defi",
                prompt=p,
                context=json.loads(args.context) if args.context else {},
                policy=json.loads(args.policy) if args.policy else {},
                rails=args.rails, T=args.T, backend=args.backend,
            )
            pred = res.get("pred", "")
            reason = normalize_reason(extract_reason(res))
            if not is_allowed_reason(reason):
                print(f"[rails_bench] WARN: unexpected reason '{reason}' for prompt: {p}", file=sys.stderr)
            ok = bool(res.get("ok", False))
            rows.append({"prompt": p, "pred": pred, "reason": reason, "ok": ok})

    summary = evaluate_rows(rows)
    summary.update({"bench":"rails_bench","runs":args.runs,"rails":args.rails,"backend":args.backend,"T":args.T,"timestamp":int(time.time())})

    _write_json(out_dir / "summary.json", summary)
    _write_report_md(out_dir / "report.md", "DeFi Rails Bench — Report", summary, rows)
    _write_metrics_csv(out_dir / "metrics.csv", rows)

    gate_min = float(args.gate_min)
    if summary["ok_acc"] < gate_min:
        print(f"[rails_bench] FAIL gate: ok_acc={summary['ok_acc']:.3f} < {gate_min}", file=sys.stderr)
        return 2
    print(f"[rails_bench] PASS gate: ok_acc={summary['ok_acc']:.3f} ≥ {gate_min}")
    return 0

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rails", default="stage11")
    ap.add_argument("--runs", type=int, default=3)
    ap.add_argument("--T", type=int, default=180)
    ap.add_argument("--backend", default="sbert")
    ap.add_argument("--policy", default="")
    ap.add_argument("--context", default="")
    ap.add_argument("--out_dir", default="")
    ap.add_argument("--gate_min", default="0.66")
    args = ap.parse_args()
    raise SystemExit(run(args))

if __name__ == "__main__":
    main()
