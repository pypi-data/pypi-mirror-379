{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cbf344",
   "metadata": {},
   "source": [
    "# CNN model\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e47803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/kaggle/ariel-data-challenge\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import types\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Project imports\n",
    "from ariel_data_preprocessing.data_generator_functions import make_training_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc846628",
   "metadata": {},
   "source": [
    "## 1. Initialize data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e59990",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 274\n",
    "smoothing_window = 160\n",
    "\n",
    "training_dataset, validation_dataset, evaluation_dataset = make_training_datasets(\n",
    "    #data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train-1100_smoothing-10-20-40-80-160.h5',\n",
    "    sample_size=sample_size,\n",
    "    smoothing_window=smoothing_window,\n",
    "    standardize_wavelengths=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d586e99",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(\n",
    "        sample_size: int,\n",
    "        **hyperparameters\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    '''Builds the convolutional neural network regression model'''\n",
    "\n",
    "    hyperparameters = types.SimpleNamespace(**hyperparameters)\n",
    "\n",
    "    # Set-up the L1L2 for the dense layers\n",
    "    regularizer = tf.keras.regularizers.L1L2(\n",
    "        l1=hyperparameters.l1,\n",
    "        l2=hyperparameters.l2\n",
    "    )\n",
    "\n",
    "    # Define the model layers in order\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input((sample_size,283,1)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hyperparameters.first_filter_set,\n",
    "            hyperparameters.first_filter_size,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_uniform',\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hyperparameters.second_filter_set,\n",
    "            hyperparameters.second_filter_size,\n",
    "            kernel_initializer='he_uniform',\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hyperparameters.third_filter_set,\n",
    "            hyperparameters.third_filter_size,\n",
    "            kernel_initializer='he_uniform',\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hyperparameters.fourth_filter_set,\n",
    "            hyperparameters.fourth_filter_size,\n",
    "            kernel_initializer='he_uniform',\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            hyperparameters.dense_units,\n",
    "            kernel_regularizer=regularizer,\n",
    "            kernel_initializer='he_uniform',\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            283,\n",
    "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "            bias_initializer=tf.keras.initializers.Constant(0.014689),\n",
    "            activation='linear'\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=hyperparameters.learning_rate,\n",
    "        beta_1=hyperparameters.beta_one,\n",
    "        beta_2=hyperparameters.beta_two,\n",
    "        amsgrad=hyperparameters.amsgrad,\n",
    "        weight_decay=hyperparameters.weight_decay,\n",
    "        use_ema=hyperparameters.use_ema\n",
    "    )\n",
    "\n",
    "    # Compile the model, specifying the type of loss to use during training and any extra\n",
    "    # metrics to evaluate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.MeanSquaredError(name='MSE'),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='RMSE')]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89a4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 274, 283, 52)      260       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 137, 141, 52)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 137, 141, 60)      78060     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 68, 70, 60)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 68, 70, 47)        70547     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 34, 35, 47)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 34, 35, 47)        79571     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 17, 17, 47)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 13583)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                434688    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 283)               9339      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 672,465\n",
      "Trainable params: 672,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'learning_rate': 0.00032893709671884643,\n",
    "    'l1': 0.5023691865516101,\n",
    "    'l2': 0.722432138551213,\n",
    "    'first_filter_set': 52,\n",
    "    'second_filter_set': 60,\n",
    "    'third_filter_set': 47,\n",
    "    'fourth_filter_set': 47,\n",
    "    'first_filter_size': 2,\n",
    "    'second_filter_size': 5,\n",
    "    'third_filter_size': 5,\n",
    "    'fourth_filter_size': 6,\n",
    "    'dense_units': 32,\n",
    "    'beta_one': 0.5394897637095215,\n",
    "    'beta_two': 0.6238511299185701,\n",
    "    'amsgrad': False,\n",
    "    'weight_decay': 0.016349602205981664,\n",
    "    'use_ema': False\n",
    "}\n",
    "\n",
    "model = compile_model(\n",
    "    sample_size=sample_size,\n",
    "    **hyperparameters\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83900f61",
   "metadata": {},
   "source": [
    "## 4. Single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 110 ksteps\n",
      "Epoch 1/500\n",
      "24/55 [============>.................] - ETA: 6s - loss: 1600.2778 - RMSE: 0.1906"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "batch_size = 4\n",
    "steps = 55\n",
    "\n",
    "# Note: it takes 5 epochs to see every planet in the training set once\n",
    "\n",
    "total_ksteps = (steps * epochs * batch_size) // 1000\n",
    "n_params = np.sum([tf.keras.backend.count_params(p) for p in model.trainable_weights]) / 1000\n",
    "\n",
    "#model_save_file = f'{config.MODELS_DIRECTORY}/ariel_cnn-{n_params}k-{total_ksteps}ksteps-tf2.11.keras'\n",
    "\n",
    "if Path(model_save_file).exists() and Path(training_results_save_file).exists():\n",
    "\n",
    "    print(f'Found existing model for {total_ksteps} ksteps, skipping training.')\n",
    "\n",
    "    # Load the existing model\n",
    "    model = tf.keras.models.load_model(model_save_file)\n",
    "\n",
    "    # Load existing training results\n",
    "    with open(training_results_save_file, 'rb') as input_file:\n",
    "        training_results = pickle.load(input_file)\n",
    "\n",
    "else:\n",
    "\n",
    "  print(f'Training model for {total_ksteps} ksteps')\n",
    "\n",
    "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=model_save_file,   # Path to save the best model\n",
    "      monitor='val_RMSE',         # Metric to monitor (e.g., 'val_accuracy', 'loss')\n",
    "      save_best_only=True,        # Only save the model if the monitored metric improves\n",
    "      mode='min',                 # 'min' for metrics like loss (lower is better), 'max' for accuracy\n",
    "      verbose=1                   # Display messages when a new best model is saved\n",
    "  )\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  training_results = model.fit(\n",
    "    training_dataset.batch(batch_size),\n",
    "    validation_data=validation_dataset.batch(batch_size),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps,\n",
    "    validation_steps=100 // batch_size,  # Evaluate on 100 planets,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    "  )\n",
    "\n",
    "  print(f'Training complete in {(time.time() - start_time)/60:.1f} minutes')\n",
    "  model.save(model_save_file)\n",
    "\n",
    "  with open(training_results_save_file, 'wb') as output_file:\n",
    "      pickle.dump(training_results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('CNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation loss\n",
    "axs[0].set_title('Training loss (mean squared error)')\n",
    "axs[0].plot(np.array(training_results.history['loss']), alpha=0.5, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_loss']), alpha=0.5, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "# axs[0].set_ylim(21, 25)\n",
    "# axs[0].set_yscale('log')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "# Plot training and validation RMSE\n",
    "axs[1].set_title('Root mean squared error')\n",
    "axs[1].plot(training_results.history['RMSE'], alpha=0.5, label='Training')\n",
    "axs[1].plot(training_results.history['val_RMSE'], alpha=0.5, label='Validation')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('RMSE')\n",
    "# axs[2].set_ylim(top=0.014)\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f'{figures_dir}/03.2.4-ariel_cnn_training_curves_8.4M-{total_ksteps}ksteps.jpg',\n",
    "    dpi=config.STD_FIG_DPI,\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc5f8e",
   "metadata": {},
   "source": [
    "### 4.1. Upload model to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets metadata gperdrizet/ariel-cnn -p ./model\n",
    "\n",
    "with open('model/dataset-metadata.json', 'r') as f:\n",
    "    data = json.loads(json.load(f))\n",
    "\n",
    "data['id'] = 'gperdrizet/ariel-cnn'\n",
    "\n",
    "with open('model/dataset-metadata.json', \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "!kaggle datasets version -p ./model/ -m \"Updated ariel CNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e0ef4",
   "metadata": {},
   "source": [
    "## 5. Model evaluation (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ea55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "planets = 550\n",
    "\n",
    "evaluation_data = evaluation_dataset.take(planets)\n",
    "\n",
    "signals = np.array([element[0].numpy() for element in evaluation_data])\n",
    "spectra = np.array([element[1].numpy() for element in evaluation_data])\n",
    "\n",
    "print(f'Signals shape: {signals.shape}')\n",
    "print(f'Spectra shape: {spectra.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543fbef",
   "metadata": {},
   "source": [
    "### 3.2. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_file = '/kaggle/working/predictions'\n",
    "\n",
    "if Path(predictions_file).is_file():\n",
    "    spectrum_predictions = np.load(predictions_file)\n",
    "\n",
    "else:\n",
    "    spectrum_predictions = []\n",
    "    \n",
    "    for planet in signals:\n",
    "        spectrum_predictions.append(model.predict(planet, batch_size=10, verbose=0))\n",
    "    \n",
    "    spectrum_predictions = np.array(spectrum_predictions)\n",
    "    np.save(predictions_file, spectrum_predictions)\n",
    "\n",
    "spectrum_predictions_avg = np.mean(spectrum_predictions, axis=1)\n",
    "spectrum_predictions_std = np.std(spectrum_predictions, axis=1)\n",
    "reference_spectra = spectra[:,0,:]\n",
    "\n",
    "print(f'Spectrum predictions shape: {spectrum_predictions.shape}')\n",
    "print(f'Spectrum predictions avg shape: {spectrum_predictions_avg.shape}')\n",
    "print(f'Spectrum predictions std shape: {spectrum_predictions_std.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461785aa",
   "metadata": {},
   "source": [
    "### 3.3. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('Spectral prediction evaluation')\n",
    "\n",
    "axs[0].set_title('Predicted vs true spectral signals')\n",
    "axs[0].scatter(\n",
    "    spectra.flatten(),\n",
    "    spectrum_predictions.flatten(),\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    color='black',\n",
    "    label='Sample predictions'\n",
    ")\n",
    "\n",
    "axs[0].scatter(\n",
    "    reference_spectra.flatten(),\n",
    "    spectrum_predictions_avg.flatten(),\n",
    "    s=2.5,\n",
    "    alpha=0.5,\n",
    "    color='red', \n",
    "    label='Averaged prediction'\n",
    ")\n",
    "\n",
    "axs[0].set_xlim(0,0.1)\n",
    "axs[0].set_ylim(0,0.1)\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_xlabel('True spectral signal')\n",
    "axs[0].set_ylabel('Predicted spectral signal')\n",
    "axs[0].legend(loc='best', markerscale=2)\n",
    "\n",
    "residuals = spectrum_predictions.flatten() - spectra.flatten()\n",
    "avg_residual = spectrum_predictions_avg.flatten() - reference_spectra.flatten()\n",
    "\n",
    "axs[1].set_title('Prediction residuals')\n",
    "axs[1].scatter(\n",
    "    spectra,\n",
    "    residuals,\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    color='black',\n",
    "    label='Sample predictions'\n",
    ")\n",
    "\n",
    "axs[1].scatter(\n",
    "    reference_spectra,\n",
    "    avg_residual,\n",
    "    s=2.5,\n",
    "    alpha=0.5,\n",
    "    color='red',\n",
    "    label='Averaged prediction'\n",
    ")\n",
    "\n",
    "axs[1].set_xlabel('True spectral signal')\n",
    "axs[1].set_ylabel('Prediction residuals')\n",
    "\n",
    "axs[2].set_title('Mean fit residual vs sample sigma')\n",
    "axs[2].scatter(avg_residual, spectrum_predictions_std.flatten(), color='black', alpha=0.5, s=2.5)\n",
    "axs[2].set_xlabel('Fit residual')\n",
    "axs[2].set_ylabel('Standard deviation')\n",
    "\n",
    "axs[3].set_title('Standard deviation of predictions')\n",
    "axs[3].hist(spectrum_predictions_std.flatten(), bins=100, color='black')\n",
    "axs[3].xaxis.set_major_formatter(FormatStrFormatter('%.2e'))\n",
    "axs[3].tick_params(axis='x', labelrotation=45)\n",
    "axs[3].set_xlabel('Standard deviation')\n",
    "axs[3].set_ylabel('Counts')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
