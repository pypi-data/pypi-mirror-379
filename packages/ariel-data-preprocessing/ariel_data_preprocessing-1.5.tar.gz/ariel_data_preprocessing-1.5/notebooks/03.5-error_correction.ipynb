{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25f31e5",
   "metadata": {},
   "source": [
    "# Error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542efaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Project imports\n",
    "import configuration as config\n",
    "from ariel_data_preprocessing.data_generator_functions import make_training_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1f8a2",
   "metadata": {},
   "source": [
    "## 2. Initialize data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0da45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 372\n",
    "n_samples = 10\n",
    "\n",
    "_, _, evaluation_dataset = make_training_datasets(\n",
    "    data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train_no_smoothing.h5',\n",
    "    sample_size=sample_size,\n",
    "    n_samples=n_samples,\n",
    "    validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf417c",
   "metadata": {},
   "source": [
    "## 3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = evaluation_dataset.take(550)\n",
    "signals = np.array([element[0].numpy() for element in evaluation_data])\n",
    "spectra = np.array([element[1].numpy() for element in evaluation_data])\n",
    "\n",
    "print(f'Signals shape: {signals.shape}')\n",
    "print(f'Spectra shape: {spectra.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd094c2b",
   "metadata": {},
   "source": [
    "## 4. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6427115",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'{config.MODELS_DIRECTORY}/ariel-cnn-8.1M-43ksteps-tf2.11.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f00a63",
   "metadata": {},
   "source": [
    "## 5. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea05913",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{config.EXPERIMENT_RESULTS_DIRECTORY}/error_correction').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "predictions_file = f'{config.EXPERIMENT_RESULTS_DIRECTORY}/error_correction/predictions'\n",
    "\n",
    "if Path(predictions_file).is_file():\n",
    "    spectrum_predictions = np.load(predictions_file)\n",
    "\n",
    "else:\n",
    "    spectrum_predictions = []\n",
    "    \n",
    "    for planet in signals:\n",
    "        spectrum_predictions.append(model.predict(planet, batch_size=10, verbose=0))\n",
    "    \n",
    "    spectrum_predictions = np.array(spectrum_predictions)\n",
    "    np.save(predictions_file, spectrum_predictions)\n",
    "\n",
    "spectrum_predictions_avg = np.mean(spectrum_predictions, axis=1)\n",
    "spectrum_predictions_std = np.std(spectrum_predictions, axis=1)\n",
    "reference_spectra = spectra[:,0,:]\n",
    "\n",
    "print(f'Spectrum predictions shape: {spectrum_predictions.shape}')\n",
    "print(f'Spectrum predictions avg shape: {spectrum_predictions_avg.shape}')\n",
    "print(f'Spectrum predictions std shape: {spectrum_predictions_std.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c65121",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('Spectral prediction evaluation')\n",
    "\n",
    "axs[0].set_title('Predicted vs true spectral signals')\n",
    "axs[0].scatter(\n",
    "    spectra.flatten(),\n",
    "    spectrum_predictions.flatten(),\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    color='black',\n",
    "    label='Sample predictions'\n",
    ")\n",
    "\n",
    "axs[0].scatter(\n",
    "    reference_spectra.flatten(),\n",
    "    spectrum_predictions_avg.flatten(),\n",
    "    s=2.5,\n",
    "    alpha=0.5,\n",
    "    color='red', \n",
    "    label='Averaged prediction'\n",
    ")\n",
    "\n",
    "axs[0].set_xlim(0,0.1)\n",
    "axs[0].set_ylim(0,0.1)\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_xlabel('True spectral signal')\n",
    "axs[0].set_ylabel('Predicted spectral signal')\n",
    "axs[0].legend(loc='best', markerscale=2)\n",
    "\n",
    "residuals = spectrum_predictions.flatten() - spectra.flatten()\n",
    "avg_residual = spectrum_predictions_avg.flatten() - reference_spectra.flatten()\n",
    "\n",
    "axs[1].set_title('Prediction residuals')\n",
    "axs[1].scatter(\n",
    "    spectra,\n",
    "    residuals,\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    color='black',\n",
    "    label='Sample predictions'\n",
    ")\n",
    "\n",
    "axs[1].scatter(\n",
    "    reference_spectra,\n",
    "    avg_residual,\n",
    "    s=2.5,\n",
    "    alpha=0.5,\n",
    "    color='red',\n",
    "    label='Averaged prediction'\n",
    ")\n",
    "\n",
    "axs[1].set_xlabel('True spectral signal')\n",
    "axs[1].set_ylabel('Prediction residuals')\n",
    "\n",
    "axs[2].set_title('Mean fit residual vs sample sigma')\n",
    "axs[2].scatter(avg_residual, spectrum_predictions_std.flatten(), color='black', alpha=0.5, s=2.5)\n",
    "axs[2].set_xlabel('Fit residual')\n",
    "axs[2].set_ylabel('Standard deviation')\n",
    "\n",
    "axs[3].set_title('Standard deviation of predictions')\n",
    "axs[3].hist(spectrum_predictions_std.flatten(), bins=100, color='black')\n",
    "axs[3].xaxis.set_major_formatter(FormatStrFormatter('%.2e'))\n",
    "axs[3].tick_params(axis='x', labelrotation=45)\n",
    "axs[3].set_xlabel('Standard deviation')\n",
    "axs[3].set_ylabel('Counts')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253b242",
   "metadata": {},
   "source": [
    "Still seem to have that weird problem, where true vs predicted signal looks OK-ish - mostly kind of diagonal. But, for some reason the predictions seem like they are capped at 0.06, while the true range of the data is up to ~ 0.08.\n",
    "\n",
    "That aside for the moment, I think the more pressing issue is that our standard deviations are way too tight. Even if our predictions are pretty good (which I doubt, but for the sake of argument..) setting the error too low will tank the score. The scoring function finds the probability that the true value was drawn from a gaussian with our mean and sigma, like this:\n",
    "\n",
    "```python\n",
    "scipy.stats.norm.logpdf(true_spectrum, loc=our_predictions, scale=our_errors)\n",
    "```\n",
    "\n",
    "Two approaches to do a better job with the errors come to mind:\n",
    "\n",
    "1. Figure out a way to use the observed signal to get a wavelength & signal dependent error for the instrument\n",
    "2. Scale the prediction ensemble error until it encompasses the true value\n",
    "\n",
    "The first approach is probably the more scientifically correct way to attack the problem - but I'm going to go with option 2. Seems like the easiest way to get validated error values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
