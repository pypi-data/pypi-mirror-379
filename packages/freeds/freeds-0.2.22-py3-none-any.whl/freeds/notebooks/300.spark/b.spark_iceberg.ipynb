{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Self check to write a small delta table and then read it from a new spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"area\": \"spark\",\n",
    "    \"description\": \"Check that spark can use iceberg tables.\",\n",
    "    \"passed\": False,\n",
    "    \"message\": \"\",\n",
    "    \"plugin\": \"spark\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/24 18:37:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'app-20250924183423-0001')\n",
      "('spark.app.name', 'self-check1')\n",
      "('spark.app.startTime', '1758738862975')\n",
      "('spark.app.submitTime', '1758738862833')\n",
      "('spark.driver.extraClassPath', '/opt/freeds/spark/jars/*')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.host', '0af737f2f7db')\n",
      "('spark.driver.port', '32851')\n",
      "('spark.executor.extraClassPath', '/opt/freeds/spark/jars/*')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.executor.memory', '2g')\n",
      "('spark.hadoop.fs.s3a.access.key', 'kbmB5u0xCQs97d0vJxjx')\n",
      "('spark.hadoop.fs.s3a.endpoint', 'http://s3-minio:9900')\n",
      "('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
      "('spark.hadoop.fs.s3a.secret.key', 'Tjq8PW7h0GU7rhiMrNMRrvCfpOhfIgvV8dwEhyIJ')\n",
      "('spark.master', 'spark://spark-master:7077')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.signal.config.value', 'custom_config_file_found')\n",
      "('spark.sql.catalog.freeds_cat', 'org.apache.iceberg.spark.SparkCatalog')\n",
      "('spark.sql.catalog.freeds_cat.type', 'jdbc')\n",
      "('spark.sql.catalog.freeds_cat.uri', 'jdbc:postgresql://postgres:5432/iceberg')\n",
      "('spark.sql.catalog.freeds_cat.warehouse', 's3a://dwh/warehouse/')\n",
      "('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
      "('spark.sql.shuffle.partitions', '12')\n",
      "('spark.sql.warehouse.dir', 's3a://dwh/warehouse')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.task.maxFailures', '1')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "Dropping and recreating database freeds_cat.selfcheck_db\n",
      "An error occurred while calling o43.sql.\n",
      ": org.apache.iceberg.jdbc.UncheckedSQLException: Failed to connect: jdbc:postgresql://postgres:5432/iceberg\n",
      "\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:90)\n",
      "\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:35)\n",
      "\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:143)\n",
      "\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:70)\n",
      "\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:65)\n",
      "\tat org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:162)\n",
      "\tat org.apache.iceberg.jdbc.JdbcCatalog.initialize(JdbcCatalog.java:147)\n",
      "\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n",
      "\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n",
      "\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n",
      "\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.DropNamespace.mapChildren(v2Commands.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.postgresql.util.PSQLException: The server requested password-based authentication, but no password was provided by plugin null\n",
      "\tat org.postgresql.core.v3.AuthenticationPluginManager.lambda$withEncodedPassword$0(AuthenticationPluginManager.java:111)\n",
      "\tat org.postgresql.core.v3.AuthenticationPluginManager.withPassword(AuthenticationPluginManager.java:82)\n",
      "\tat org.postgresql.core.v3.AuthenticationPluginManager.withEncodedPassword(AuthenticationPluginManager.java:108)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:716)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:207)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:298)\n",
      "\tat java.sql/java.sql.DriverManager.getConnection(Unknown Source)\n",
      "\tat java.sql/java.sql.DriverManager.getConnection(Unknown Source)\n",
      "\tat org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:88)\n",
      "\t... 89 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from freeds.spark import get_spark_session, show_spark_info\n",
    "\n",
    "try:\n",
    "    spark = get_spark_session(\"self-check1\")\n",
    "    cfg = spark.sparkContext.getConf().getAll()\n",
    "    cfg.sort()\n",
    "    for item in cfg:\n",
    "        print(item)\n",
    "    db_name = \"freeds_cat.selfcheck_db\"\n",
    "    table_name = f\"{db_name}.selfcheck_tbl\"\n",
    "\n",
    "    # create some data in delta\n",
    "    print(f\"Dropping and recreating database {db_name}\")\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "    print(f\"Writing table {table_name}\")\n",
    "    data = spark.range(100)\n",
    "    (\n",
    "        data.write.option(  # .mode(\"overwrite\")  # Options: 'overwrite', 'append', 'ignore', 'error' (default)\n",
    "            \"mergeSchema\", \"true\"\n",
    "        )\n",
    "        .format(\"iceberg\")  # Options: 'parquet', 'csv', 'json', 'orc', 'iceberg', 'delta' etc.\n",
    "        .saveAsTable(table_name)\n",
    "    )\n",
    "    spark.stop()\n",
    "\n",
    "    # read some data in delta\n",
    "    spark = get_spark_session(\"self-check2\")\n",
    "    data = spark.table(table_name)\n",
    "    show_spark_info(spark)\n",
    "    data.show(5)\n",
    "\n",
    "    # clean up\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "    spark.stop()\n",
    "\n",
    "    result[\"message\"] = \"Executed spark cell ok.\"\n",
    "    result[\"passed\"] = True\n",
    "\n",
    "except Exception as ex:\n",
    "    result[\"message\"] = str(ex)\n",
    "    print(str(ex))\n",
    "    result[\"passed\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Secrets/configs ---\n",
    "s3_cfg = {\n",
    "    \"endpoint\": \"http://s3-minio:9900\",\n",
    "    \"access_key\": \"kbmB5u0xCQs97d0vJxjx\",\n",
    "    \"secret_key\": \"Tjq8PW7h0GU7rhiMrNMRrvCfpOhfIgvV8dwEhyIJ\",\n",
    "    \"warehouse\": \"s3a://dwh/warehouse/\"\n",
    "}\n",
    "\n",
    "jdbc_cfg = {\n",
    "    \"uri\": \"jdbc:postgresql://postgres:5432/iceberg\",\n",
    "    \"user\": \"jdbc_cat\",\n",
    "    \"password\": \"muppgpt\"\n",
    "}\n",
    "\n",
    "# --- Spark session setup ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergJDBCTest\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "\n",
    "    # Extra jars (Iceberg + Hadoop + Postgres)\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/freeds/spark/jars/*\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/freeds/spark/jars/*\")\n",
    "\n",
    "    # Iceberg extensions\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "    # JDBC catalog\n",
    "    .config(\"spark.sql.catalog.freeds_cat\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.freeds_cat\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \\\n",
    "    .config(\"spark.sql.catalog.freeds_cat.type\", \"jdbc\")\n",
    "    .config(\"spark.sql.catalog.freeds_cat.uri\", jdbc_cfg[\"uri\"])\n",
    "    .config(\"spark.sql.catalog.freeds_cat.warehouse\", s3_cfg[\"warehouse\"])\n",
    "    .config(\"spark.sql.catalog.freeds_cat.jdbc.user\", jdbc_cfg[\"user\"])\n",
    "    .config(\"spark.sql.catalog.freeds_cat.jdbc.password\", jdbc_cfg[\"password\"])\n",
    "\n",
    "    # S3 (MinIO) access\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_cfg[\"endpoint\"])\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3_cfg[\"access_key\"])\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_cfg[\"secret_key\"])\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# --- Test Spark session ---\n",
    "print(\"Available catalogs:\")\n",
    "for cat in spark.catalog.listCatalogs():\n",
    "    print(cat)\n",
    "\n",
    "# Example: create a database in the JDBC catalog\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS freeds_cat.iceberg_db\")\n",
    "\n",
    "# Example: create a table using Iceberg\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS freeds_cat.iceberg_db.test_table (\n",
    "    id INT,\n",
    "    name STRING\n",
    ") USING ICEBERG\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FREEDS",
   "language": "python",
   "name": "freeds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
