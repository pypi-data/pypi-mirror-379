{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example 3 - Manual Control\n",
    "\n",
    "By default this runs a few \"move forward\" actions for two agents, in a separate window.\n",
    "\n",
    "If you uncomment the \"input\" line below, it opens a text box in the Jupyter notebook, allowing basic manual control.\n",
    "\n",
    "eg Enter `\"0 2 s<enter>\"` to tell agent 0 to move forward, and step the environment.\n",
    "\n",
    "You should be able to see the red agent step forward, and get a reward from the env, looking like this:\n",
    "\n",
    "`Rewards:  {0: -1.0, 1: -1.0}   [done= {0: False, 1: False, '__all__': False} ]`\n",
    "\n",
    "Note that this example is set up to use the straightforward \"PIL\" renderer - without the special SBB artwork!\n",
    "The agent observations are displayed as squares of varying sizes, with a paler version of the agent colour.  The targets are half-size squares in the full agent colour.\n",
    "\n",
    "You can switch to the \"PILSVG\" renderer which is prettier but currently renders the agents one step behind, because it needs to know which way the agent is turning.  This can be confusing if you are debugging step-by-step.\n",
    "\n",
    "The image below is what the separate window should look like.\n",
    "\n",
    "\n",
    "(See also: simple_rendering_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simple_example_3.png](simple_example_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, clear_output\n",
    "import ipywidgets as ipw\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import time      \n",
    "  \n",
    "def create_rendering_area():\n",
    "    rendering_area = ipw.Image()\n",
    "    display(rendering_area)\n",
    "    return rendering_area\n",
    "\n",
    "def render_env_to_image(flatland_renderer):\n",
    "    flatland_renderer.render_env(show=False, show_observations=False)\n",
    "    image = flatland_renderer.get_image()\n",
    "    return image\n",
    "\n",
    "def render_env(flatland_renderer, rendering_area : ipw.Image):\n",
    "    pil_image = PIL.Image.fromarray(render_env_to_image(flatland_renderer))\n",
    "    if rendering_area is None:\n",
    "        clear_output(wait=False)\n",
    "        display(pil_image)\n",
    "        return\n",
    "\n",
    "    # convert numpy to PIL to png-format bytes  \n",
    "    with BytesIO() as fOut:\n",
    "        pil_image.save(fOut, format=\"png\")\n",
    "        byPng = fOut.getvalue()\n",
    "\n",
    "    # set the png bytes as the image value; \n",
    "    # this updates the image in the browser.\n",
    "    rendering_area.value=byPng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nAgents = 3\n",
    "n_cities = 2\n",
    "max_rails_between_cities = 2\n",
    "max_rails_in_city = 4\n",
    "seed = 0\n",
    "env = RailEnv(\n",
    "        width=20,\n",
    "        height=30,\n",
    "        rail_generator=sparse_rail_generator(\n",
    "            max_num_cities=n_cities,\n",
    "            seed=seed,\n",
    "            grid_mode=True,\n",
    "            max_rails_between_cities=max_rails_between_cities,\n",
    "            max_rail_pairs_in_city=max_rails_in_city\n",
    "        ),\n",
    "        line_generator=sparse_line_generator(),\n",
    "        number_of_agents=nAgents,\n",
    "        obs_builder_object=TreeObsForRailEnv(max_depth=3, predictor=ShortestPathPredictorForRailEnv())\n",
    "    )\n",
    "\n",
    "init_observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Direction  root :  0 ,  0 ,  0 ,  0 ,  0 ,  0 ,  45.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t Direction  L : -np.inf\n",
      "\t Direction  F :  inf ,  inf ,  inf ,  inf ,  2 ,  7 ,  38.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  8 ,  9 ,  36.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L : -np.inf\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  31 ,  40 ,  5.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R :  inf ,  inf ,  inf ,  inf ,  10 ,  42 ,  47.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  8 ,  40 ,  47.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  41 ,  42 ,  45.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  41 ,  42 ,  45.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R : -np.inf\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  R : -np.inf\n",
      "\t\t Direction  B : -np.inf\n",
      "\t Direction  R : -np.inf\n",
      "\t Direction  B : -np.inf\n",
      " Direction  root :  0 ,  0 ,  0 ,  0 ,  0 ,  0 ,  49.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t Direction  L : -np.inf\n",
      "\t Direction  F :  inf ,  inf ,  inf ,  inf ,  1 ,  7 ,  42.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  8 ,  40 ,  9.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  41 ,  42 ,  7.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  41 ,  42 ,  49.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R : -np.inf\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  29 ,  38 ,  51.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  39 ,  40 ,  49.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  inf ,  39 ,  50.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R : -np.inf\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  R : -np.inf\n",
      "\t\t Direction  B : -np.inf\n",
      "\t Direction  R : -np.inf\n",
      "\t Direction  B : -np.inf\n",
      " Direction  root :  0 ,  0 ,  0 ,  0 ,  0 ,  0 ,  19.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t Direction  L : -np.inf\n",
      "\t Direction  F :  inf ,  inf ,  inf ,  inf ,  2 ,  7 ,  12.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  8 ,  9 ,  10.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L : -np.inf\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  inf ,  11 ,  8.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R :  inf ,  inf ,  inf ,  inf ,  10 ,  14 ,  7.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  8 ,  12 ,  7.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  L :  inf ,  inf ,  inf ,  inf ,  13 ,  14 ,  5.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  F :  inf ,  inf ,  inf ,  inf ,  13 ,  14 ,  47.0 ,  0 ,  0 ,  0 ,  1.0 ,  0\n",
      "\t\t\t Direction  R : -np.inf\n",
      "\t\t\t Direction  B : -np.inf\n",
      "\t\t Direction  R : -np.inf\n",
      "\t\t Direction  B : -np.inf\n",
      "\t Direction  R : -np.inf\n",
      "\t Direction  B : -np.inf\n"
     ]
    }
   ],
   "source": [
    "# Print the observation vector for agent 0\n",
    "obs, all_rewards, done, _ = env.step({0: 0})\n",
    "for i in range(env.get_num_agents()):\n",
    "    env.obs_builder.util_print_obs_subtree(tree=obs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual control: \n",
    "\n",
    "s = perform step\n",
    "q = quit \n",
    "\n",
    "[agent id] [1-2-3 action]  turnleft+move, move to front, turnright+move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7323b94fc2e24799a5adbfb7ac809a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n",
      "Rewards:  {0: 0, 1: 0, 2: 0}   [done= {0: False, 1: False, 2: False, '__all__': False} ]\n"
     ]
    }
   ],
   "source": [
    "rendering_area = create_rendering_area()\n",
    "\n",
    "env_renderer = RenderTool(env, gl=\"PIL\",\n",
    "                                  agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "                                  show_debug=True,\n",
    "                                  screen_height=750,\n",
    "                                  screen_width=750)\n",
    "\n",
    "\n",
    "\n",
    "for step in range(10):\n",
    "\n",
    "    # This is an example command, setting agent 0's action to 2 (move forward), and agent 1's action to 2, \n",
    "    # then stepping the environment.\n",
    "    cmd = \"0 2 1 2 s\"\n",
    "    \n",
    "    # uncomment this input statement if you want to try interactive manual commands\n",
    "    # cmd = input(\">> \")\n",
    "    \n",
    "    cmds = cmd.split(\" \")\n",
    "\n",
    "    action_dict = {}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(cmds):\n",
    "        if cmds[i] == 'q':\n",
    "            import sys\n",
    "\n",
    "            sys.exit()\n",
    "        elif cmds[i] == 's':\n",
    "            obs, all_rewards, done, _ = env.step(action_dict)\n",
    "            action_dict = {}\n",
    "            print(\"Rewards: \", all_rewards, \"  [done=\", done, \"]\")\n",
    "        else:\n",
    "            agent_id = int(cmds[i])\n",
    "            action = int(cmds[i + 1])\n",
    "            action_dict[agent_id] = action\n",
    "            i = i + 1\n",
    "        i += 1\n",
    "\n",
    "    render_env(env_renderer, rendering_area)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
