# Contributing to CrawlStudio

Welcome to CrawlStudio! ğŸ‰ We're building the **ultimate unified web crawling library** and aiming for **10K GitHub stars**. Your contributions are essential to making this vision a reality!

## ğŸš€ Vision & Mission

**Mission**: Create a unified, easy-to-use wrapper around all major web crawlers (Firecrawl, Crawl4AI, Scrapy, and future ones) with consistent APIs and excellent developer experience.

**Goal**: Reach 10,000 GitHub stars by providing the best web crawling solution in Python.

## ğŸ—ï¸ Current Status

### âœ… **What's Working (80% Complete)**
- âœ… **3 Backend Integrations**: Firecrawl, Crawl4AI, Scrapy
- âœ… **Unified API**: Consistent interface across all backends
- âœ… **Single URL Scraping**: All formats (markdown, HTML, structured)
- âœ… **Comprehensive Testing**: Automated test suite
- âœ… **Cross-Platform Support**: Windows, Linux, macOS
- âœ… **Error Handling**: Robust validation and exception handling

### ğŸš§ **What's Needed (20% Remaining)**
- âŒ **Multi-page crawling** for all backends
- âŒ **Batch processing** capabilities
- âŒ **CLI tool** (`crawlstudio crawl website.com`)
- âŒ **URL mapping & discovery**
- âŒ **Advanced features** (proxy rotation, rate limiting)
- âŒ **Documentation** improvements

## ğŸ› ï¸ How to Contribute

### 1. **Getting Started**

```bash
# Clone the repository
git clone https://github.com/aiapicore/CrawlStudio.git
cd CrawlStudio

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Add your API keys to .env file
```

### 2. **Development Setup**

```bash
# Install in development mode
pip install -e .

# Run tests to ensure everything works
python tests/run_all_tests.py

# Or run individual backend tests
python simple_crawl4ai_test.py
python simple_scrapy_test.py
```

### 3. **Project Structure**

```
CrawlStudio/
â”œâ”€â”€ crawlstudio/           # ğŸ“¦ Main package
â”‚   â”œâ”€â”€ backends/          # ğŸ”§ Crawler implementations
â”‚   â”œâ”€â”€ models.py          # ğŸ“‹ Data models
â”‚   â””â”€â”€ utils.py           # ğŸ› ï¸ Utilities
â”œâ”€â”€ tests/                 # ğŸ§ª Test suite
â””â”€â”€ examples/              # ğŸ“– Usage examples
```

## ğŸ¯ Priority Contribution Areas

### ğŸ¥‡ **High Priority (Needed for 10K stars)**

#### 1. **Multi-Page Crawling**
Add website crawling capabilities to all backends:

```python
# Target API
result = await backend.crawl_website(
    url="https://example.com", 
    max_pages=50, 
    depth=2
)
```

**Files to modify:**
- `crawlstudio/backends/firecrawl.py` - Add `crawl_website()` method
- `crawlstudio/backends/crawl4ai.py` - Add multi-page support  
- `crawlstudio/backends/scrapy.py` - Add depth crawling

#### 2. **CLI Tool**
Create a command-line interface:

```bash
crawlstudio crawl https://example.com --format markdown --backend firecrawl
crawlstudio batch urls.txt --output results.json
crawlstudio compare-backends https://example.com
```

**New files needed:**
- `crawlstudio/cli.py` - CLI implementation
- Update `pyproject.toml` - Entry points configuration

#### 3. **Batch Processing**
Add concurrent URL processing:

```python
# Target API  
results = await backend.crawl_batch([
    "https://site1.com",
    "https://site2.com", 
    "https://site3.com"
])
```

### ğŸ¥ˆ **Medium Priority**

#### 4. **Additional Backends**
Add support for more crawlers:
- **Playwright**: Browser automation
- **BeautifulSoup**: Lightweight HTML parsing
- **Selenium**: Legacy browser automation

#### 5. **Advanced Features**
- **Proxy rotation** and **IP management**
- **Rate limiting** and **throttling**
- **Retry logic** with exponential backoff
- **Caching improvements**

#### 6. **Performance Optimizations**
- **Connection pooling**
- **Async optimizations**
- **Memory management**

### ğŸ¥‰ **Lower Priority**

#### 7. **Integrations**
- **LangChain** document loader
- **Streamlit** components
- **FastAPI** middleware

#### 8. **Export Options**
- **Database integrations** (MongoDB, PostgreSQL)
- **Cloud storage** (S3, GCS)
- **Multiple formats** (JSON, CSV, Excel)

## ğŸ§ª Testing Guidelines

### **Running Tests**

```bash
# Run all tests
python tests/run_all_tests.py

# Run specific backend tests
python tests/backends/test_firecrawl.py
python tests/backends/test_crawl4ai.py
python tests/backends/test_scrapy.py

# Run comparison tests
python final_backend_test.py
```

### **Writing Tests**

1. **Add tests for new features**:
   ```python
   # tests/backends/test_new_feature.py
   async def test_new_feature():
       backend = YourBackend(config)
       result = await backend.new_method()
       assert result.success
   ```

2. **Update integration tests**:
   ```python
   # tests/integration/test_backend_comparison.py
   # Add comparison tests for new features
   ```

### **Test Structure Cleanup**

**Current Issue**: We have test files scattered in the root directory from development. Help us organize:

**Root test files** (need to be moved/consolidated):
- `test_firecrawl.py` â†’ `tests/backends/`
- `simple_*.py` â†’ `examples/` or `tests/manual/`
- `comprehensive_test.py` â†’ `tests/integration/`
- `final_backend_test.py` â†’ `examples/demo.py`

## ğŸ“ Code Style & Standards

### **Code Quality**
- **Type hints**: Use type annotations
- **Docstrings**: Document all public methods
- **Error handling**: Proper exception handling
- **Async/await**: Use async patterns consistently

### **Example Code Style**
```python
from typing import Dict, List, Optional
from .base import CrawlBackend
from ..models import CrawlResult, CrawlConfig

class NewBackend(CrawlBackend):
    """Backend for XYZ crawler."""
    
    async def crawl(self, url: str, format: str) -> CrawlResult:
        """Crawl a single URL.
        
        Args:
            url: Target URL to crawl
            format: Output format (markdown, html, structured)
            
        Returns:
            CrawlResult with extracted data
            
        Raises:
            ValueError: If crawling fails
        """
        try:
            # Implementation here
            return CrawlResult(...)
        except Exception as e:
            raise ValueError(f"Backend failed: {str(e)}")
```

## ğŸ¤ How to Submit

### **Pull Request Process**

1. **Fork** the repository
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes** with proper tests
4. **Update documentation** if needed
5. **Run tests**: Ensure all tests pass
6. **Commit** with clear messages: `git commit -m "Add amazing feature"`
7. **Push**: `git push origin feature/amazing-feature`
8. **Submit PR** with detailed description

### **PR Requirements**
- âœ… All tests must pass
- âœ… Code follows project style
- âœ… New features include tests
- âœ… Documentation updated if needed
- âœ… No breaking changes (unless discussed)

## ğŸ† Recognition

### **Contributors Wall**
We'll recognize all contributors in:
- ğŸ“– **README.md** contributors section
- ğŸŒŸ **Release notes** for major contributions
- ğŸ‰ **Special mentions** for 10K star milestone contributors

### **Contribution Types**
- ğŸ’» **Code contributions** (backends, features, fixes)
- ğŸ§ª **Testing** (test cases, CI/CD, quality assurance)
- ğŸ“– **Documentation** (guides, tutorials, API docs)
- ğŸ› **Bug reports** with detailed reproduction steps
- ğŸ’¡ **Feature requests** with clear use cases
- ğŸ¨ **Design** (UI/UX for CLI, logos, branding)

## ğŸŒŸ Roadmap to 10K Stars

### **Phase 1: Foundation** (âœ… Complete)
- âœ… Core backends working
- âœ… Unified API  
- âœ… Testing infrastructure

### **Phase 2: Features** (ğŸš§ In Progress)
- ğŸš§ Multi-page crawling
- ğŸš§ CLI tool
- ğŸš§ Batch processing

### **Phase 3: Polish** (ğŸ“… Next)
- ğŸ“… Advanced features
- ğŸ“… Performance optimization
- ğŸ“… Comprehensive docs

### **Phase 4: Ecosystem** (ğŸ”® Future)
- ğŸ”® Framework integrations
- ğŸ”® Cloud deployment
- ğŸ”® Enterprise features

## ğŸ’¬ Communication

### **Get Help**
- ğŸ› **Bug reports**: [GitHub Issues](https://github.com/aiapicore/CrawlStudio/issues)
- ğŸ’¡ **Feature requests**: [GitHub Discussions](https://github.com/aiapicore/CrawlStudio/discussions)
- â“ **Questions**: Start a discussion or create an issue

### **Code of Conduct**
- ğŸ¤ **Be respectful** and inclusive
- ğŸ¯ **Focus on the mission** (10K stars goal)
- ğŸš€ **Be constructive** in feedback
- ğŸ“š **Help others learn** and grow

## ğŸ¯ Quick Start Checklist

For new contributors:

- [ ] Read this CONTRIBUTING.md
- [ ] Set up development environment
- [ ] Run existing tests successfully
- [ ] Pick an issue or feature from roadmap
- [ ] Create feature branch
- [ ] Make changes with tests
- [ ] Submit PR with clear description

## ğŸŒŸ Thank You!

Every contribution brings us closer to **10K GitHub stars** and our vision of the **best web crawling library in Python**. Whether you're fixing a bug, adding a feature, or improving documentation, you're helping developers worldwide! 

**Together, we're building something amazing!** ğŸ‰

---

*Ready to contribute? Start by exploring our [issues](https://github.com/aiapicore/CrawlStudio/issues) or [roadmap](enhancement_suggestions.md)!*