import os
import time
import uuid
import json
import logging
import threading
import asyncio
import multiprocessing
from typing import Dict, Any
from enum import Enum
from collections import defaultdict, namedtuple

import redis
from redis.asyncio.lock import Lock as AsyncLock

from ..utils.serializer import dumps_str

logger = logging.getLogger('app')

from .heartbeat_process import HeartbeatProcessManager
from .worker_scanner import WorkerScanner


class ConsumerStrategy(Enum):
    """æ¶ˆè´¹è€…åç§°ç­–ç•¥
    
    ç­–ç•¥é€‰æ‹©æŒ‡å—ï¼š
    
    âš ï¸  POD (ä»…æ¨èå•è¿›ç¨‹ä½¿ç”¨):
       - åŸºäºK8s Podåç§°çš„å›ºå®šconsumer
       - é€‚ç”¨åœºæ™¯: å•è¿›ç¨‹åº”ç”¨ (asyncio/threadæ‰§è¡Œå™¨)
       - ä¼˜ç‚¹: è¯­ä¹‰æ¸…æ™°ï¼Œä¾¿äºç›‘æ§
       - ç¼ºç‚¹: å¤šè¿›ç¨‹ä¸‹ä¼šäº§ç”Ÿå†²çª
       
    ğŸ”§ FIXED (é«˜çº§ç”¨æˆ·):
       - å®Œå…¨è‡ªå®šä¹‰çš„consumeråç§°
       - é€‚ç”¨åœºæ™¯: æœ‰ç‰¹æ®Šå‘½åéœ€æ±‚çš„åœºæ™¯ 
       - ä¼˜ç‚¹: å®Œå…¨å¯æ§
       - ç¼ºç‚¹: éœ€è¦ç”¨æˆ·ç¡®ä¿å”¯ä¸€æ€§
    
    ğŸ”¥ HEARTBEAT (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ):
       - åŸºäºå¿ƒè·³çš„ç®€åŒ–ç­–ç•¥
       - é€‚ç”¨åœºæ™¯: æ— çŠ¶æ€æœåŠ¡å¹³å°ï¼ˆCloud Runã€Serverlessã€K8sï¼‰
       - ä¼˜ç‚¹: é€»è¾‘ç®€å•ï¼Œç¨³å®šå¯é ï¼Œè‡ªåŠ¨æ•…éšœæ¢å¤
       - ç‰¹ç‚¹: ä½¿ç”¨éšæœºconsumer nameï¼Œé€šè¿‡æœ‰åºé›†åˆç»´æŠ¤å¿ƒè·³
    """
    FIXED = "fixed"      # å›ºå®šåç§°
    POD = "pod"          # K8s Podåç§° (âš ï¸ å¤šè¿›ç¨‹ä¸‹ä¸æ¨è)
    HEARTBEAT = "heartbeat"  # å¿ƒè·³ç­–ç•¥ (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ)


class ConsumerManager:
    """æ¶ˆè´¹è€…åç§°ç®¡ç†å™¨"""
    
    def __init__(
        self, 
        redis_client: redis.StrictRedis,
        strategy: ConsumerStrategy = ConsumerStrategy.HEARTBEAT,
        config: Dict[str, Any] = None
    ):
        self.redis_client = redis_client
        self.strategy = strategy
        self.config = config or {}
        self._consumer_name = None
        
        # Redis prefix configuration
        self.redis_prefix = config.get('redis_prefix', 'jettask')
        
        # éªŒè¯ç­–ç•¥é…ç½®çš„åˆç†æ€§
        self._validate_strategy_configuration()
        
        # å¿ƒè·³ç­–ç•¥å®ä¾‹ - å¦‚æœæ˜¯HEARTBEATç­–ç•¥ï¼Œç«‹å³åˆå§‹åŒ–
        if self.strategy == ConsumerStrategy.HEARTBEAT:
            # ä¼ é€’é˜Ÿåˆ—ä¿¡æ¯åˆ°å¿ƒè·³ç­–ç•¥
            heartbeat_config = self.config.copy()
            heartbeat_config['queues'] = self.config.get('queues', [])
            self._heartbeat_strategy = HeartbeatConsumerStrategy(
                self.redis_client,
                heartbeat_config
            )
        else:
            self._heartbeat_strategy = None
    
    def get_prefixed_queue_name(self, queue: str) -> str:
        """ä¸ºé˜Ÿåˆ—åç§°æ·»åŠ å‰ç¼€"""
        return f"{self.redis_prefix}:QUEUE:{queue}"
    
    def _validate_strategy_configuration(self):
        """éªŒè¯æ¶ˆè´¹è€…ç­–ç•¥é…ç½®çš„åˆç†æ€§"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­
        current_process = multiprocessing.current_process()
        is_multiprocess = current_process.name != 'MainProcess'
        
        if self.strategy == ConsumerStrategy.POD and is_multiprocess:
            # PODç­–ç•¥åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ˜¯ä¸å…è®¸çš„ï¼Œç›´æ¥é€€å‡º
            error_msg = (
                "\n"
                "âŒ é”™è¯¯: PODç­–ç•¥ä¸èƒ½åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨ï¼\n"
                "\n"
                "åŸå› : PODç­–ç•¥ä½¿ç”¨å›ºå®šçš„consumeråç§°ï¼Œå¤šè¿›ç¨‹ä¼šå¯¼è‡´æ¶ˆæ¯é‡å¤æ¶ˆè´¹ã€‚\n"
                "\n"
                "è§£å†³æ–¹æ¡ˆ:\n"
                "  1. ä½¿ç”¨ ConsumerStrategy.HEARTBEAT - å¿ƒè·³ç­–ç•¥ (æ¨è)\n"
                "  2. ä½¿ç”¨ ConsumerStrategy.FIXED - è‡ªå®šä¹‰å›ºå®šåç§°\n"
                "  3. ä½¿ç”¨å•è¿›ç¨‹æ‰§è¡Œå™¨ (asyncio/thread)\n"
                "\n"
                f"å½“å‰ç¯å¢ƒ: {current_process.name} (PID: {os.getpid()})\n"
            )
            logger.error(error_msg)
            # ç«‹å³é€€å‡ºç¨‹åº
            import sys
            sys.exit(1)
        
        # è®°å½•ç­–ç•¥é€‰æ‹©ç”¨äºè°ƒè¯•
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Consumer strategy: {self.strategy.value}, Process: {current_process.name}")
        
    def get_consumer_name(self, queue: str) -> str:
        """è·å–æ¶ˆè´¹è€…åç§°"""
        # print(f'è·å–æ¶ˆè´¹è€…åç§°: {self.strategy} {queue}')
        if self.strategy == ConsumerStrategy.FIXED:
            return self._get_fixed_name(queue)
        elif self.strategy == ConsumerStrategy.POD:
            return self._get_pod_name(queue)
        elif self.strategy == ConsumerStrategy.HEARTBEAT:
            return self._get_heartbeat_name(queue)
        else:
            raise ValueError(f"Unknown consumer strategy: {self.strategy}")
    
    def _get_fixed_name(self, queue: str) -> str:
        """è·å–å›ºå®šçš„æ¶ˆè´¹è€…åç§°"""
        if not self._consumer_name:
            # å¯ä»¥ä»é…ç½®ã€ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶ä¸­è¯»å–
            self._consumer_name = self.config.get('consumer_name') or \
                                  os.environ.get('EASYTASK_CONSUMER_NAME') or \
                                  f"worker-{os.getpid()}"
        return f"{self._consumer_name}-{queue}"
    
    def _get_pod_name(self, queue: str) -> str:
        """è·å–åŸºäºK8s Podçš„æ¶ˆè´¹è€…åç§°
        
        æ³¨æ„ï¼šPODç­–ç•¥åªèƒ½åœ¨å•è¿›ç¨‹ç¯å¢ƒä¸‹ä½¿ç”¨
        """
        if not self._consumer_name:
            # åœ¨K8sä¸­ï¼Œé€šå¸¸é€šè¿‡ç¯å¢ƒå˜é‡è·å–Podåç§°
            pod_name = os.environ.get('HOSTNAME') or \
                       os.environ.get('POD_NAME') or \
                       os.environ.get('K8S_POD_NAME')
            
            if not pod_name:
                logger.warning("Pod name not found, falling back to hostname")
                import socket
                pod_name = socket.gethostname()
            
            # ç”±äºå·²ç»åœ¨_validate_strategy_configurationä¸­éªŒè¯è¿‡ï¼Œ
            # è¿™é‡Œåº”è¯¥åªä¼šåœ¨MainProcessä¸­æ‰§è¡Œ
            self._consumer_name = pod_name
            logger.debug(f"ä½¿ç”¨Podç­–ç•¥çš„consumeråç§°: {self._consumer_name}")
                
        return f"{self._consumer_name}-{queue}"
    
    
    def _get_heartbeat_name(self, queue: str) -> str:
        """åŸºäºå¿ƒè·³ç­–ç•¥è·å–æ¶ˆè´¹è€…åç§°"""
        if not self._heartbeat_strategy:
            raise RuntimeError("Heartbeat strategy not initialized properly")
        
        return self._heartbeat_strategy.get_consumer_name(queue)
    
    def cleanup(self):
        """æ¸…ç†èµ„æºï¼ˆä¼˜é›…å…³é—­æ—¶è°ƒç”¨ï¼‰"""
        # å¤„ç†å¿ƒè·³ç­–ç•¥çš„æ¸…ç†
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.cleanup()
    
    def update_stats(self, queue: str, success: bool = True, processing_time: float = 0.0,
                    total_latency: float = None):
        """æ›´æ–°æ¶ˆè´¹è€…çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.update_stats(queue, success, processing_time, total_latency)
    
    def task_started(self, queue: str):
        """ä»»åŠ¡å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.task_started(queue)
    
    def task_finished(self, queue: str):
        """ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.task_finished(queue)
    
    def is_heartbeat_timeout(self) -> bool:
        """æ£€æŸ¥å¿ƒè·³æ˜¯å¦å·²è¶…æ—¶ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            return self._heartbeat_strategy.is_heartbeat_timeout()
        return False
    
    def record_group_info(self, queue: str, task_name: str, group_name: str, consumer_name: str):
        """è®°å½•taskçš„groupä¿¡æ¯åˆ°worker hashè¡¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.record_group_info(queue, task_name, group_name, consumer_name)
    
    async def record_group_info_async(self, queue: str, task_name: str, group_name: str, consumer_name: str):
        """å¼‚æ­¥è®°å½•taskçš„groupä¿¡æ¯åˆ°worker hashè¡¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            await self._heartbeat_strategy.record_group_info_async(queue, task_name, group_name, consumer_name)
    
    def cleanup_expired_consumers(self, queue: str):
        """æ¸…ç†è¿‡æœŸçš„æ¶ˆè´¹è€…ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        try:
            # è·å–æ¶ˆè´¹è€…ç»„çš„pendingæ¶ˆæ¯ä¿¡æ¯
            prefixed_queue = self.get_prefixed_queue_name(queue)
            pending_info = self.redis_client.xpending(prefixed_queue, prefixed_queue)
            if not pending_info:
                return
                
            # è·å–è¯¦ç»†çš„pendingæ¶ˆæ¯
            consumers = self.redis_client.xpending_range(
                prefixed_queue, prefixed_queue, min='-', max='+', count=100
            )
            
            for consumer_info in consumers:
                consumer_name = consumer_info['consumer']
                idle_time = consumer_info['time_since_delivered']
                
                # å¦‚æœæ¶ˆæ¯ç©ºé—²æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œå¯èƒ½æ¶ˆè´¹è€…å·²ç»æ­»äº¡
                # ä½¿ç”¨120ç§’ä½œä¸ºé»˜è®¤çš„æ­»äº¡æ£€æµ‹é˜ˆå€¼
                if idle_time > 120 * 1000:  # 120ç§’
                    logger.warning(
                        f"Consumer {consumer_name} has pending messages "
                        f"idle for {idle_time/1000}s, may be dead"
                    )
                    # è¿™é‡Œå¯ä»¥å®ç°æ¶ˆæ¯é‡æ–°åˆ†é…é€»è¾‘
                    
        except Exception as e:
            logger.error(f"Error cleaning up expired consumers: {e}")

class HeartbeatConsumerStrategy:
    """åŸºäºå¿ƒè·³çš„ç®€åŒ–æ¶ˆè´¹è€…ç­–ç•¥
    
    ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨éšæœºconsumer name
    2. æ¯ä¸ªé˜Ÿåˆ—ç»´æŠ¤ç‹¬ç«‹çš„å¿ƒè·³æœ‰åºé›†åˆ
    3. å¿ƒè·³æ•°æ®åŒ…å«workerçš„è¯¦ç»†ä¿¡æ¯
    4. è‡ªåŠ¨é‡ç½®æ­»äº¡workerçš„pendingä»»åŠ¡
    """
    
    def __init__(self, redis_client: redis.StrictRedis, config: Dict = None):
        self.redis = redis_client
        self.config = config or {}
        # è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯ï¼ˆä»appæ¨¡å—ï¼‰
        try:
            from ..core.app import get_async_redis_pool
            from redis import asyncio as aioredis
            redis_url = config.get('redis_url') or 'redis://localhost:6379'
            async_pool = get_async_redis_pool(redis_url)
            self.async_redis = aioredis.StrictRedis(connection_pool=async_pool)
        except Exception as e:
            logger.warning(f"Failed to create async redis client: {e}")
            self.async_redis = None
        # é…ç½®å‚æ•°
        self.heartbeat_interval = self.config.get('heartbeat_interval', 1)  # 5ç§’å¿ƒè·³
        self.heartbeat_timeout = self.config.get('heartbeat_timeout', 3)  # 30ç§’è¶…æ—¶
        self.scan_interval = self.config.get('scan_interval', 5)  # 10ç§’æ‰«æä¸€æ¬¡
        
        # è·å–Rediså‰ç¼€ï¼ˆä»é…ç½®ä¸­ï¼‰
        self.redis_prefix = config.get('redis_prefix', 'jettask')
        
        # è·å–Workerå‰ç¼€ï¼ˆä»é…ç½®ä¸­ï¼Œé»˜è®¤ä¸ºWORKERï¼‰
        # å…è®¸ä¸åŒçš„æœåŠ¡ä½¿ç”¨ä¸åŒçš„å‰ç¼€æ¥åŒºåˆ†å‘½åç©ºé—´
        self.worker_prefix = config.get('worker_prefix', 'WORKER')
        
        # ä¿å­˜é…ç½®ä¸­çš„é˜Ÿåˆ—åˆ—è¡¨
        self.configured_queues = config.get('queues', [])
        
        # è·å–ä¸»æœºåå‰ç¼€
        import socket
        try:
            # é¦–å…ˆå°è¯•è·å–hostname
            hostname = socket.gethostname()
            # å°è¯•è·å–IPåœ°å€
            ip = socket.gethostbyname(hostname)
            # ä¼˜å…ˆä½¿ç”¨hostnameï¼Œå¦‚æœhostnameæ˜¯localhoståˆ™ä½¿ç”¨IP
            prefix = hostname if hostname != 'localhost' else ip
        except:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
            prefix = os.environ.get('HOSTNAME', 'unknown')
        
        # ä¿å­˜ä¸»æœºåå‰ç¼€ï¼Œå»¶è¿Ÿåˆ›å»ºconsumer_id
        self.hostname_prefix = prefix
        self.consumer_id = None  # å»¶è¿Ÿåˆ›å»ºï¼Œé¿å…åœ¨ä¸»è¿›ç¨‹ä¸­åˆ›å»º
        
        # æ–°çš„æ•°æ®ç»“æ„è®¾è®¡ - worker_key ä¹Ÿå»¶è¿Ÿåˆ›å»º
        self._worker_key = None
        
        self.consumer_names = {}  # queue -> consumer_name mapping
        self.active_queues = set()  # è®°å½•å½“å‰æ´»è·ƒçš„é˜Ÿåˆ—
        
        # åå°æ§åˆ¶
        self._scanner_thread = None
        self._scanner_task = None
        self._scanner_stop = threading.Event()
        
        # ç»Ÿè®¡åˆ·æ–°çº¿ç¨‹/åç¨‹
        self._stats_flusher_thread = None
        self._stats_flusher_task = None
        self._stats_flusher_stop = threading.Event()
        
        # å¿ƒè·³è¿›ç¨‹ç®¡ç†å™¨
        self._heartbeat_process_manager = None
        self._heartbeat_processes = {}  # queue -> process mapping
        logger.debug("HeartbeatStrategy initialized with process-based heartbeat support")
        
        # ç»Ÿè®¡ç¼“å†²åŒº - ä½¿ç”¨æ— é”è®¾è®¡
        # å®šä¹‰ç»Ÿè®¡äº‹ä»¶ç±»å‹
        self.StatsEvent = namedtuple('StatsEvent', ['type', 'queue', 'value', 'timestamp'])
        
        # ä½¿ç”¨ç®€å•åˆ—è¡¨æ›¿ä»£é˜Ÿåˆ—ï¼ˆç°åœ¨æ˜¯çº¯å¼‚æ­¥ç¯å¢ƒï¼‰
        self.stats_events = []  # ç»Ÿè®¡äº‹ä»¶åˆ—è¡¨
        
        # æœ¬åœ°ç´¯ç§¯ç¼“å†²åŒºï¼ˆä»…åœ¨flushæ—¶ä½¿ç”¨ï¼‰
        self.stats_accumulator = {
            'running_tasks': defaultdict(int),
            'success_count': defaultdict(int),
            'failed_count': defaultdict(int),
            'total_time': defaultdict(float),
            'total_count': defaultdict(int),
            'total_latency': defaultdict(float)
        }
        
        self.stats_flush_interval = self.config.get('stats_flush_interval', 0.5)  # æ›´é¢‘ç¹åœ°åˆ·æ–°
        self.last_stats_flush = time.time()
        
        # å»¶è¿Ÿå¯åŠ¨æ‰«æçº¿ç¨‹ï¼Œåªæœ‰åœ¨çœŸæ­£éœ€è¦æ—¶æ‰å¯åŠ¨
        self._scanner_started = False
        self._scanner_needs_start = False  # æ ‡è®°æ˜¯å¦éœ€è¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­å¯åŠ¨
        self._startup_time = time.time()  # è®°å½•å¯åŠ¨æ—¶é—´ï¼Œç”¨äºå¿ƒè·³è¶…æ—¶å®½é™æœŸ
        
        # Worker æ‰«æå™¨ - ç›´æ¥åˆå§‹åŒ–
        self.scanner = WorkerScanner(
            self.redis, self.async_redis,
            self.redis_prefix, self.heartbeat_timeout
        )
        
        # å»¶è¿Ÿå¯åŠ¨ç»Ÿè®¡åˆ·æ–°çº¿ç¨‹
        self._stats_flusher_started = False
        
        # æ³¨å†Œé€€å‡ºå¤„ç†
        import atexit
        atexit.register(self.cleanup)
    
    def _find_reusable_worker_id(self, prefix: str) -> str:
        """æŸ¥æ‰¾å¯ä»¥å¤ç”¨çš„ç¦»çº¿worker ID
        
        ä½¿ç”¨åˆ†å¸ƒå¼é”æ¥é˜²æ­¢å¤šä¸ªè¿›ç¨‹åŒæ—¶å¤ç”¨åŒä¸€ä¸ªworker ID
        
        Args:
            prefix: ä¸»æœºåå‰ç¼€
            
        Returns:
            å¯å¤ç”¨çš„consumer_idï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # ä½¿ç”¨Redisçš„åˆ†å¸ƒå¼é”ï¼Œå¯ä»¥è‡ªåŠ¨ç­‰å¾…é”é‡Šæ”¾
        reuse_lock_key = f"{self.redis_prefix}:{self.worker_prefix}:REUSE:LOCK"
        # åˆ›å»ºRedisé”å¯¹è±¡ï¼Œè¶…æ—¶æ—¶é—´5ç§’ï¼Œé˜»å¡ç­‰å¾…æœ€å¤š2ç§’
        from redis.lock import Lock
        lock = Lock(self.redis, reuse_lock_key, timeout=5, blocking=True, blocking_timeout=2)
        
        try:
            acquired = lock.acquire()
            if not acquired:
                logger.debug("Could not acquire worker reuse lock, creating new ID")
                return None
            
            # æ‰«ææ‰€æœ‰workeré”®
            pattern = f"{self.redis_prefix}:{self.worker_prefix}:*"
            worker_keys = []
            cursor = 0
            
            # ä½¿ç”¨SCANè¿­ä»£è·å–æ‰€æœ‰workeré”®
            while True:
                cursor, keys = self.redis.scan(cursor, match=pattern, count=100)
                # è¿‡æ»¤æ‰HISTORYç›¸å…³çš„é”®ã€é”é”®å’ŒREUSINGæ ‡è®°é”®
                for key in keys:
                    # key æ˜¯ bytes ç±»å‹ï¼Œéœ€è¦è§£ç æˆ–ä½¿ç”¨ bytes è¿›è¡Œæ¯”è¾ƒ
                    key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                    if ':HISTORY:' not in key_str and ':REUSE:LOCK' not in key_str and ':REUSING' not in key_str:
                        worker_keys.append(key)
                if cursor == 0:
                    break
            
            if not worker_keys:
                logger.debug("No worker keys found during scan")
                return None
            else:
                logger.debug(f"Found {len(worker_keys)} worker keys to check")
            
            # æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„ç¦»çº¿worker
            offline_workers = []
            
            for worker_key in worker_keys:
                try:
                    # è·å–workeræ•°æ®
                    worker_data = self.redis.hgetall(worker_key)
                    # logger.debug(f'{worker_key=} {worker_data=}')
                    if not worker_data:
                        continue
                    # è·å–workerçš„çŠ¶æ€ä¿¡æ¯
                    is_alive_val = worker_data.get('is_alive', 'true')
                    if isinstance(is_alive_val, bytes):
                        is_alive_val = is_alive_val.decode('utf-8')
                    is_alive = is_alive_val.lower() == 'true'
                    
                    last_heartbeat_val = worker_data.get('last_heartbeat', 0)
                    if isinstance(last_heartbeat_val, bytes):
                        last_heartbeat_val = last_heartbeat_val.decode('utf-8')
                    last_heartbeat = float(last_heartbeat_val)
                    current_time = time.time()
                    
                    # è·å–ç¦»çº¿æ—¶é—´
                    offline_time_str = worker_data.get('offline_time', '0')
                    if isinstance(offline_time_str, bytes):
                        offline_time_str = offline_time_str.decode('utf-8')
                    try:
                        offline_time = float(offline_time_str) if offline_time_str else last_heartbeat
                    except:
                        offline_time = last_heartbeat
                    
                    # åˆ¤æ–­workeræ˜¯å¦çœŸçš„ç¦»çº¿
                    # 1. is_aliveæ ‡è®°ä¸ºfalseï¼Œæˆ–è€…
                    # 2. æœ€åå¿ƒè·³æ—¶é—´è¶…è¿‡äº†heartbeat_timeout
                    is_truly_offline = (not is_alive) or (current_time - last_heartbeat > self.heartbeat_timeout)
                    # logger.debug(f'{is_truly_offline=} {worker_data=}')
                    if not is_truly_offline:
                        logger.debug(f"Worker {is_alive=} {current_time - last_heartbeat} {self.heartbeat_timeout} {worker_data.get('consumer_id')} is still active (last_heartbeat: {current_time - last_heartbeat:.1f}s ago)")
                        continue
                    
                    # éœ€è¦ç¦»çº¿è¶…è¿‡heartbeat_timeoutæ‰èƒ½è¢«å¤ç”¨ï¼ˆä¸ç¦»çº¿æ£€æµ‹ä¿æŒä¸€è‡´ï¼‰
                    # min_offline_duration = self.heartbeat_timeout
                    # if offline_time > 0 and (current_time - offline_time) < min_offline_duration:
                    #     logger.debug(f"Worker {worker_data.get('consumer_id')} offline for only {current_time - offline_time:.1f}s, need {min_offline_duration}s")
                    #     continue
                    
                    # è·å–consumer_id
                    consumer_id = worker_data.get('consumer_id', '')
                    if isinstance(consumer_id, bytes):
                        consumer_id = consumer_id.decode('utf-8')
                    if not consumer_id:
                        continue
                    
                    # ä¸å†æ£€æŸ¥å‰ç¼€ï¼Œå…è®¸å¤ç”¨ä»»ä½•ç¦»çº¿çš„worker
                        
                    # ä½¿ç”¨ç¦»çº¿æ—¶é—´æˆ–æœ€åå¿ƒè·³æ—¶é—´
                    if 'offline_time' in worker_data:
                        offline_time = float(worker_data.get('offline_time'))
                    else:
                        # å¦‚æœæ²¡æœ‰offline_timeï¼Œä½¿ç”¨æœ€åå¿ƒè·³æ—¶é—´
                        offline_time = last_heartbeat
                        logger.debug(f"Worker {consumer_id} has no offline_time, using last_heartbeat")
                 
                    offline_workers.append((consumer_id, offline_time, worker_key))
                        
                except Exception as e:
                    logger.debug(f"Error checking worker {worker_key}: {e}")
                    continue
            
            if not offline_workers:
                logger.debug(f"No offline workers found matching prefix {prefix}")
                return None
            else:
                logger.debug(f"Found {len(offline_workers)} offline workers: {[w[0] for w in offline_workers]}")
            
            # æŒ‰ç¦»çº¿æ—¶é—´æ’åºï¼Œé€‰æ‹©ç¦»çº¿æ—¶é—´æœ€é•¿çš„ï¼ˆæœ€æ—©ç¦»çº¿çš„ï¼‰
            offline_workers.sort(key=lambda x: x[1])
            selected_consumer_id, selected_offline_time, selected_worker_key = offline_workers[0]
            
            # é‡ç½®è¯¥workerçš„çŠ¶æ€ - ä¿ç•™æ‰€æœ‰ç»Ÿè®¡æ•°æ®ï¼Œä½†ä¸ä¿ç•™queues
            pipeline = self.redis.pipeline()
            
            # æ›´æ–°åŸºæœ¬ä¿¡æ¯ï¼Œä¿ç•™åŸæœ‰çš„queueså­—æ®µ
            pipeline.hset(selected_worker_key, mapping={
                'consumer_id': selected_consumer_id,
                'is_alive': 'true',
                'last_heartbeat': str(time.time()),
                'pid': str(os.getpid()),
                'created_at': str(time.time()),
                'messages_transferred': 'false'  # é‡ç½®æ¶ˆæ¯è½¬ç§»æ ‡è®°ï¼Œè¿™æ˜¯æ–°çš„ç”Ÿå‘½å‘¨æœŸ
            })
            
            # æ³¨æ„ï¼šä¸åˆ é™¤queueså­—æ®µï¼Œè®©å¿ƒè·³çº¿ç¨‹æ ¹æ®å®é™…æƒ…å†µæ›´æ–°
            # è¿™é¿å…äº†åœ¨å¤ç”¨æ—¶æ¸…ç©ºqueueså¯¼è‡´çš„æ˜¾ç¤ºé—®é¢˜
            
            # ä¿ç•™æ‰€æœ‰ç»Ÿè®¡æ•°æ®ï¼Œä¸æ¸…ç©º
            
            pipeline.execute()
            
            logger.debug(f"Found reusable worker: {selected_consumer_id}, offline since {time.time() - selected_offline_time:.1f}s ago")
            return selected_consumer_id
            
        except Exception as e:
            logger.error(f"Error finding reusable worker ID: {e}")
            return None
        finally:
            try:
                lock.release()
            except:
                pass
    
    def get_prefixed_queue_name(self, queue: str) -> str:
        """ä¸ºé˜Ÿåˆ—åç§°æ·»åŠ å‰ç¼€"""
        return f"{self.redis_prefix}:QUEUE:{queue}"
    
    def update_stats(self, queue: str, success: bool = True, processing_time: float = 0.0, 
                    total_latency: float = None):
        """æ›´æ–°workerçš„ç»Ÿè®¡ä¿¡æ¯ - ä½¿ç”¨æ— é”é˜Ÿåˆ—
        
        Args:
            queue: é˜Ÿåˆ—åç§°
            success: æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
            processing_time: å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰ - å®é™…æ‰§è¡Œæ—¶é—´
            total_latency: æ€»å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ - ä»ä»»åŠ¡åˆ›å»ºåˆ°å®Œæˆçš„æ€»æ—¶é—´
        """
        try:
            # åˆ›å»ºç»Ÿè®¡äº‹ä»¶å¹¶æ·»åŠ åˆ°åˆ—è¡¨
            timestamp = time.time()
            
            # æˆåŠŸ/å¤±è´¥è®¡æ•°
            event_type = 'success' if success else 'failed'
            self.stats_events.append(
                self.StatsEvent(event_type, queue, 1, timestamp)
            )
            
            # å¤„ç†æ—¶é—´
            if processing_time > 0:
                self.stats_events.append(
                    self.StatsEvent('processing_time', queue, processing_time, timestamp)
                )
            
            # æ€»å»¶è¿Ÿæ—¶é—´
            if total_latency is not None and total_latency > 0:
                self.stats_events.append(
                    self.StatsEvent('total_latency', queue, total_latency, timestamp)
                )
                
        except Exception as e:
            logger.error(f"Error updating stats: {e}")
    
    def task_started(self, queue: str):
        """ä»»åŠ¡å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨ - æ·»åŠ åˆ°äº‹ä»¶åˆ—è¡¨"""
        self.stats_events.append(
            self.StatsEvent('task_started', queue, 1, time.time())
        )
    
    def task_finished(self, queue: str):
        """ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ - æ·»åŠ åˆ°äº‹ä»¶åˆ—è¡¨"""
        self.stats_events.append(
            self.StatsEvent('task_finished', queue, -1, time.time())
        )
    
    async def flush_stats_buffer(self):
        """åˆ·æ–°ç»Ÿè®¡ç¼“å†²åˆ° Redis - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        # å¦‚æœworkerä»æœªåˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
        if self.consumer_id is None or self._worker_key is None:
            logger.debug("Worker not initialized, skipping stats flush")
            return
            
        # ç›´æ¥è·å–æ‰€æœ‰å¾…å¤„ç†çš„äº‹ä»¶
        events = self.stats_events.copy()  # å¤åˆ¶å½“å‰äº‹ä»¶åˆ—è¡¨
        self.stats_events.clear()  # æ¸…ç©ºåŸåˆ—è¡¨
        start_time = time.time()
        
        try:
            if not events:
                return
            
            # æ¸…ç©ºç´¯ç§¯å™¨
            for buffer in self.stats_accumulator.values():
                buffer.clear()
            
            # å¤„ç†æ‰€æœ‰äº‹ä»¶ï¼Œç´¯ç§¯åˆ°æœ¬åœ°ç¼“å†²åŒº
            for event in events:
                if event.type == 'success':
                    self.stats_accumulator['success_count'][event.queue] += event.value
                    self.stats_accumulator['total_count'][event.queue] += event.value
                elif event.type == 'failed':
                    self.stats_accumulator['failed_count'][event.queue] += event.value
                    self.stats_accumulator['total_count'][event.queue] += event.value
                elif event.type == 'processing_time':
                    self.stats_accumulator['total_time'][event.queue] += event.value
                elif event.type == 'total_latency':
                    self.stats_accumulator['total_latency'][event.queue] += event.value
                elif event.type == 'task_started':
                    self.stats_accumulator['running_tasks'][event.queue] += event.value
                elif event.type == 'task_finished':
                    self.stats_accumulator['running_tasks'][event.queue] += event.value  # æ³¨æ„ï¼štask_finishedçš„valueæ˜¯-1
            
            # æ‰¹é‡æ›´æ–°åˆ° Redis
            pipeline = self.async_redis.pipeline()
            processed_queues = set()
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦æ›´æ–°çš„é˜Ÿåˆ—
            for buffer in self.stats_accumulator.values():
                processed_queues.update(buffer.keys())
            
            # ä¸ºæ¯ä¸ªé˜Ÿåˆ—æ„å»ºæ‰¹é‡æ›´æ–°
            for queue in processed_queues:
                # è¿è¡Œä¸­ä»»åŠ¡æ•°ï¼ˆå¯èƒ½ä¸ºè´Ÿæ•°ï¼Œè¡¨ç¤ºå‡å°‘ï¼‰
                if queue in self.stats_accumulator['running_tasks']:
                    delta = self.stats_accumulator['running_tasks'][queue]
                    if delta != 0:
                        pipeline.hincrby(self._worker_key, f'{queue}:running_tasks', delta)
                
                # æˆåŠŸè®¡æ•°
                if queue in self.stats_accumulator['success_count']:
                    pipeline.hincrby(self._worker_key, f'{queue}:success_count', 
                                   self.stats_accumulator['success_count'][queue])
                
                # å¤±è´¥è®¡æ•°
                if queue in self.stats_accumulator['failed_count']:
                    pipeline.hincrby(self._worker_key, f'{queue}:failed_count', 
                                   self.stats_accumulator['failed_count'][queue])
                
                # æ€»è®¡æ•°
                if queue in self.stats_accumulator['total_count']:
                    pipeline.hincrby(self._worker_key, f'{queue}:total_count', 
                                   self.stats_accumulator['total_count'][queue])
                
                # å¤„ç†æ—¶é—´
                if queue in self.stats_accumulator['total_time']:
                    pipeline.hincrbyfloat(self._worker_key, f'{queue}:total_processing_time', 
                                        self.stats_accumulator['total_time'][queue])
                
                # å»¶è¿Ÿæ—¶é—´
                if queue in self.stats_accumulator['total_latency']:
                    pipeline.hincrbyfloat(self._worker_key, f'{queue}:total_latency_time', 
                                        self.stats_accumulator['total_latency'][queue])
            
            # æ‰§è¡Œæ‰€æœ‰æ›´æ–°
            redis_start = time.time()
            await pipeline.execute()
            redis_duration = time.time() - redis_start
            
            # æ‰¹é‡è®¡ç®—å¹¶æ›´æ–°å¹³å‡å€¼ï¼ˆä½¿ç”¨å•ç‹¬çš„pipelineä»¥æé«˜æ•ˆç‡ï¼‰
            if processed_queues:
                # æ‰¹é‡è·å–æ‰€æœ‰éœ€è¦çš„æ•°æ®
                fields = []
                for queue in processed_queues:
                    fields.extend([
                        f'{queue}:total_count',
                        f'{queue}:total_processing_time',
                        f'{queue}:total_latency_time'
                    ])
                
                if fields:
                    values = await self.async_redis.hmget(self._worker_key, fields)
                    
                    # è®¡ç®—å¹³å‡å€¼å¹¶æ‰¹é‡æ›´æ–°
                    pipeline = self.async_redis.pipeline()
                    idx = 0
                    for queue in processed_queues:
                        total_count = values[idx] if values[idx] else '0'
                        total_time = values[idx + 1] if values[idx + 1] else '0'
                        total_latency = values[idx + 2] if values[idx + 2] else '0'
                        idx += 3
                        
                        if int(total_count) > 0:
                            # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
                            if float(total_time) > 0:
                                avg_time = float(total_time) / int(total_count)
                                pipeline.hset(self._worker_key, f'{queue}:avg_processing_time', f'{avg_time:.3f}')
                            
                            # è®¡ç®—å¹³å‡å»¶è¿Ÿæ—¶é—´
                            if float(total_latency) > 0:
                                avg_latency = float(total_latency) / int(total_count)
                                pipeline.hset(self._worker_key, f'{queue}:avg_latency_time', f'{avg_latency:.3f}')
                    
                    await pipeline.execute()
            
            # æ€§èƒ½ç»Ÿè®¡æ—¥å¿—
            total_duration = time.time() - start_time
            if total_duration > 0.05 or len(events) > 100:  # è¶…è¿‡50msæˆ–å¤„ç†è¶…è¿‡100ä¸ªäº‹ä»¶æ—¶è®°å½•
                logger.info(
                    f"Stats flush performance: "
                    f"events={len(events)}, "
                    f"queues={len(processed_queues)}, "
                    f"total_time={total_duration:.3f}s, "
                    f"redis_time={redis_duration:.3f}s, "
                    f"events_remaining={len(self.stats_events)}, "
                    f"dropped=0"
                )
                
        except Exception as e:
            logger.error(f"Failed to flush stats buffer: {e}")
            # å°†æœªå¤„ç†çš„äº‹ä»¶æ”¾å›åˆ—è¡¨ï¼ˆå°½åŠ›è€Œä¸ºï¼‰
            # åªæ”¾å›ååŠéƒ¨åˆ†ï¼Œé¿å…æ— é™å¾ªç¯
            self.stats_events.extend(events[len(events) - len(events) // 2:])
    
    def get_stats(self, queue: str) -> dict:
        """è·å–é˜Ÿåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ - ä»Redis Hashè¯»å–"""
        try:
            # å¦‚æœworkeræœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºç»Ÿè®¡
            if self.consumer_id is None or self._worker_key is None:
                return {
                    'success_count': 0,
                    'failed_count': 0,
                    'total_count': 0,
                    'running_tasks': 0,
                    'avg_processing_time': 0.0
                }
                
            # æ‰¹é‡è·å–è¯¥é˜Ÿåˆ—çš„æ‰€æœ‰ç»Ÿè®¡å­—æ®µ
            fields = [
                f'{queue}:success_count',
                f'{queue}:failed_count', 
                f'{queue}:total_count',
                f'{queue}:running_tasks',
                f'{queue}:avg_processing_time'
            ]
            
            values = self.redis.hmget(self._worker_key, fields)
            
            return {
                'success_count': int(values[0] or 0),
                'failed_count': int(values[1] or 0),
                'total_count': int(values[2] or 0),
                'running_tasks': int(values[3] or 0),
                'avg_processing_time': float(values[4] or 0.0)
            }
        except Exception as e:
            logger.error(f"Failed to get stats for queue {queue}: {e}")
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_count': 0,
                'running_tasks': 0,
                'avg_processing_time': 0.0
            }
    
    def _ensure_consumer_id(self):
        """ç¡®ä¿consumer_idå·²åˆ›å»º"""
        if self.consumer_id is None:
            # å»¶è¿Ÿåˆ›å»ºconsumer_id
            self.consumer_id = self._find_reusable_worker_id(self.hostname_prefix)
            if not self.consumer_id:
                # å¦‚æœæ²¡æœ‰å¯å¤ç”¨çš„ï¼Œç”Ÿæˆæ–°çš„consumer ID
                self.consumer_id = f"{self.hostname_prefix}-{uuid.uuid4().hex[:8]}-{os.getpid()}"
                logger.debug(f"Created new consumer ID: {self.consumer_id}")
            else:
                logger.debug(f"Reusing offline worker ID: {self.consumer_id}")
            
            # æ›´æ–°worker_key
            self._worker_key = f'{self.redis_prefix}:{self.worker_prefix}:{self.consumer_id}'
    
    @property
    def worker_key(self):
        """è·å–worker_keyï¼Œç¡®ä¿consumer_idå·²åˆå§‹åŒ–"""
        self._ensure_consumer_id()
        return self._worker_key
    
    def get_consumer_name(self, queue: str) -> str:
        """è·å–æ¶ˆè´¹è€…åç§°"""
        # ç¡®ä¿consumer_idå·²åˆ›å»º
        self._ensure_consumer_id()
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶å¯åŠ¨æ‰«æå™¨
        if not self._scanner_started:
            self._start_scanner()
            self._scanner_started = True
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶å¯åŠ¨ç»Ÿè®¡åˆ·æ–°å™¨
        if not self._stats_flusher_started:
            self._start_stats_flusher()
            self._stats_flusher_started = True
            
        if queue not in self.consumer_names:
            # ä¸ºæ¯ä¸ªé˜Ÿåˆ—ç”Ÿæˆå”¯ä¸€çš„consumer name
            self.consumer_names[queue] = f"{self.consumer_id}-{queue}"
            self.active_queues.add(queue)
            
            # ä¸ºè¿™ä¸ªé˜Ÿåˆ—å¯åŠ¨å¿ƒè·³è¿›ç¨‹
            if queue not in self._heartbeat_processes:
                self._start_heartbeat_process_for_queue(queue)
            
            logger.debug(f"Created consumer name for queue {queue}: {self.consumer_names[queue]}")
        return self.consumer_names[queue]
    
    def record_group_info(self, queue: str, task_name: str, group_name: str, consumer_name: str):
        """è®°å½•taskçš„groupä¿¡æ¯åˆ°worker hashè¡¨
        
        Args:
            queue: é˜Ÿåˆ—å
            task_name: ä»»åŠ¡å
            group_name: consumer groupåç§°
            consumer_name: consumeråç§°
        """
        try:
            # ç¡®ä¿worker_keyå·²åˆå§‹åŒ–
            if not self._worker_key:
                self._ensure_consumer_id()
                if not self._worker_key:
                    logger.warning("Cannot record group info: worker_key not initialized")
                    return
            
            # æ„å»ºgroupä¿¡æ¯
            import json
            group_info = {
                'queue': queue,
                'task_name': task_name,
                'group_name': group_name,
                'consumer_name': consumer_name,
                'stream_key': f"{self.redis_prefix}:QUEUE:{queue}"
            }
            
            # å°†groupä¿¡æ¯å­˜å‚¨åˆ°workerçš„hashä¸­
            # ä½¿ç”¨ group_info:{group_name} ä½œä¸ºfield
            field_name = f"group_info:{group_name}"
            self.redis.hset(
                self._worker_key,
                field_name,
                json.dumps(group_info)
            )
            
            logger.debug(f"Recorded group info for task {task_name}: {group_info}")
            
        except Exception as e:
            logger.error(f"Error recording task group info: {e}")
    
    async def record_group_info_async(self, queue: str, task_name: str, group_name: str, consumer_name: str):
        """å¼‚æ­¥è®°å½•taskçš„groupä¿¡æ¯åˆ°worker hashè¡¨
        
        Args:
            queue: é˜Ÿåˆ—å
            task_name: ä»»åŠ¡å
            group_name: consumer groupåç§°
            consumer_name: consumeråç§°
        """
        try:
            # ç¡®ä¿worker_keyå·²åˆå§‹åŒ–
            if not self._worker_key:
                self._ensure_consumer_id()
                if not self._worker_key:
                    logger.warning("Cannot record group info: worker_key not initialized")
                    return
            
            # æ„å»ºgroupä¿¡æ¯
            import json
            group_info = {
                'queue': queue,
                'task_name': task_name,
                'group_name': group_name,
                'consumer_name': consumer_name,
                'stream_key': f"{self.redis_prefix}:QUEUE:{queue}"
            }
            
            # å°†groupä¿¡æ¯å­˜å‚¨åˆ°workerçš„hashä¸­
            # ä½¿ç”¨ group_info:{group_name} ä½œä¸ºfield
            field_name = f"group_info:{group_name}"
            await self.async_redis.hset(
                self._worker_key,
                field_name,
                json.dumps(group_info)
            )
            
            logger.debug(f"Recorded group info for task {task_name}: {group_info}")
            
        except Exception as e:
            logger.error(f"Error recording task group info: {e}")
    
    def _ensure_worker_initialized(self):
        """ç¡®ä¿workerå·²åˆå§‹åŒ–"""
        if self.consumer_id is None:
            self._ensure_consumer_id()
        if self._worker_key is None:
            self._worker_key = f"{self.redis_prefix}:{self.worker_prefix}:{self.consumer_id}"
    
    def _start_heartbeat_process_for_queue(self, queue: str):
        """ä¸ºç‰¹å®šé˜Ÿåˆ—å¯åŠ¨å¿ƒè·³è¿›ç¨‹"""
        # åªéœ€è¦å¯åŠ¨ä¸€æ¬¡å¿ƒè·³è¿›ç¨‹ï¼Œä¸éœ€è¦ä¸ºæ¯ä¸ªé˜Ÿåˆ—éƒ½å¯åŠ¨
        if self._heartbeat_process_manager is not None:
            # å¿ƒè·³è¿›ç¨‹å·²ç»åœ¨è¿è¡Œï¼Œåªéœ€è¦è®°å½•è¿™ä¸ªé˜Ÿåˆ—
            self._heartbeat_processes[queue] = True
            return
        logger.debug('å¯åŠ¨å¿ƒè·³è¿›ç¨‹')
        # ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºå¿ƒè·³è¿›ç¨‹ç®¡ç†å™¨
        if self._heartbeat_process_manager is None:
            # è·å–Redis URL
            redis_url = None
            if hasattr(self.redis.connection_pool, 'connection_kwargs'):
                redis_url = self.redis.connection_pool.connection_kwargs.get('url')
            
            if not redis_url:
                # æ„é€ Redis URL
                connection_kwargs = self.redis.connection_pool.connection_kwargs
                host = connection_kwargs.get('host', 'localhost')
                port = connection_kwargs.get('port', 6379)
                db = connection_kwargs.get('db', 0)
                password = connection_kwargs.get('password')
                if password:
                    redis_url = f"redis://:{password}@{host}:{port}/{db}"
                else:
                    redis_url = f"redis://{host}:{port}/{db}"
            
            self._heartbeat_process_manager = HeartbeatProcessManager(
                redis_url=redis_url,
                consumer_id=self.consumer_id,
                heartbeat_interval=self.heartbeat_interval,
                heartbeat_timeout=self.heartbeat_timeout
            )
        
        # ç¡®ä¿worker keyå­˜åœ¨å¹¶åˆå§‹åŒ–
        self._ensure_worker_initialized()
        
        # åˆå§‹åŒ–workerä¿¡æ¯ï¼ˆå¿ƒè·³è¿›ç¨‹åªè´Ÿè´£æ›´æ–°last_heartbeatï¼‰
        current_time = time.time()
        import socket
        try:
            hostname = socket.gethostname()
            if not hostname or hostname == 'localhost':
                hostname = socket.gethostbyname(socket.gethostname())
        except:
            hostname = os.environ.get('HOSTNAME', 'unknown')
        
        # è®¾ç½®åˆå§‹workerä¿¡æ¯
        worker_info = {
            'consumer_id': self.consumer_id,
            'host': hostname,
            'pid': str(os.getpid()),
            'created_at': str(current_time),
            'last_heartbeat': str(current_time),
            'is_alive': 'true',
            'heartbeat_timeout': str(self.heartbeat_timeout),
            'queues': ','.join(sorted(self.configured_queues)) if self.configured_queues else queue,
            'messages_transferred': 'false'  # æ–°workerçš„æ¶ˆæ¯æœªè½¬ç§»
        }
        
        # ä½¿ç”¨hsetç›´æ¥è®¾ç½®ï¼Œç¡®ä¿æ•°æ®å†™å…¥
        self.redis.hset(self._worker_key, mapping=worker_info)
        # åŒæ—¶æ·»åŠ åˆ° sorted set
        self.redis.zadd(f"{self.redis_prefix}:ACTIVE_WORKERS", {self.consumer_id: current_time})
        logger.debug(f"Initialized worker {self.consumer_id} with key {self._worker_key}")
        
        self._heartbeat_process_manager.add_queue(queue, self._worker_key)
        self._heartbeat_processes[queue] = True
        # logger.debug(f"Started heartbeat process for queue {queue}")
    
    def _start_scanner(self):
        """å¯åŠ¨æ‰«æå™¨åç¨‹"""
        try:
            loop = asyncio.get_running_loop()
            self._scanner_task = loop.create_task(self._scanner_loop())
            # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰«æï¼Œæ¸…ç†å¯èƒ½å­˜åœ¨çš„æ­»äº¡worker
            loop.create_task(self._immediate_scan())
            # logger.debug("Started heartbeat scanner coroutine")
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œæ ‡è®°ä¸ºéœ€è¦å¯åŠ¨
            logger.debug("No running event loop, scanner will be started when async context is available")
            self._scanner_needs_start = True
    
    async def _immediate_scan(self):
        """å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰«æï¼ˆåç¨‹ç‰ˆæœ¬ï¼‰"""
        try:
            # logger.debug("Performing immediate scan for dead workers...")
            await self._perform_scan()
            # logger.debug("Immediate scan completed")
        except Exception as e:
            logger.error(f"Error in immediate scan: {e}")
    
    
    async def _perform_scan(self):
        """æ‰§è¡Œæ‰«ææ“ä½œ - ä½¿ç”¨é«˜æ•ˆçš„ O(log N) ç®—æ³•"""
        try:
            # ä½¿ç”¨ Worker æ‰«æå™¨
            timeout_workers = await self.scanner.scan_timeout_workers()
            
            if timeout_workers:
                for worker_info in timeout_workers:
                    await self._mark_worker_offline(
                        worker_info['worker_key'],
                        worker_info['worker_data']
                    )
            return
        except Exception as e:
            logger.error(f"Scanner error: {e}")
        
        # åŸå§‹æ‰«æé€»è¾‘ä½œä¸ºåå¤‡
        current_time = time.time()
        # æ³¨æ„ï¼šä¸å†ä½¿ç”¨å…¨å±€çš„heartbeat_timeoutï¼Œè€Œæ˜¯ä½¿ç”¨æ¯ä¸ªworkerè‡ªå·±çš„å€¼
        
        try:
            # æ‰«ææ‰€æœ‰worker hashé”®
            pattern = f"{self.redis_prefix}:{self.worker_prefix}:*"
            worker_keys = []
            cursor = 0
            
            # ä½¿ç”¨SCANè¿­ä»£è·å–æ‰€æœ‰workeré”®ï¼Œæ’é™¤HISTORYç›¸å…³çš„é”®
            while True:
                cursor, keys = await self.async_redis.scan(cursor, match=pattern, count=100)
                # è¿‡æ»¤æ‰HISTORYç›¸å…³çš„é”®ã€é”é”®å’ŒREUSINGæ ‡è®°é”®
                for key in keys:
                    # key æ˜¯ bytes ç±»å‹ï¼Œéœ€è¦è§£ç æˆ–ä½¿ç”¨ bytes è¿›è¡Œæ¯”è¾ƒ
                    key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                    if ':HISTORY:' not in key_str and ':REUSE:LOCK' not in key_str and ':REUSING' not in key_str:
                        worker_keys.append(key)
                if cursor == 0:
                    break
            
            # åŒæ—¶æ¸…ç†æ®‹ç•™çš„recovery consumer
            await self._cleanup_stale_recovery_consumers()
            
            if not worker_keys:
                logger.debug("No worker keys found")
                return
            
            timeout_workers = []
            
            # æ£€æŸ¥æ¯ä¸ªworkerçš„å¿ƒè·³æ—¶é—´
            for worker_key in worker_keys:
                try:
                    # å…ˆæ£€æŸ¥keyçš„ç±»å‹ï¼ˆç°åœ¨åº”è¯¥ä¸éœ€è¦äº†ï¼Œä½†ä¿ç•™ä½œä¸ºå®‰å…¨æ£€æŸ¥ï¼‰
                    key_type = await self.async_redis.type(worker_key)
                    if key_type != 'hash':
                        logger.warning(f"Worker key {worker_key} is not a hash, type: {key_type}, skipping")
                        continue
                    
                    worker_data = await self.async_redis.hgetall(worker_key)
                    if not worker_data:
                        continue
                        
                    last_heartbeat = float(worker_data.get('last_heartbeat', 0))
                    consumer_id = worker_data.get('consumer_id')
                    is_alive = worker_data.get('is_alive', 'true').lower() == 'true'
                    
                    # è·å–è¯¥workerè‡ªå·±çš„heartbeat_timeout
                    # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                    worker_heartbeat_timeout = float(worker_data.get('heartbeat_timeout', self.heartbeat_timeout))
                    
                    # è·³è¿‡è‡ªå·±ï¼ˆå¦‚æœconsumer_idå·²åˆå§‹åŒ–ï¼‰
                    if self.consumer_id and consumer_id == self.consumer_id:
                        continue
                    
                    # ä½¿ç”¨è¯¥workerè‡ªå·±çš„è¶…æ—¶æ—¶é—´æ¥åˆ¤æ–­
                    worker_timeout_threshold = current_time - worker_heartbeat_timeout
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¿™ä¸ªworker
                    # åªå¤„ç†å¿ƒè·³è¶…æ—¶çš„æ´»è·ƒworker
                    needs_processing = False
                    
                    if is_alive and last_heartbeat < worker_timeout_threshold:
                        # å¿ƒè·³è¶…æ—¶çš„æ´»è·ƒworker
                        logger.debug(f"Worker {consumer_id} timeout detected: "
                                  f"last_heartbeat={last_heartbeat}, "
                                  f"timeout={worker_heartbeat_timeout}s, "
                                  f"threshold={worker_timeout_threshold}")
                        needs_processing = True
                    
                    if needs_processing:
                        timeout_workers.append((worker_key, worker_data))
                        
                except (ValueError, TypeError) as e:
                    logger.error(f"Error parsing worker data from {worker_key}: {e}")
                    continue
            
            if timeout_workers:
                logger.debug(f"Found {len(timeout_workers)} timeout workers")
                
                for worker_key, worker_data in timeout_workers:
                    consumer_id = worker_data.get('consumer_id')
                    # queues = worker_data.get('queues', '').split(',') if worker_data.get('queues') else []
                    
                    # ä½¿ç”¨RedisåŸç”Ÿåˆ†å¸ƒå¼é”æ¥é¿å…å¤šä¸ªscanneråŒæ—¶å¤„ç†åŒä¸€ä¸ªworker
                    lock_key = f"{self.redis_prefix}:SCANNER:LOCK:{consumer_id}"
                    lock_ttl = max(1, int(self.scan_interval * 2))  # ç¡®ä¿æ˜¯æ•´æ•°ï¼Œæœ€å°1ç§’
                    
                    # åˆ›å»ºRedisé”
                    lock = AsyncLock(
                        self.async_redis,
                        lock_key,
                        timeout=lock_ttl,
                        blocking=False  # ä¸é˜»å¡ï¼Œç›´æ¥è·³è¿‡
                    )
                    
                    # å°è¯•è·å–é”
                    if not await lock.acquire():
                        logger.debug(f"Another scanner is processing worker {consumer_id}, skipping")
                        continue
                    
                    try:
                        # å†æ¬¡æ£€æŸ¥workeræ˜¯å¦çœŸçš„è¶…æ—¶ï¼ˆé¿å…ç«æ€æ¡ä»¶ï¼‰
                        current_heartbeat = await self.async_redis.hget(worker_key, 'last_heartbeat')
                        if current_heartbeat and float(current_heartbeat) >= timeout_threshold:
                            logger.debug(f"Worker {consumer_id} is now alive, skipping")
                            continue
                        
                        logger.debug(f"Processing timeout worker: {consumer_id}")
                        # åªæ ‡è®°workerä¸ºç¦»çº¿
                        await self._mark_worker_offline(worker_key, worker_data)
                        
                    except Exception as e:
                        logger.error(f"Error processing timeout worker {consumer_id}: {e}")
                    finally:
                        # é‡Šæ”¾é”
                        await lock.release()
                        
        except Exception as e:
            logger.error(f"Error in scanner: {e}")
    
    async def _mark_worker_offline(self, worker_key: str, worker_data: dict):
        """åªæ ‡è®°workerä¸ºç¦»çº¿çŠ¶æ€"""
        consumer_id = worker_data.get('consumer_id')
        
        try:
            current_time = time.time()
            is_alive = worker_data.get('is_alive', 'true').lower() == 'true'
            
            # åªæœ‰ä¹‹å‰æ˜¯åœ¨çº¿çš„workeræ‰éœ€è¦åˆå§‹åŒ–æ¶ˆæ¯è½¬ç§»çŠ¶æ€
            if is_alive:
                # æ ‡è®°workerä¸ºç¦»çº¿çŠ¶æ€ï¼Œå¹¶è®¾ç½®æ¶ˆæ¯è½¬ç§»çŠ¶æ€ä¸ºæœªè½¬ç§»
                await self.async_redis.hset(worker_key, mapping={
                    'is_alive': 'false',
                    'offline_time': str(current_time),
                    'shutdown_reason': 'heartbeat_timeout',
                    'messages_transferred': 'false'  # åˆå§‹çŠ¶æ€ï¼šæ¶ˆæ¯æœªè½¬ç§»
                })
                logger.debug(f"Marked worker {consumer_id} as offline with messages_transferred=false")
            else:
                # å·²ç»æ˜¯ç¦»çº¿çŠ¶æ€çš„workerï¼Œåªæ›´æ–°ç¦»çº¿æ—¶é—´
                await self.async_redis.hset(worker_key, 'offline_time', str(current_time))
                logger.debug(f"Worker {consumer_id} was already offline, updated offline_time")
                    
        except Exception as e:
            logger.error(f"Error marking worker {consumer_id} offline: {e}")
    
    
    async def _scanner_loop(self):
        """æ‰«æè¶…æ—¶workerçš„å¾ªç¯ï¼ˆåç¨‹ç‰ˆæœ¬ï¼‰"""
        while not self._scanner_stop.is_set():
            try:
                await self._perform_scan()
                await asyncio.sleep(self.scan_interval)
            except Exception as e:
                logger.error(f"Error in scanner loop: {e}")
                await asyncio.sleep(5)  # é”™è¯¯æ—¶ç­‰å¾…5ç§’åé‡è¯•
    
    
    def _start_stats_flusher(self):
        """å¯åŠ¨ç»Ÿè®¡åˆ·æ–°å™¨åç¨‹"""
        try:
            loop = asyncio.get_running_loop()
            self._stats_flusher_task = loop.create_task(self._stats_flusher_loop())
            logger.debug("Started stats flusher coroutine")
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œæ ‡è®°ä¸ºéœ€è¦å¯åŠ¨
            logger.debug("No running event loop for stats flusher, will be started when async context is available")
            self._stats_flusher_needs_start = True
    
    async def _stats_flusher_loop(self):
        """ç»Ÿè®¡åˆ·æ–°å¾ªç¯ï¼ˆåç¨‹ç‰ˆæœ¬ï¼‰"""
        while not self._stats_flusher_stop.is_set():
            try:
                # å‘¨æœŸæ€§åˆ·æ–°ç»Ÿè®¡ç¼“å†²åŒº
                if len(self.stats_events) > 0:
                    # ç›´æ¥è°ƒç”¨å¼‚æ­¥çš„ flush_stats_buffer
                    await self.flush_stats_buffer()
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªåˆ·æ–°å‘¨æœŸ
                await asyncio.sleep(self.stats_flush_interval)
            except Exception as e:
                logger.error(f"Error in stats flusher loop: {e}")
                await asyncio.sleep(1)  # é”™è¯¯æ—¶ç­‰å¾…1ç§’åé‡è¯•
    
    
    
    def _cleanup_stream_consumer(self, queue: str, consumer_name: str):
        """ä»Redis Streamæ¶ˆè´¹è€…ç»„ä¸­åˆ é™¤consumer"""
        try:
            # åˆ é™¤æ¶ˆè´¹è€…ï¼ˆè¿™ä¼šé˜»æ­¢å®ƒé‡æ–°åŠ å…¥åç»§ç»­æ¶ˆè´¹æ¶ˆæ¯ï¼‰
            prefixed_queue = self.get_prefixed_queue_name(queue)
            result = self.redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
            if result > 0:
                logger.debug(f"Deleted stream consumer {consumer_name} from group {queue}")
            else:
                logger.debug(f"Stream consumer {consumer_name} was not found in group {queue}")
        except Exception as e:
            logger.error(f"Error deleting stream consumer {consumer_name}: {e}")

    async def _handle_dead_worker(self, queue: str, worker_info: dict, worker_data: bytes):
        """å¤„ç†æ­»äº¡çš„workerï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        consumer_name = worker_info.get('consumer_name', 'unknown')
        
        # ä½¿ç”¨RedisåŸç”Ÿåˆ†å¸ƒå¼é”æ¥é¿å…å¤šä¸ªscanneråŒæ—¶å¤„ç†åŒä¸€ä¸ªconsumer
        consumer_lock_key = f"{self.redis_prefix}:CONSUMER:LOCK:{consumer_name}"
        consumer_lock_ttl = 30  # 30ç§’é”è¶…æ—¶
        
        # åˆ›å»ºRedisé”
        lock = AsyncLock(
            self.async_redis,
            consumer_lock_key,
            timeout=consumer_lock_ttl,
            blocking=False  # ä¸é˜»å¡ï¼Œç›´æ¥è¿”å›
        )
        
        # å°è¯•è·å–é”
        if not await lock.acquire():
            logger.debug(f"Another scanner is handling consumer {consumer_name}, skipping")
            return
        
        try:
            heartbeat_key = f"{self.heartbeat_key_prefix}{queue}"
            
            # å†æ¬¡æ£€æŸ¥workeræ˜¯å¦çœŸçš„è¶…æ—¶ï¼ˆé¿å…ç«æ€æ¡ä»¶ï¼‰
            current_score = await self.async_redis.zscore(heartbeat_key, worker_data)
            if current_score and time.time() - current_score < self.heartbeat_timeout:
                logger.debug(f"Worker {consumer_name} is now alive, skipping")
                return
            
            # ä»æœ‰åºé›†åˆä¸­åˆ é™¤æ­»äº¡çš„workerï¼ˆä½¿ç”¨åŸå§‹çš„worker_dataï¼‰
            removed = await self.async_redis.zrem(heartbeat_key, worker_data)
            if removed:
                logger.debug(f"Removed dead worker {consumer_name} from heartbeat set for queue {queue}")
                
                # é‡ç½®è¯¥consumerçš„pendingæ¶ˆæ¯
                await self._reset_consumer_pending_messages(queue, consumer_name)
            else:
                logger.debug(f"Worker {consumer_name} already removed by another scanner")
            
        except Exception as e:
            logger.error(f"Error handling dead worker {consumer_name}: {e}")
        finally:
            # é‡Šæ”¾é”
            await lock.release()
    
    async def _reset_consumer_pending_messages(self, queue: str, consumer_name: str):
        """é‡ç½®æŒ‡å®šconsumerçš„pendingæ¶ˆæ¯ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿ä»»åŠ¡ä¸ä¼šä¸¢å¤±ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        recovery_lock_key = f"RECOVERY:{queue}:{consumer_name}"
        max_retries = 3
        
        try:
            # ä½¿ç”¨RedisåŸç”Ÿåˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘æ¢å¤åŒä¸€ä¸ªconsumer
            recovery_lock = AsyncLock(
                self.async_redis,
                recovery_lock_key,
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
                blocking=False  # ä¸é˜»å¡
            )
            
            # å°è¯•è·å–é”
            if not await recovery_lock.acquire():
                logger.warning(f"Another process is recovering messages for {consumer_name}, skipping")
                return
            
            # é¦–å…ˆè·å–è¯¥consumerçš„æ‰€æœ‰pendingæ¶ˆæ¯
            consumer_messages = []
            try:
                # åˆ†æ‰¹è·å–è¯¥consumerçš„æ‰€æœ‰pendingæ¶ˆæ¯
                batch_size = 1000
                last_id = '-'
                
                while True:
                    # è·å–ä¸€æ‰¹pendingæ¶ˆæ¯
                    prefixed_queue = self.get_prefixed_queue_name(queue)
                    pending_batch = await self.async_redis.xpending_range(
                        prefixed_queue, prefixed_queue,
                        min=last_id, max='+',
                        count=batch_size
                    )
                    
                    if not pending_batch:
                        break
                    
                    # è¿‡æ»¤å‡ºå±äºè¯¥consumerçš„æ¶ˆæ¯
                    for msg in pending_batch:
                        msg_consumer = msg['consumer']
                        # å¤„ç†bytesç±»å‹
                        if isinstance(msg_consumer, bytes):
                            msg_consumer = msg_consumer.decode('utf-8')
                        if msg_consumer == consumer_name:
                            consumer_messages.append(msg)
                    
                    # å¦‚æœè·å–çš„æ¶ˆæ¯æ•°å°äºbatch_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰æ¶ˆæ¯
                    if len(pending_batch) < batch_size:
                        break
                    
                    # æ›´æ–°last_idä¸ºæœ€åä¸€æ¡æ¶ˆæ¯çš„IDï¼Œç”¨äºä¸‹ä¸€æ‰¹æŸ¥è¯¢
                    last_id = pending_batch[-1]['message_id']
                
                if not consumer_messages:
                    logger.debug(f"No pending messages for consumer {consumer_name}")
                    # ä»ç„¶å°è¯•åˆ é™¤consumer
                    try:
                        prefixed_queue = self.get_prefixed_queue_name(queue)
                        await self.async_redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
                    except:
                        pass
                    return
                
                logger.debug(f"Found {len(consumer_messages)} pending messages for dead consumer {consumer_name}")
                
                # è·å–æ¶ˆæ¯IDåˆ—è¡¨
                message_ids = [msg['message_id'] for msg in consumer_messages]
                
                # ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„consumeræ¥claimè¿™äº›æ¶ˆæ¯
                temp_consumer = f"recovery-{consumer_name}-{uuid.uuid4().hex[:8]}"
                
                # è®°å½•æ¢å¤å¼€å§‹
                await self.async_redis.hset(f"RECOVERY:STATUS:{temp_consumer}", mapping={
                    'start_time': str(time.time()),
                    'total_messages': str(len(message_ids)),
                    'queue': queue,
                    'original_consumer': consumer_name,
                    'status': 'in_progress'
                })
                await self.async_redis.expire(f"RECOVERY:STATUS:{temp_consumer}", 3600)  # 1å°æ—¶è¿‡æœŸ
                
                # åˆ†æ‰¹å¤„ç†æ¶ˆæ¯ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
                recovered_count = 0
                failed_messages = []
                
                for i in range(0, len(message_ids), 100):
                    batch = message_ids[i:i+100]
                    
                    for retry in range(max_retries):
                        try:
                            # ä½¿ç”¨pipelineç¡®ä¿åŸå­æ€§
                            pipeline = self.async_redis.pipeline()
                            
                            # 1. Claimæ¶ˆæ¯åˆ°ä¸´æ—¶consumer
                            prefixed_queue = self.get_prefixed_queue_name(queue)
                            claimed = await self.async_redis.xclaim(
                                prefixed_queue, prefixed_queue,
                                temp_consumer,
                                min_idle_time=0,
                                message_ids=batch,
                                force=True
                            )
                            
                            if claimed:
                                # 2. å‡†å¤‡æ‰¹é‡æ·»åŠ çš„æ•°æ®
                                messages_to_add = []
                                claimed_ids = []
                                
                                for msg_id, msg_data in claimed:
                                    messages_to_add.append((msg_data, msg_id))
                                    claimed_ids.append(msg_id)
                                
                                # 3. åœ¨pipelineä¸­æ‰§è¡Œæ‰€æœ‰æ“ä½œ
                                for msg_data, original_id in messages_to_add:
                                    # æ·»åŠ æ¢å¤æ ‡è®°
                                    msg_data['_recovered_from'] = consumer_name
                                    msg_data['_recovery_time'] = str(time.time())
                                    msg_data['_original_id'] = original_id
                                    pipeline.xadd(prefixed_queue, msg_data)
                                
                                # 4. ACKåŸå§‹æ¶ˆæ¯
                                pipeline.xack(prefixed_queue, prefixed_queue, *claimed_ids)
                                
                                # 5. æ‰§è¡Œpipeline
                                results = await pipeline.execute()
                                
                                # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸ
                                new_ids = [r for r in results[:-1]]  # å‰é¢çš„éƒ½æ˜¯xaddçš„ç»“æœ
                                if all(new_ids):
                                    recovered_count += len(claimed_ids)
                                    logger.debug(f"Successfully recovered batch of {len(claimed_ids)} messages")
                                    break
                                else:
                                    logger.error(f"Pipeline execution failed for some messages, retry {retry + 1}/{max_retries}")
                            else:
                                # æ²¡æœ‰æˆåŠŸclaimåˆ°æ¶ˆæ¯ï¼Œå¯èƒ½å·²ç»è¢«å…¶ä»–è¿›ç¨‹å¤„ç†
                                logger.warning(f"No messages claimed from batch, they may have been processed")
                                break
                                
                        except Exception as e:
                            logger.error(f"Error recovering batch (retry {retry + 1}/{max_retries}): {e}")
                            if retry == max_retries - 1:
                                failed_messages.extend(batch)
                    
                    # æ›´æ–°æ¢å¤è¿›åº¦
                    if (i + len(batch)) % 1000 == 0 or i + len(batch) >= len(message_ids):
                        await self.async_redis.hset(f"RECOVERY:STATUS:{temp_consumer}", 
                                      'recovered_count', str(recovered_count))
                
                # è®°å½•æ¢å¤ç»“æœ
                await self.async_redis.hset(f"RECOVERY:STATUS:{temp_consumer}", mapping={
                    'end_time': str(time.time()),
                    'recovered_count': str(recovered_count),
                    'failed_count': str(len(failed_messages)),
                    'status': 'completed' if not failed_messages else 'completed_with_errors'
                })
                
                logger.debug(f"Recovery completed: {recovered_count}/{len(message_ids)} messages recovered from {consumer_name}")
                
                if failed_messages:
                    logger.error(f"Failed to recover {len(failed_messages)} messages: {failed_messages[:10]}...")
                    # å°†å¤±è´¥çš„æ¶ˆæ¯IDè®°å½•åˆ°Redisä¾›åç»­åˆ†æ
                    await self.async_redis.rpush(f"RECOVERY:FAILED:{queue}", *[str(mid) for mid in failed_messages[:100]])
                    await self.async_redis.expire(f"RECOVERY:FAILED:{queue}", 86400)  # ä¿ç•™24å°æ—¶
                
            except Exception as e:
                logger.error(f"Error getting pending messages: {e}")
                await self.async_redis.hset(f"RECOVERY:STATUS:{temp_consumer}", mapping={
                    'error': str(e),
                    'status': 'failed'
                })
            
            # æ¸…ç†ä¸´æ—¶consumerï¼ˆå¦‚æœåˆ›å»ºäº†çš„è¯ï¼‰
            if 'temp_consumer' in locals():
                try:
                    prefixed_queue = self.get_prefixed_queue_name(queue)
                    # ç¡®ä¿ä¸´æ—¶consumeræ²¡æœ‰æ–°çš„pendingæ¶ˆæ¯
                    temp_pending = await self.async_redis.xpending(prefixed_queue, prefixed_queue)
                    
                    # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    if temp_pending and isinstance(temp_pending, dict) and temp_pending.get('consumers'):
                        for consumer_info in temp_pending['consumers']:
                            # å¤„ç†ä¸åŒçš„consumer_infoæ ¼å¼
                            if isinstance(consumer_info, dict):
                                # æ–°æ ¼å¼ï¼š{'name': 'consumer_name', 'pending': count}
                                consumer_name_check = consumer_info.get('name', '')
                                pending_count = consumer_info.get('pending', 0)
                            elif isinstance(consumer_info, (list, tuple)) and len(consumer_info) >= 2:
                                # æ—§æ ¼å¼ï¼š['consumer_name', count]
                                consumer_name_check = consumer_info[0]
                                pending_count = consumer_info[1]
                            else:
                                continue
                            
                            # å¤„ç†bytesç±»å‹
                            if isinstance(consumer_name_check, bytes):
                                consumer_name_check = consumer_name_check.decode('utf-8')
                            
                            if consumer_name_check == temp_consumer and int(pending_count) > 0:
                                logger.warning(f"Temp consumer {temp_consumer} still has {pending_count} pending messages")
                                # é€’å½’æ¢å¤ä¸´æ—¶consumerçš„æ¶ˆæ¯
                                await self._reset_consumer_pending_messages(queue, temp_consumer)
                    
                    # åˆ é™¤ä¸´æ—¶consumer
                    await self.async_redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, temp_consumer)
                    logger.debug(f"Cleaned up temp consumer {temp_consumer}")
                except Exception as e:
                    logger.error(f"Error cleaning up temp consumer: {e}")
            
            # æœ€ååˆ é™¤æ­»äº¡çš„consumer
            try:
                prefixed_queue = self.get_prefixed_queue_name(queue)
                await self.async_redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
                logger.debug(f"Deleted consumer {consumer_name}")
            except:
                pass
                        
        except Exception as e:
            logger.error(f"Error resetting pending messages for {consumer_name}: {e}")
        finally:
            # é‡Šæ”¾æ¢å¤é”
            await self.async_redis.delete(recovery_lock_key)
    
    async def _cleanup_stale_recovery_consumers(self):
        """æ¸…ç†æ®‹ç•™çš„recovery consumerï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # è·å–æ‰€æœ‰é˜Ÿåˆ—
            queues_pattern = f"{self.redis_prefix}:*"
            all_keys = []
            cursor = 0
            
            while True:
                cursor, keys = await self.async_redis.scan(cursor, match=queues_pattern, count=100)
                all_keys.extend(keys)
                if cursor == 0:
                    break
            
            # ç­›é€‰å‡ºstreamç±»å‹çš„é˜Ÿåˆ—
            stream_queues = []
            for key in all_keys:
                try:
                    if await self.async_redis.type(key) == 'stream':
                        stream_queues.append(key)
                except:
                    continue
            
            cleaned_count = 0
            for queue in stream_queues:
                try:
                    # è·³è¿‡éé˜Ÿåˆ—çš„streamï¼ˆæ¯”å¦‚å¯èƒ½çš„å…¶ä»–ç”¨é€”çš„streamï¼‰
                    if ':QUEUE:' not in queue:
                        continue
                    
                    # è·å–è¯¥é˜Ÿåˆ—çš„æ‰€æœ‰consumerä¿¡æ¯
                    # åœ¨jettaskä¸­ï¼Œconsumer groupåç§°å’Œstreamåç§°ç›¸åŒï¼ˆéƒ½æ˜¯å¸¦å‰ç¼€çš„ï¼‰
                    try:
                        pending_info = await self.async_redis.xpending(queue, queue)
                    except Exception as xpending_error:
                        # å¦‚æœxpendingå¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºconsumer groupä¸å­˜åœ¨
                        logger.debug(f"xpending failed for {queue}: {xpending_error}")
                        continue
                    
                    # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    if not pending_info:
                        continue
                    
                    # å¦‚æœè¿”å›çš„æ˜¯æ•°å­—0ï¼Œè¯´æ˜æ²¡æœ‰pendingæ¶ˆæ¯
                    if isinstance(pending_info, int) and pending_info == 0:
                        continue
                    
                    # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡
                    if not isinstance(pending_info, dict):
                        logger.debug(f"Unexpected xpending response for {queue}: {type(pending_info)} - {pending_info}")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰consumerså­—æ®µ
                    consumers = pending_info.get('consumers')
                    if not consumers:
                        continue
                    
                    # æ£€æŸ¥recovery consumer
                    for consumer_info in consumers:
                        # å¤„ç†ä¸åŒçš„consumer_infoæ ¼å¼
                        if isinstance(consumer_info, dict):
                            # æ–°æ ¼å¼ï¼š{'name': 'consumer_name', 'pending': count}
                            consumer_name = consumer_info.get('name', '')
                            pending_count = consumer_info.get('pending', 0)
                        elif isinstance(consumer_info, (list, tuple)) and len(consumer_info) >= 2:
                            # æ—§æ ¼å¼ï¼š['consumer_name', count]
                            consumer_name = consumer_info[0]
                            pending_count = consumer_info[1]
                        else:
                            logger.warning(f"Unexpected consumer info format: {consumer_info}")
                            continue
                        
                        # å¤„ç†bytesç±»å‹
                        if isinstance(consumer_name, bytes):
                            consumer_name = consumer_name.decode('utf-8')
                        
                        # ç¡®ä¿pending_countæ˜¯æ•´æ•°
                        try:
                            pending_count = int(pending_count)
                        except (ValueError, TypeError):
                            logger.warning(f"Invalid pending count for {consumer_name}: {pending_count}")
                            continue
                        
                        # è¯†åˆ«recovery consumer
                        if consumer_name.startswith('recovery-'):
                            
                            # æ£€æŸ¥recoveryçŠ¶æ€
                            status_key = f"RECOVERY:STATUS:{consumer_name}"
                            status = await self.async_redis.hget(status_key, 'status')
                            
                            # å¦‚æœçŠ¶æ€å·²å®Œæˆæˆ–ä¸å­˜åœ¨çŠ¶æ€ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯æ—§çš„æ®‹ç•™ï¼‰
                            if not status or status in ['completed', 'completed_with_errors', 'failed']:
                                # å¦‚æœè¿˜æœ‰pendingæ¶ˆæ¯ï¼Œå…ˆæ¢å¤å®ƒä»¬
                                if pending_count > 0:
                                    logger.warning(f"Found stale recovery consumer {consumer_name} with {pending_count} pending messages")
                                    # é€’å½’æ¢å¤è¿™äº›æ¶ˆæ¯
                                    queue_name = queue.split(':', 1)[-1] if ':' in queue else queue
                                    await self._reset_consumer_pending_messages(queue_name, consumer_name)
                                else:
                                    # æ²¡æœ‰pendingæ¶ˆæ¯ï¼Œç›´æ¥åˆ é™¤
                                    try:
                                        await self.async_redis.execute_command('XGROUP', 'DELCONSUMER', queue, queue, consumer_name)
                                        logger.debug(f"Cleaned up stale recovery consumer {consumer_name}")
                                        cleaned_count += 1
                                    except Exception as e:
                                        logger.error(f"Failed to delete recovery consumer {consumer_name}: {e}")
                        
                except Exception as e:
                    import traceback
                    logger.error(f"Error cleaning recovery consumers in queue {queue}: {e}")
                    logger.error(f"Traceback:\n{traceback.format_exc()}")
            
            if cleaned_count > 0:
                logger.debug(f"Cleaned up {cleaned_count} stale recovery consumers")
                
        except Exception as e:
            logger.error(f"Error in cleanup_stale_recovery_consumers: {e}")
    
    def is_heartbeat_timeout(self) -> bool:
        """æ£€æŸ¥å¿ƒè·³æ˜¯å¦å·²è¶…æ—¶"""
        # ç»™æ–°å¯åŠ¨çš„workerä¸€ä¸ªå®½é™æœŸï¼ˆ15ç§’ï¼‰ï¼Œé¿å…è¯¯åˆ¤
        if hasattr(self, '_startup_time'):
            if time.time() - self._startup_time < 15:
                return False
        
        if self._heartbeat_process_manager:
            return self._heartbeat_process_manager.is_heartbeat_timeout()
        return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # å¦‚æœconsumer_idä»æœªè¢«åˆ›å»ºï¼Œè¯´æ˜è¿™ä¸ªå®ä¾‹ä»æœªçœŸæ­£è¿è¡Œ
        if self.consumer_id is None:
            logger.debug("HeartbeatConsumerStrategy cleanup: never initialized, skipping")
            return
            
        logger.debug(f"Cleaning up heartbeat consumer {self.consumer_id}")
        
        # å…ˆåœæ­¢æ‰€æœ‰å¿ƒè·³è¿›ç¨‹ï¼Œé¿å…æ–°çš„æ•°æ®äº§ç”Ÿ
        if self._heartbeat_process_manager and self._heartbeat_process_manager.heartbeat_process:
            logger.debug("Stopping heartbeat processes...")
            self._heartbeat_process_manager.stop_all()
            
        # åœæ­¢æ‰«æå™¨ï¼ˆå¦‚æœå·²å¯åŠ¨ï¼‰
        if self._scanner_started:
            self._scanner_stop.set()
            # å¦‚æœæ˜¯åç¨‹ï¼Œå–æ¶ˆå®ƒ
            if self._scanner_task and not self._scanner_task.done():
                self._scanner_task.cancel()
        
        # åœæ­¢ç»Ÿè®¡åˆ·æ–°çº¿ç¨‹/åç¨‹ï¼ˆå¦‚æœå·²å¯åŠ¨ï¼‰
        if self._stats_flusher_started:
            self._stats_flusher_stop.set()
            # å¦‚æœæ˜¯åç¨‹ï¼Œå–æ¶ˆå®ƒ
            if self._stats_flusher_task and not self._stats_flusher_task.done():
                self._stats_flusher_task.cancel()
        
        # åœ¨cleanupæ—¶ç®€å•è®°å½•æœªå¤„ç†çš„ç»Ÿè®¡äº‹ä»¶æ•°é‡
        try:
            events_count = len(self.stats_events)
            if events_count > 0:
                logger.warning(f"Dropped {events_count} stats events during cleanup (async flush not available)")
                self.stats_events.clear()  # æ¸…ç©ºä»¥é¿å…å†…å­˜æ³„éœ²
        except Exception as e:
            logger.error(f"Failed to clear stats buffer during cleanup: {e}")
        
        # ç«‹å³å°†workeræ ‡è®°ä¸ºç¦»çº¿çŠ¶æ€
        worker_data = None
        try:
            current_time = time.time()
            
            # åªæœ‰åœ¨consumer_idå·²åˆå§‹åŒ–çš„æƒ…å†µä¸‹æ‰è¿›è¡Œæ¸…ç†
            if self.consumer_id is None:
                logger.debug("Consumer ID was never initialized, skipping worker cleanup")
                return
                
            # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„worker_keyï¼Œä¸è¦è§¦å‘getter
            worker_key = self._worker_key
            if not worker_key:
                logger.debug("Worker key was never initialized, skipping worker cleanup")
                return
                
            # è·å–å½“å‰workerçš„æ•°æ®ç”¨äºä¿å­˜å†å²
            worker_data = self.redis.hgetall(worker_key)
            
            # å¦‚æœworkerä»æœªè¿è¡Œè¿‡ï¼ˆæ²¡æœ‰æ•°æ®ï¼‰ï¼Œåˆ™ä¸éœ€è¦å¤„ç†
            if not worker_data:
                logger.debug(f"Worker {self.consumer_id} never started, skipping cleanup")
                return
            
            # æ›´æ–°workerçŠ¶æ€ä¸ºç¦»çº¿ï¼ˆä¿ç•™æ‰€æœ‰ç°æœ‰æ•°æ®ï¼‰
            pipeline = self.redis.pipeline()
            pipeline.hset(worker_key, mapping={
                'is_alive': 'false',
                'offline_time': str(current_time),
                'shutdown_reason': 'graceful_shutdown',
                'messages_transferred': 'false'  # æ ‡è®°æ¶ˆæ¯éœ€è¦è½¬ç§»
            })
            
            # è·å–workerçš„é˜Ÿåˆ—åˆ—è¡¨
            queues = worker_data.get('queues', '').split(',') if worker_data.get('queues') else []
            
            # å°†æ‰€æœ‰é˜Ÿåˆ—çš„è¿è¡Œä¸­ä»»åŠ¡æ•°å½’é›¶
            for queue in queues:
                if queue.strip():
                    pipeline.hset(worker_key, f'{queue}:running_tasks', '0')
            
            # æ‰§è¡Œæ‰¹é‡æ›´æ–°
            pipeline.execute()
            
            # ä¸å†ä¿å­˜å†å²è®°å½•ï¼ŒWORKERé”®æœ¬èº«å°±åŒ…å«äº†æ‰€æœ‰ä¿¡æ¯
            
            logger.debug(f"Marked worker {self.consumer_id} as offline immediately")
            
        except Exception as e:
            logger.error(f"Failed to mark worker as offline during cleanup: {e}")
        
        # å¦‚æœä»æœªæˆåŠŸè¿è¡Œè¿‡ï¼Œç›´æ¥è¿”å›
        if not worker_data:
            logger.debug(f"Heartbeat consumer {self.consumer_id} stopped gracefully (never started)")
            return
        
        # ç­‰å¾…æ‰«æçº¿ç¨‹ç»“æŸï¼ˆéé˜»å¡ï¼‰
        if self._scanner_started and self._scanner_thread and self._scanner_thread.is_alive():
            max_wait_time = 0.5  # æœ€å¤šç­‰å¾…0.5ç§’
            self._scanner_thread.join(timeout=max_wait_time)
            if self._scanner_thread.is_alive():
                logger.warning("Scanner thread did not stop in time")
        
        # ç­‰å¾…ç»Ÿè®¡åˆ·æ–°çº¿ç¨‹ç»“æŸï¼ˆéé˜»å¡ï¼‰
        if self._stats_flusher_started and self._stats_flusher_thread and self._stats_flusher_thread.is_alive():
            max_wait_time = 0.5  # æœ€å¤šç­‰å¾…0.5ç§’
            self._stats_flusher_thread.join(timeout=max_wait_time)
            if self._stats_flusher_thread.is_alive():
                logger.warning("Stats flusher thread did not stop in time")
        
        # é‡è¦ï¼šä¸åˆ é™¤å¿ƒè·³è®°å½•ï¼
        # å¿ƒè·³è®°å½•å¿…é¡»ä¿ç•™ï¼Œè®©scannerèƒ½å¤Ÿæ£€æµ‹åˆ°workerç¦»çº¿å¹¶æ¢å¤pendingæ¶ˆæ¯
        # å¿ƒè·³ä¼šå› ä¸ºè¶…æ—¶è‡ªåŠ¨è¢«scanneræ¸…ç†
        logger.debug(f"Heartbeat consumer {self.consumer_id} stopped")