{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to extract score from options based on values in the response\n",
    "def val_score_mapping(s1, s2):\n",
    "    split_options = s2.strip().split(\"),\")\n",
    "    split_response = s1.strip().split(\": \")[1].split(\",\")\n",
    "    scores = {}\n",
    "\n",
    "    for i in split_options:\n",
    "        if len(i) > 0:\n",
    "            val_num = i.split(\"(score\")[0].split(\": \")[1].strip()\n",
    "            score_num = i.split(\"(score\")[1].split(\": \")[1].strip()\n",
    "            scores[val_num] = score_num\n",
    "\n",
    "    response_score_mapping = {\n",
    "        split_response[i].strip(): scores[split_response[i].strip()]\n",
    "        for i in range(len(split_response))\n",
    "    }\n",
    "    list_response_score_mapping = list(response_score_mapping.values())\n",
    "    str_response_score_mapping = \", \".join(\n",
    "        str(value) for value in list_response_score_mapping\n",
    "    )\n",
    "    return str_response_score_mapping.replace(\")\", \"\")\n",
    "\n",
    "\n",
    "# Function to cleanup and split time range in the response\n",
    "def clean_time_range(df, column_name):\n",
    "    cleaned = []\n",
    "    for i in range(len(df[column_name])):\n",
    "        if pd.notna(df[column_name][i]) and str(df[column_name][i]).startswith(\n",
    "            \"time_range\"\n",
    "        ):\n",
    "            t = re.sub(r\"[a-zA-Z\\s+(\\)_:]\", \"\", df[column_name][i])\n",
    "            t = t.replace(\",\", \":\")\n",
    "            if re.search(r\"^[0-9]:\", t):  # 9,30/12,30\n",
    "                ttemp = \"0\" + t\n",
    "            elif re.search(r\":[0-9]$\", t):  # 12,5/12,30\n",
    "                ttemp = t.replace(\":\", \":0\")\n",
    "            else:\n",
    "                ttemp = t\n",
    "            # tpos = datetime.strptime(str(ttemp), '%H:%M')\n",
    "            # thm = tpos.strftime('%H:%M')\n",
    "            thm = ttemp\n",
    "        else:\n",
    "            ttemp = df[column_name][i]\n",
    "            thm = ttemp\n",
    "        cleaned.append(thm)\n",
    "    return cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup path variable and get list of report.csv files\n",
    "\n",
    "input_path = os.path.expanduser(\"~/NIMH EMA Data v2/Input Files/\")  # noqa: PTH111\n",
    "all_files = os.listdir(os.path.join(input_path, \"EMA_applet_data\"))\n",
    "files = [file for file in all_files if file.startswith(\"report\")]\n",
    "input_files = os.listdir(input_path)\n",
    "output_path = os.path.expanduser(\"~/NIMH EMA Data v2/Output Files/\")  # noqa: PTH111\n",
    "\n",
    "# Read all the report.csv files\n",
    "report_all = []\n",
    "for i in range(len(files)):\n",
    "    temp_df = pd.read_csv(\n",
    "        os.path.join(input_path, \"EMA_applet_data\", files[i]), encoding=\"ISO-8859-1\"\n",
    "    )\n",
    "    report_all.append(temp_df)\n",
    "\n",
    "# Concat report.csv to one file and read other input files\n",
    "dat_full = pd.concat(report_all, ignore_index=True)\n",
    "flow = pd.read_csv(os.path.join(input_path, \"flow-items.csv\"))\n",
    "history = pd.read_csv(os.path.join(input_path, \"user-flow-schedule.csv\"))\n",
    "if \"user-activity-schedule.csv\" in input_files:\n",
    "    act_history_exist = 1\n",
    "    act_history = pd.read_csv(os.path.join(input_path, \"user-activity-schedule.csv\"))\n",
    "else:\n",
    "    act_history_exist = 0\n",
    "dat_full = dat_full.applymap(str)\n",
    "\n",
    "# Write out the concatenated report.csv file\n",
    "dat_full.to_csv(os.path.join(output_path, \"report_all.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id column as it contains special characters\n",
    "dat_full.rename(columns={dat_full.columns[0]: \"id\"}, inplace=True)\n",
    "\n",
    "# Add timezone_offset\n",
    "dat_full[\"offset\"] = np.where(dat_full[\"timezone_offset\"] == \"nan\", 0, 1)\n",
    "\n",
    "dat_full[\"activity_start_time_offsetADD\"] = np.where(\n",
    "    dat_full[\"timezone_offset\"] != \"nan\",\n",
    "    pd.to_numeric(dat_full[\"activity_start_time\"], errors=\"coerce\")\n",
    "    + (pd.to_numeric(dat_full[\"timezone_offset\"], errors=\"coerce\") * 60 * 1000),\n",
    "    pd.to_numeric(dat_full[\"activity_start_time\"], errors=\"coerce\"),\n",
    ")\n",
    "dat_full[\"activity_end_time_offsetADD\"] = np.where(\n",
    "    dat_full[\"timezone_offset\"] != \"nan\",\n",
    "    pd.to_numeric(dat_full[\"activity_end_time\"], errors=\"coerce\")\n",
    "    + (pd.to_numeric(dat_full[\"timezone_offset\"], errors=\"coerce\") * 60 * 1000),\n",
    "    pd.to_numeric(dat_full[\"activity_end_time\"], errors=\"coerce\"),\n",
    ")\n",
    "dat_full[\"activity_scheduled_time_offsetADD\"] = np.where(\n",
    "    dat_full[\"timezone_offset\"] != \"nan\",\n",
    "    pd.to_numeric(dat_full[\"activity_scheduled_time\"], errors=\"coerce\")\n",
    "    + (pd.to_numeric(dat_full[\"timezone_offset\"], errors=\"coerce\") * 60 * 1000),\n",
    "    pd.to_numeric(dat_full[\"activity_scheduled_time\"], errors=\"coerce\"),\n",
    ")\n",
    "\n",
    "dat_full[\"activity_start_time_offsetADD\"] = pd.to_numeric(\n",
    "    dat_full[\"activity_start_time_offsetADD\"], downcast=\"integer\"\n",
    ")\n",
    "dat_full[\"activity_end_time_offsetADD\"] = pd.to_numeric(\n",
    "    dat_full[\"activity_end_time_offsetADD\"], downcast=\"integer\"\n",
    ")\n",
    "dat_full[\"activity_scheduled_time_offsetADD\"] = pd.to_numeric(\n",
    "    dat_full[\"activity_scheduled_time_offsetADD\"], downcast=\"integer\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R code translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here is the translation of R code\n",
    "dat_full[\"start_Time\"] = dat_full[\"activity_start_time_offsetADD\"]\n",
    "dat_full[\"end_Time\"] = dat_full[\"activity_end_time_offsetADD\"]\n",
    "dat_full[\"schedule_Time\"] = dat_full[\"activity_scheduled_time_offsetADD\"]\n",
    "\n",
    "# Cleanup similar to R code\n",
    "dat_processed = (\n",
    "    dat_full.groupby(\n",
    "        [\"secret_user_id\", \"activity_flow_id\", \"activity_scheduled_time\"],\n",
    "        group_keys=True,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda x: x.assign(\n",
    "            start_Time=x[\"start_Time\"].min(), end_Time=x[\"end_Time\"].max()\n",
    "        )\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# added extra columns than what exists in R cleanup code\n",
    "dat_subset = dat_processed[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"activity_scheduled_time\",\n",
    "        \"secret_user_id\",\n",
    "        \"userId\",\n",
    "        \"activity_id\",\n",
    "        \"activity_name\",\n",
    "        \"activity_flow_id\",\n",
    "        \"activity_flow_name\",\n",
    "        \"item\",\n",
    "        \"response\",\n",
    "        \"options\",\n",
    "        \"event_id\",\n",
    "        \"start_Time\",\n",
    "        \"end_Time\",\n",
    "        \"schedule_Time\",\n",
    "        \"version\",\n",
    "        \"activity_start_time\",\n",
    "        \"offset\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Creating additional column to add scores in the next steps\n",
    "dat_subset = dat_subset.copy()\n",
    "dat_subset[\"response_scores\"] = None\n",
    "\n",
    "# Cleanup similar to R code\n",
    "for i in range(len(dat_subset[\"response\"])):\n",
    "    if re.search(r\"score: \", dat_subset[\"options\"][i]):\n",
    "        s = val_score_mapping(dat_subset[\"response\"][i], dat_subset[\"options\"][i])\n",
    "        dat_subset.loc[i, \"response_scores\"] = s\n",
    "    if re.search(r\"value\", dat_subset[\"response\"][i]):\n",
    "        r = dat_subset[\"response\"][i].replace(\"value: \", \"\")\n",
    "        dat_subset.loc[i, \"response\"] = r\n",
    "    elif re.search(r\"time:\", dat_subset[\"response\"][i]):\n",
    "        if re.search(r\"hr [0-9],\", dat_subset[\"response\"][i]):\n",
    "            egapp = dat_subset[\"response\"][i].replace(\"time: hr \", \"0\")\n",
    "            if re.search(r\", min [0-9]$\", egapp):\n",
    "                egtemp = egapp.replace(\", min \", \":0\")\n",
    "            elif re.search(r\", min [0-9][0-9]$\", egapp):\n",
    "                egtemp = egapp.replace(\", min \", \":\")\n",
    "            egpos = datetime.strptime(egtemp, \"%H:%M\")\n",
    "            egpos2 = egpos.strftime(\"%H:%M\")\n",
    "            dat_subset.loc[i, \"response\"] = egpos2\n",
    "        elif re.search(r\"hr [0-9][0-9],\", dat_subset[\"response\"][i]):\n",
    "            egapp = dat_subset[\"response\"][i].replace(\"time: hr \", \"\")\n",
    "            if re.search(r\", min [0-9]$\", egapp):\n",
    "                egtemp = egapp.replace(\", min \", \":0\")\n",
    "            elif re.search(r\", min [0-9][0-9]$\", egapp):\n",
    "                egtemp = egapp.replace(\", min \", \":\")\n",
    "            egpos = datetime.strptime(egtemp, \"%H:%M\")\n",
    "            egpos2 = egpos.strftime(\"%H:%M\")\n",
    "            dat_subset.loc[i, \"response\"] = egpos2\n",
    "    elif re.search(r\"geo:\", dat_subset[\"response\"][i]):\n",
    "        g = dat_subset[\"response\"][i].replace(\"geo: \", \"\")\n",
    "        dat_subset.loc[i, \"response\"] = g\n",
    "\n",
    "# Combining scores and other formats of responses into one column\n",
    "dat_subset[\"response2\"] = np.where(\n",
    "    dat_subset[\"response_scores\"].isna(),\n",
    "    dat_subset[\"response\"],\n",
    "    dat_subset[\"response_scores\"],\n",
    ")\n",
    "\n",
    "# Sorting and Selecting required columns\n",
    "dat_subset = dat_subset.sort_values(\n",
    "    by=[\n",
    "        \"secret_user_id\",\n",
    "        \"activity_flow_id\",\n",
    "        \"activity_id\",\n",
    "        \"schedule_Time\",\n",
    "        \"activity_start_time\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new binary to indicate whether responses were from activity or flow\n",
    "dat_subset[\"is_activity\"] = np.where(dat_subset[\"activity_flow_id\"] == \"nan\", \"Y\", \"N\")\n",
    "\n",
    "# Creating a new column such that:\n",
    "#   activity_flow_id if item is from flow\n",
    "#   activity_id if them is from activity\n",
    "dat_subset[\"activity_flow\"] = np.where(\n",
    "    dat_subset[\"activity_flow_id\"] == \"nan\",\n",
    "    (dat_subset[\"activity_id\"] + \"|\" + dat_subset[\"activity_name\"]),\n",
    "    dat_subset[\"activity_flow_id\"],\n",
    ")\n",
    "dat_subset = dat_subset[\n",
    "    [\n",
    "        \"userId\",\n",
    "        \"secret_user_id\",\n",
    "        \"activity_flow_id\",\n",
    "        \"activity_id\",\n",
    "        \"activity_flow\",\n",
    "        \"activity_flow_name\",\n",
    "        \"is_activity\",\n",
    "        \"offset\",\n",
    "        \"item\",\n",
    "        \"response\",\n",
    "        \"response_scores\",\n",
    "        \"response2\",\n",
    "        \"options\",\n",
    "        \"start_Time\",\n",
    "        \"end_Time\",\n",
    "        \"schedule_Time\",\n",
    "        \"activity_scheduled_time\",\n",
    "        \"version\",\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "dat_subset = dat_subset.rename(columns={\"event_id\": \"event_id_report\"})\n",
    "\n",
    "# Making sure there are no NAs\n",
    "dat_subset[\"schedule_Time\"] = np.where(\n",
    "    dat_subset[\"schedule_Time\"].isna(), \"NO SCHEDULE\", dat_subset[\"schedule_Time\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding answer_ids to help with debugging in the final output\n",
    "answers = (\n",
    "    dat_subset.groupby(\n",
    "        [\n",
    "            \"userId\",\n",
    "            \"secret_user_id\",\n",
    "            \"activity_flow\",\n",
    "            \"activity_flow_name\",\n",
    "            \"event_id_report\",\n",
    "            \"is_activity\",\n",
    "            \"start_Time\",\n",
    "            \"end_Time\",\n",
    "            \"schedule_Time\",\n",
    "            \"offset\",\n",
    "            \"version\",\n",
    "        ]\n",
    "    )[\"id\"]\n",
    "    .apply(lambda x: \"|\".join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Widening data\n",
    "dat_wide = pd.pivot_table(\n",
    "    dat_subset,\n",
    "    index=[\n",
    "        \"userId\",\n",
    "        \"secret_user_id\",\n",
    "        \"activity_flow\",\n",
    "        \"activity_flow_name\",\n",
    "        \"event_id_report\",\n",
    "        \"is_activity\",\n",
    "        \"start_Time\",\n",
    "        \"end_Time\",\n",
    "        \"schedule_Time\",\n",
    "        \"offset\",\n",
    "        \"version\",\n",
    "    ],\n",
    "    columns=\"item\",\n",
    "    values=\"response2\",\n",
    "    aggfunc=\"last\",\n",
    ").reset_index()\n",
    "\n",
    "# Joining Wide format table with answers to include concatinated answer_ids\n",
    "dat_wide = pd.merge(\n",
    "    dat_wide,\n",
    "    answers,\n",
    "    on=[\n",
    "        \"userId\",\n",
    "        \"secret_user_id\",\n",
    "        \"activity_flow\",\n",
    "        \"activity_flow_name\",\n",
    "        \"event_id_report\",\n",
    "        \"is_activity\",\n",
    "        \"start_Time\",\n",
    "        \"end_Time\",\n",
    "        \"schedule_Time\",\n",
    "        \"offset\",\n",
    "        \"version\",\n",
    "    ],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "dat_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific item cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function from the Set Up section to cleanup time range and split start and end dates from the response\n",
    "\n",
    "# Breaking up the old headache_time range question into 2\n",
    "dat_wide[[\"headache_time_start_old\", \"headache_time_end_old\"]] = dat_wide[\n",
    "    \"headache_time\"\n",
    "].str.split(\"/\", expand=True)\n",
    "dat_wide[\"headache_time_start_old\"] = clean_time_range(\n",
    "    dat_wide, \"headache_time_start_old\"\n",
    ")\n",
    "dat_wide[\"headache_time_end_old\"] = clean_time_range(dat_wide, \"headache_time_end_old\")\n",
    "\n",
    "\n",
    "# Cleanup gps response and split to 2 columns for lat and lon\n",
    "dat_wide[\"now_gps\"] = dat_wide[\"now_gps\"].replace(r\"[a-zA-Z\\s+(\\)]\", \"\", regex=True)\n",
    "dat_wide[[\"now_gps_lat\", \"now_gps_long\"]] = dat_wide[\"now_gps\"].str.split(\n",
    "    \"/\", expand=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Range Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of time range columns that need to be  split and formatted\n",
    "# PLEASE ADD MORE TO THIS LIST IF NECESSARY\n",
    "time_range_split = [\n",
    "    \"since_activity_monitor_time1\",\n",
    "    \"since_activity_monitor_time\",\n",
    "    \"since_light_device_time1\",\n",
    "    \"since_light_device_time\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in time_range_split:\n",
    "    start = i + \"_start\"\n",
    "    end = i + \"_end\"\n",
    "    dat_wide[[start, end]] = dat_wide[i].str.split(\"/\", expand=True)\n",
    "    dat_wide[start] = clean_time_range(dat_wide, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup similar to R code\n",
    "dat_wide_full = dat_wide.applymap(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch to Timestamp\n",
    "dat_wide_full[\"start_Time\"] = pd.to_numeric(\n",
    "    dat_wide_full[\"start_Time\"], errors=\"coerce\"\n",
    ")\n",
    "dat_wide_full[\"start_Time\"] = pd.to_datetime(\n",
    "    dat_wide_full[\"start_Time\"] / 1000, unit=\"s\"\n",
    ")\n",
    "\n",
    "dat_wide_full[\"end_Time\"] = pd.to_numeric(dat_wide_full[\"end_Time\"], errors=\"coerce\")\n",
    "dat_wide_full[\"end_Time\"] = pd.to_datetime(dat_wide_full[\"end_Time\"] / 1000, unit=\"s\")\n",
    "\n",
    "dat_wide_full[\"schedule_Time\"] = pd.to_numeric(\n",
    "    dat_wide_full[\"schedule_Time\"], errors=\"coerce\"\n",
    ")\n",
    "dat_wide_full[\"schedule_Time\"] = pd.to_datetime(\n",
    "    dat_wide_full[\"schedule_Time\"] / 1000, unit=\"s\"\n",
    ")\n",
    "\n",
    "# Timestamp cleanup\n",
    "dat_wide_full[\"schedule_Time\"] = pd.to_datetime(dat_wide_full[\"schedule_Time\"])\n",
    "dat_wide_full[\"schedule_Time\"] = np.where(\n",
    "    dat_wide_full[\"offset\"] == \"1\",\n",
    "    dat_wide_full[\"schedule_Time\"],\n",
    "    dat_wide_full[\"schedule_Time\"]\n",
    "    .dt.tz_localize(\"UTC\")\n",
    "    .dt.tz_convert(\"America/New_York\")\n",
    "    .dt.tz_localize(None),\n",
    ")\n",
    "\n",
    "dat_wide_full[\"start_Time\"] = pd.to_datetime(dat_wide_full[\"end_Time\"])\n",
    "dat_wide_full[\"start_Time\"] = dat_wide_full[\"end_Time\"].dt.floor(\"1s\")\n",
    "dat_wide_full[\"start_Time\"] = np.where(\n",
    "    dat_wide_full[\"offset\"] == \"1\",\n",
    "    dat_wide_full[\"end_Time\"],\n",
    "    dat_wide_full[\"start_Time\"]\n",
    "    .dt.tz_localize(\"UTC\")\n",
    "    .dt.tz_convert(\"America/New_York\")\n",
    "    .dt.tz_localize(None),\n",
    ")\n",
    "\n",
    "dat_wide_full[\"end_Time\"] = pd.to_datetime(dat_wide_full[\"end_Time\"])\n",
    "dat_wide_full[\"end_Time\"] = dat_wide_full[\"end_Time\"].dt.floor(\"1s\")\n",
    "dat_wide_full[\"end_Time\"] = np.where(\n",
    "    dat_wide_full[\"offset\"] == \"1\",\n",
    "    dat_wide_full[\"end_Time\"],\n",
    "    dat_wide_full[\"end_Time\"]\n",
    "    .dt.tz_localize(\"UTC\")\n",
    "    .dt.tz_convert(\"America/New_York\")\n",
    "    .dt.tz_localize(None),\n",
    ")\n",
    "\n",
    "# Creating Dates\n",
    "dat_wide_full[\"scheduled_Date\"] = dat_wide_full[\"schedule_Time\"].dt.date\n",
    "dat_wide_full[\"start_Date\"] = dat_wide_full[\"start_Time\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of secret_user_id for userIds. This step could be excluded in future, once export file and schedule file have same secret_user_id\n",
    "ID_List = dat_wide_full[[\"userId\", \"secret_user_id\"]].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data: Flows and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Separting the data by flows vs activities\n",
    "    act_dat_wide = dat_wide_full[dat_wide_full[\"is_activity\"] == \"Y\"]\n",
    "    dat_wide_full = dat_wide_full[dat_wide_full[\"is_activity\"] == \"N\"]\n",
    "\n",
    "    # Split activity_flow column to grab the activity_id and activity_name\n",
    "    act_dat_wide[[\"activity_id_report\", \"activity_name_report\"]] = act_dat_wide[\n",
    "        \"activity_flow\"\n",
    "    ].str.split(\"|\", expand=True)\n",
    "\n",
    "    # Dropping unnecessary columns\n",
    "    act_dat_wide = act_dat_wide.drop(columns=[\"activity_flow\", \"is_activity\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow schedule history cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp formatting\n",
    "history[\"scheduled_Time_start\"] = (\n",
    "    history[\"scheduled_date\"] + \" \" + history[\"schedule_start_time\"]\n",
    ")\n",
    "history[\"scheduled_Time_start\"] = pd.to_datetime(history[\"scheduled_Time_start\"])\n",
    "history[\"scheduled_Date2\"] = history[\"scheduled_Time_start\"].dt.date\n",
    "\n",
    "history[\"scheduled_Time_end\"] = (\n",
    "    history[\"scheduled_date\"] + \" \" + history[\"schedule_end_time\"]\n",
    ")\n",
    "history[\"scheduled_Time_end\"] = pd.to_datetime(history[\"scheduled_Time_end\"])\n",
    "\n",
    "# Renaming columns\n",
    "history = history.rename(columns={\"secret_user_id\": \"history_sui\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow submission (report.csv) & Flow schedule history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine history table data with wide data to get missing schedule rows\n",
    "conn = sqlite3.connect(\":memory:\")  # Make the db in memory\n",
    "# write the tables\n",
    "history.to_sql(\"history\", conn, index=False)\n",
    "dat_wide_full.to_sql(\"dat_wide_full\", conn, index=False)\n",
    "\n",
    "qry = \"\"\"\n",
    "    select  \n",
    "        *\n",
    "    from\n",
    "        history full outer join dat_wide_full\n",
    "            on history.user_id = dat_wide_full.userId\n",
    "            and history.event_id = dat_wide_full.event_id_report\n",
    "            and history.flow_id = dat_wide_full.activity_flow\n",
    "            and strftime('%Y-%m-%d %H:%M:%S' , dat_wide_full.schedule_Time) between strftime('%Y-%m-%d %H:%M:%S' , history.scheduled_Time_start) and strftime('%Y-%m-%d %H:%M:%S', history.scheduled_Time_end) \n",
    "    \"\"\"\n",
    "dat_joined = pd.read_sql_query(qry, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if joined correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if joined correctly\n",
    "## !! Should not be 0 !!\n",
    "\n",
    "len(\n",
    "    dat_joined[\n",
    "        (pd.notna(dat_joined[\"history_sui\"])) & (pd.notna(dat_joined[\"userId\"]))\n",
    "    ].index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Check joined data, if necessary\n",
    "\n",
    "# dat_joined.to_csv(os.path.join(output_path, 'flow_joined_check.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Items cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 0:\n",
    "    # Setting this for binary values\n",
    "    flow[\"value\"] = 1\n",
    "    flow.rename(columns={\"applet_version\": \"version\"}, inplace=True)\n",
    "    # Wide binary column build\n",
    "    flow_wide = pd.pivot_table(\n",
    "        flow,\n",
    "        index=[\"version\", \"flow_id\"],\n",
    "        columns=\"activity_name\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "    flow_wide = flow_wide.fillna(0)\n",
    "    flow_wide = flow_wide.reset_index()\n",
    "else:\n",
    "    # Creating activities binary columns\n",
    "    flow_activities_report = act_dat_wide.copy()\n",
    "    flow_activities_history = act_history.copy()\n",
    "\n",
    "    flow_activities_report = flow_activities_report[\n",
    "        [\"activity_id_report\", \"activity_name_report\", \"version\"]\n",
    "    ]\n",
    "    flow_activities_history = flow_activities_history[\n",
    "        [\"activity_id\", \"activity_name\", \"applet_version\"]\n",
    "    ]\n",
    "\n",
    "    flow_activities_report.rename(\n",
    "        columns={\n",
    "            \"activity_id_report\": \"activity_id\",\n",
    "            \"activity_name_report\": \"activity_name\",\n",
    "            \"version\": \"applet_version\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    flow_activities_report[\"value\"] = 0\n",
    "    flow_activities_history[\"value\"] = 0\n",
    "\n",
    "    flow_activities_all = [flow_activities_report, flow_activities_history]\n",
    "    flow_activities = pd.concat(flow_activities_all, ignore_index=True)\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    flow_activities = flow_activities.drop_duplicates()\n",
    "\n",
    "    # Getting other binary columns from flow history to ensure all activities are listed\n",
    "    flows = flow.copy()\n",
    "    flows = flows[[\"flow_id\", \"activity_name\", \"applet_version\"]]\n",
    "    flows.rename(columns={\"applet_version\": \"version\"}, inplace=True)\n",
    "    flows[\"value_flow\"] = 1\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    flows = flows.drop_duplicates()\n",
    "\n",
    "    # Joining the two binary column data together\n",
    "    flows_final = pd.merge(\n",
    "        flows,\n",
    "        flow_activities,\n",
    "        how=\"outer\",\n",
    "        left_on=[\"version\", \"flow_id\"],\n",
    "        right_on=[\"applet_version\", \"activity_id\"],\n",
    "    )\n",
    "\n",
    "    # Filling in data with the join so all information is in one column\n",
    "    flows_final[\"flow_id_dup\"] = flows_final[\"flow_id\"]\n",
    "    flows_final[\"flow_id\"] = np.where(\n",
    "        flows_final[\"flow_id_dup\"].isna(),\n",
    "        flows_final[\"activity_id\"],\n",
    "        flows_final[\"flow_id_dup\"],\n",
    "    )\n",
    "\n",
    "    flows_final[\"activity_name\"] = flows_final[\"activity_name_x\"]\n",
    "    flows_final[\"activity_name\"] = np.where(\n",
    "        flows_final[\"activity_name_x\"].isna(),\n",
    "        flows_final[\"activity_name_y\"],\n",
    "        flows_final[\"activity_name_x\"],\n",
    "    )\n",
    "\n",
    "    flows_final[\"version_dup\"] = flows_final[\"version\"]\n",
    "    flows_final[\"version\"] = np.where(\n",
    "        flows_final[\"version_dup\"].isna(),\n",
    "        flows_final[\"applet_version\"],\n",
    "        flows_final[\"version_dup\"],\n",
    "    )\n",
    "\n",
    "    flows_final[\"value_dup\"] = flows_final[\"value\"]\n",
    "    flows_final[\"value\"] = np.where(\n",
    "        flows_final[\"value_flow\"].isna(),\n",
    "        flows_final[\"value_dup\"],\n",
    "        flows_final[\"value_flow\"],\n",
    "    )\n",
    "\n",
    "    flows_final = flows_final[[\"flow_id\", \"activity_name\", \"version\", \"value\"]]\n",
    "\n",
    "    # Making binary data wide to join and filling any NAs with 0\n",
    "    flow_wide = pd.pivot_table(\n",
    "        flows_final,\n",
    "        index=[\"version\", \"flow_id\"],\n",
    "        columns=\"activity_name\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"first\",\n",
    "    ).reset_index()\n",
    "    flow_wide = flow_wide.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Flows Data and Flow Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the version column such that binary values exist for null response rows\n",
    "dat_joined[\"version_report\"] = dat_joined[\"version\"]\n",
    "dat_joined[\"version\"] = np.where(\n",
    "    dat_joined[\"version_report\"].isna(),\n",
    "    dat_joined[\"applet_version\"],\n",
    "    dat_joined[\"version_report\"],\n",
    ")\n",
    "\n",
    "# Getting the flow column such that binary values exist for null response rows\n",
    "dat_joined[\"activity_flow_id\"] = np.where(\n",
    "    dat_joined[\"activity_flow\"].isna(),\n",
    "    dat_joined[\"flow_id\"],\n",
    "    dat_joined[\"activity_flow\"],\n",
    ")\n",
    "dat_joined[\"activity_flow\"] = np.where(\n",
    "    dat_joined[\"activity_flow_name\"].isna(),\n",
    "    dat_joined[\"flow_name\"],\n",
    "    dat_joined[\"activity_flow_name\"],\n",
    ")\n",
    "\n",
    "# Merging Wide fomart export data with binary wide data\n",
    "dat_wide2 = pd.merge(\n",
    "    dat_joined,\n",
    "    flow_wide,\n",
    "    how=\"left\",\n",
    "    left_on=[\"version\", \"activity_flow_id\"],\n",
    "    right_on=[\"version\", \"flow_id\"],\n",
    ")\n",
    "\n",
    "# Getting the user_id column to ensure user_id is always available\n",
    "dat_wide2[\"userId_report\"] = dat_wide2[\"userId\"]\n",
    "dat_wide2[\"userId\"] = np.where(\n",
    "    dat_wide2[\"userId_report\"].isna(), dat_wide2[\"user_id\"], dat_wide2[\"userId_report\"]\n",
    ")\n",
    "\n",
    "# Combining Headache Time separated version and time range version together\n",
    "dat_wide2[\"headache_time_start_dup\"] = dat_wide2[\"headache_time_start\"]\n",
    "dat_wide2[\"headache_time_start\"] = np.where(\n",
    "    (dat_wide2[\"headache_time_start_dup\"] == \"nan\")\n",
    "    | (dat_wide2[\"headache_time_start_dup\"].isna()),\n",
    "    dat_wide2[\"headache_time_start_old\"],\n",
    "    dat_wide2[\"headache_time_start_dup\"],\n",
    ")\n",
    "dat_wide2[\"headache_time_end_dup\"] = dat_wide2[\"headache_time_end\"]\n",
    "dat_wide2[\"headache_time_end\"] = np.where(\n",
    "    (dat_wide2[\"headache_time_end_dup\"] == \"nan\")\n",
    "    | (dat_wide2[\"headache_time_end_dup\"].isna()),\n",
    "    dat_wide2[\"headache_time_end_old\"],\n",
    "    dat_wide2[\"headache_time_end_dup\"],\n",
    ")\n",
    "\n",
    "# Timestamp formatting - TO MAKE SURE!\n",
    "dat_wide2[\"schedule_Time\"] = pd.to_datetime(dat_wide2[\"schedule_Time\"])\n",
    "dat_wide2[\"start_Time\"] = pd.to_datetime(dat_wide2[\"start_Time\"])\n",
    "dat_wide2[\"end_Time\"] = pd.to_datetime(dat_wide2[\"end_Time\"])\n",
    "dat_wide2[\"scheduled_Time_start\"] = pd.to_datetime(dat_wide2[\"scheduled_Time_start\"])\n",
    "\n",
    "# Getting secret_user_id for all the user_ids. This step could be excluded in future, once export file and schedule file have same secret_user_id\n",
    "final = pd.merge(\n",
    "    dat_wide2, ID_List, how=\"left\", left_on=[\"userId\"], right_on=[\"userId\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flows Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensure secret_user_id available for all rows\n",
    "final[\"secret_user_id\"] = np.where(\n",
    "    final[\"secret_user_id_x\"].isna(),\n",
    "    final[\"secret_user_id_y\"],\n",
    "    final[\"secret_user_id_x\"],\n",
    ")\n",
    "final[\"secret_user_id\"] = np.where(\n",
    "    final[\"secret_user_id\"].isna(), final[\"history_sui\"], final[\"secret_user_id\"]\n",
    ")\n",
    "\n",
    "# If activity schedule time is null, it will be replaced with timestamp from schedule file\n",
    "final[\"schedule_Time\"] = np.where(\n",
    "    final[\"schedule_Time\"].isna(), final[\"scheduled_Time_start\"], final[\"schedule_Time\"]\n",
    ")\n",
    "\n",
    "# If event_id is null, it will be replaced with event_id from schedule file\n",
    "final[\"event_id_sched\"] = final[\"event_id\"]\n",
    "final[\"event_id\"] = np.where(\n",
    "    final[\"event_id_report\"].isna(), final[\"event_id_sched\"], final[\"event_id_report\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### PLEASE SELECT APPROPRIATE ITEMS & ORDER ###############################################################\n",
    "\n",
    "# selecting the required columns and ordering\n",
    "\n",
    "final = final[\n",
    "    [\n",
    "        \"userId\",\n",
    "        \"event_id\",\n",
    "        \"id\",\n",
    "        \"secret_user_id\",\n",
    "        \"activity_flow\",\n",
    "        \"schedule_Time\",\n",
    "        \"start_Time\",\n",
    "        \"end_Time\",\n",
    "        \"version\",\n",
    "        \" Sleep \",\n",
    "        \"Activity Watch\",\n",
    "        \"Context of Assessment\",\n",
    "        \"Diet\",\n",
    "        #  'Diet Test Activity schedule', #TESTING ACTIVITY\n",
    "        \"Food and Drink Intake\",\n",
    "        \"Food and Drink Intake (Supplemental)\",  # Only in Testing\n",
    "        \"Life Events\",\n",
    "        \"Light Device\",\n",
    "        \"Menstrual Period\",\n",
    "        \"Mood Circumplex\",\n",
    "        \"Pain\",\n",
    "        \"Physical Activity\",\n",
    "        \"Physical Activity (with Supplement)\",  # Only in Testing\n",
    "        \"Physical Health\",\n",
    "        \"Procedure Tracking\",  # Only in Testing\n",
    "        \"Saliva Sample\",\n",
    "        \"Substance Use\",  # Only in Testing\n",
    "        \"Substance Use (AM)\",  # Only in Testing\n",
    "        \"Substance Use (Core)\",  # Only in Testing\n",
    "        \"Substance Use (Supplement)\",\n",
    "        \"saliva_instructions\",\n",
    "        \"since_activity_monitor\",\n",
    "        \"since_activity_monitor_time_start\",\n",
    "        \"since_activity_monitor_time_end\",\n",
    "        \"since_light_device\",\n",
    "        \"since_light_device_time_start\",\n",
    "        \"since_light_device_time_end\",\n",
    "        \"morning_bedtime\",\n",
    "        \"morning_lights_off\",\n",
    "        \"morning_fall_asleep\",\n",
    "        \"morning_wake_number\",\n",
    "        \"morning_awakenings_last\",\n",
    "        \"morning_waketime\",\n",
    "        \"morning_outbed\",\n",
    "        \"morning_sleep_quality\",\n",
    "        \"morning_sleep_refreshed\",\n",
    "        \"morning_sleep_problems\",\n",
    "        \"morning_sleeping_pills\",\n",
    "        \"morning_sleeping_pills_type\",\n",
    "        \"now_gps_lat\",\n",
    "        \"now_gps_long\",\n",
    "        \"now_where\",\n",
    "        \"now_inside\",\n",
    "        \"now_outside\",\n",
    "        \"since_where\",\n",
    "        \"since_inside\",\n",
    "        \"since_outside\",\n",
    "        \"now_company\",\n",
    "        # 'now_company_confirm',\n",
    "        \"since_company\",\n",
    "        \"now_activity\",\n",
    "        \"since_activity\",\n",
    "        \"since_internet\",\n",
    "        \"internet_use_duration\",\n",
    "        \"internet_use_category\",\n",
    "        \"videogame_duration\",\n",
    "        \"socialmedia_duration\",\n",
    "        \"socialmedia_activity\",\n",
    "        \"friends_communication_method\",\n",
    "        \"strangers_communication_method\",\n",
    "        \"comments_byothers\",\n",
    "        \"comments_byself\",\n",
    "        \"now_sadness\",\n",
    "        \"now_anxiousness\",\n",
    "        \"now_active\",\n",
    "        \"now_tired\",\n",
    "        \"now_distracted\",\n",
    "        \"now_irritable\",\n",
    "        \"now_quick_thinking\",\n",
    "        \"now_enjoyment\",\n",
    "        \"now_fidgety\",\n",
    "        \"now_thoughts_positive\",\n",
    "        \"now_thoughts_negative\",\n",
    "        \"now_thoughts_negative_about\",\n",
    "        \"now_thoughts_negative_severity\",\n",
    "        \"since_thoughts_negative\",\n",
    "        \"since_thoughts_negative_suicide\",\n",
    "        \"since_thoughts_negative_suicide_message\",\n",
    "        \"instructions\",  # event_instructions\n",
    "        \"event_emotion\",\n",
    "        \"event_category\",\n",
    "        \"event_people\",\n",
    "        \"event_where\",\n",
    "        \"event_health\",\n",
    "        \"event_content\",\n",
    "        \"event_issue\",\n",
    "        \"event_stress\",\n",
    "        \"since_physical_activity\",\n",
    "        \"since_sedentary\",\n",
    "        \"since_nap_rest_time\",\n",
    "        \"since_rest_duration\",\n",
    "        \"since_rest_fell_asleep\",\n",
    "        \"since_sleep_duration\",\n",
    "        \"since_vigorous_activity\",\n",
    "        \"since_moderate_activity\",\n",
    "        \"since_light_activity\",\n",
    "        \"now_thirsty\",\n",
    "        \"since_had_drink\",\n",
    "        \"not_alcohol_amount\",\n",
    "        \"since_had_drink_alcohol_type\",\n",
    "        \"since_had_drink_alcohol_quantity\",\n",
    "        \"alcohol_time\",\n",
    "        \"since_feel_drink\",\n",
    "        \"since_had_drink_caffeinated_type\",\n",
    "        \"now_hungry\",\n",
    "        \"since_times_eat\",\n",
    "        \"since_eaten_amount\",\n",
    "        \"since_eaten_when\",\n",
    "        \"since_eaten_type\",\n",
    "        \"since_food1\",\n",
    "        \"since_food2\",\n",
    "        \"since_food3\",\n",
    "        \"since_food4\",\n",
    "        \"since_food5\",\n",
    "        \"since_crave\",\n",
    "        \"craving_strong_tobacco\",\n",
    "        \"craving_strong_cannabis\",\n",
    "        \"craving_strong_otherdrug\",\n",
    "        \"craving_strong_alcohol\",\n",
    "        \"since_substances\",\n",
    "        \"substances_tobacco\",\n",
    "        \"since_substances_cigarettes\",\n",
    "        \"since_substances_cigarettes_time\",\n",
    "        \"since_substances_enicotine\",\n",
    "        \"since_substances_enicotine_time\",\n",
    "        \"since_substances_other_tobacco\",\n",
    "        \"since_substances_other_tobacco_time\",\n",
    "        \"since_cannabis_type\",\n",
    "        \"since_cannabis_time\",\n",
    "        \"since_substances_cannabis\",\n",
    "        \"since_cannabis_high\",\n",
    "        \"since_substances_other\",\n",
    "        \"since_substances_other_specify\",\n",
    "        \"since_substances_other_time\",\n",
    "        \"since_substances_other_high\",\n",
    "        \"since_substances_company\",\n",
    "        \"now_pain\",\n",
    "        \"now_pain_where\",\n",
    "        \"since_pain\",\n",
    "        \"since_pain_where\",\n",
    "        \"pain_intensity\",\n",
    "        \"headache\",\n",
    "        \"headache_prevent\",\n",
    "        \"headache_time_start\",\n",
    "        \"headache_time_end\",\n",
    "        \"headache_intensity\",\n",
    "        \"headache_location\",\n",
    "        \"headache_pulsating\",\n",
    "        \"headache_effort\",\n",
    "        \"headache_nausea\",\n",
    "        \"headache_light\",\n",
    "        \"headache_heat\",\n",
    "        \"headache_noise\",\n",
    "        \"headache_smell\",\n",
    "        \"headache_trigger\",\n",
    "        \"headache_vision_changes\",\n",
    "        \"headache_vision_change_time\",\n",
    "        \"headache_numbing\",\n",
    "        \"headache_numbing_time\",\n",
    "        \"headache_confusing\",\n",
    "        \"headache_confusing_time\",\n",
    "        \"headache_sudden\",\n",
    "        \"headache_medication\",\n",
    "        \"headache_interference\",\n",
    "        \"day_physical_health\",\n",
    "        \"day_problem_categories\",\n",
    "        \"day_bother\",\n",
    "        \"day_over_medication\",\n",
    "        \"day_over_medication_why\",\n",
    "        \"day_prescribed_medication\",\n",
    "        \"day_prescribed_medication_conditions\",\n",
    "        \"day_problems_allergies\",\n",
    "        \"day_problems_breath\",\n",
    "        \"day_problems_belly_symptoms\",\n",
    "        \"day_problems_belly\",\n",
    "        \"day_problems_muscle\",\n",
    "        \"day_problems_heart\",\n",
    "        \"dizziness_situation\",\n",
    "        \"dizziness_faint\",\n",
    "        \"day_lethargic\",\n",
    "        \"activity_planned\",\n",
    "        \"day_period\",\n",
    "        ##New items (2024 Jul 31st)\n",
    "        \"audio_test\",\n",
    "        \"headache_current\",\n",
    "        \"headache_same\",\n",
    "        \"since_activity_monitor_now\",\n",
    "        # 'since_cannabis_craving',\n",
    "        # 'since_light_activity_planned',\n",
    "        \"since_light_device_now\",\n",
    "        # 'since_moderate_activity_planned',\n",
    "        # 'since_substances_other_crave',\n",
    "        # 'since_vigorous_activity_planned',\n",
    "        # 'substances_lastnight_time',\n",
    "        # 'substances_lastnight'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring consistent NA wording across all columns\n",
    "final_out = final.copy()\n",
    "final_out.replace(\"nan\", \"NA\", inplace=True)\n",
    "final_out.fillna(\"NA\", inplace=True)\n",
    "final_out.replace(\"NA NA\", \"NA\", inplace=True)\n",
    "final_out.rename(columns={\"userId_x\": \"user_id\"}, inplace=True)\n",
    "\n",
    "# Excluding test flows\n",
    "final_out = final_out[final_out[\"activity_flow\"] != \"Test Flow (All Activities)\"]\n",
    "\n",
    "# Checking final flow data\n",
    "final_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out final flow dataframe, if necessary:\n",
    "\n",
    "final_out.to_csv(\n",
    "    os.path.join(output_path, \"flow_final.csv\"),\n",
    "    index=False,\n",
    "    date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    na_rep=\"NA\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ACTIVITIES Submissions data:\n",
    "# act_dat_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity schedule history cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Timestamp formatting\n",
    "    act_history[\"scheduled_Time_start\"] = (\n",
    "        act_history[\"scheduled_date\"] + \" \" + act_history[\"schedule_start_time\"]\n",
    "    )\n",
    "    act_history[\"scheduled_Time_start\"] = pd.to_datetime(\n",
    "        act_history[\"scheduled_Time_start\"]\n",
    "    )\n",
    "    act_history[\"scheduled_Date2\"] = act_history[\"scheduled_Time_start\"].dt.date\n",
    "\n",
    "    act_history[\"scheduled_Time_end\"] = (\n",
    "        act_history[\"scheduled_date\"] + \" \" + act_history[\"schedule_end_time\"]\n",
    "    )\n",
    "    act_history[\"scheduled_Time_end\"] = pd.to_datetime(\n",
    "        act_history[\"scheduled_Time_end\"]\n",
    "    )\n",
    "\n",
    "    # Renaming columns\n",
    "    act_history = act_history.rename(columns={\"secret_user_id\": \"history_sui\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity Submission (report.csv) & Activity schedule history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Combine history table data with wide data to get missing schedule rows\n",
    "    conn = sqlite3.connect(\":memory:\")  # Make the db in memory\n",
    "    # write the tables\n",
    "    act_history.to_sql(\"history\", conn, index=False)\n",
    "    act_dat_wide.to_sql(\"dat_wide_full\", conn, index=False)\n",
    "\n",
    "    qry = \"\"\"\n",
    "        select  \n",
    "            *\n",
    "        from\n",
    "            history full outer join dat_wide_full\n",
    "            on history.user_id = dat_wide_full.userId\n",
    "            and history.event_id = dat_wide_full.event_id_report\n",
    "            and history.activity_id = dat_wide_full.activity_id_report\n",
    "            and strftime('%Y-%m-%d %H:%M:%S' , dat_wide_full.schedule_Time) between strftime('%Y-%m-%d %H:%M:%S' , history.scheduled_Time_start) and strftime('%Y-%m-%d %H:%M:%S', history.scheduled_Time_end) \n",
    "        \"\"\"\n",
    "    act_dat_joined = pd.read_sql_query(qry, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it joined correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if joined correctly\n",
    "## !! Should not be 0 !!\n",
    "if act_history_exist == 1:\n",
    "    print(\n",
    "        len(\n",
    "            act_dat_joined[\n",
    "                (pd.notna(act_dat_joined[\"history_sui\"]))\n",
    "                & (pd.notna(act_dat_joined[\"userId\"]))\n",
    "            ].index\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Check joined data, if necessary\n",
    "\n",
    "# if act_history_exist == 1:\n",
    "#     dat_joined.to_csv(os.path.join(output_path, 'activities_joined_check.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Getting the user_id column to ensure user_id is always available\n",
    "    act_dat_joined[\"userId_report\"] = act_dat_joined[\"userId\"]\n",
    "    act_dat_joined[\"userId\"] = np.where(\n",
    "        act_dat_joined[\"userId_report\"].isna(),\n",
    "        act_dat_joined[\"user_id\"],\n",
    "        act_dat_joined[\"userId_report\"],\n",
    "    )\n",
    "\n",
    "    # Getting the version column such that binary values exist for null response rows\n",
    "    act_dat_joined[\"version_report\"] = act_dat_joined[\"version\"]\n",
    "    act_dat_joined[\"version\"] = np.where(\n",
    "        act_dat_joined[\"version_report\"].isna(),\n",
    "        act_dat_joined[\"applet_version\"],\n",
    "        act_dat_joined[\"version_report\"],\n",
    "    )\n",
    "\n",
    "    # If event_id is null, it will be replaced with event_id from schedule file\n",
    "    act_dat_joined[\"event_id_sched\"] = act_dat_joined[\"event_id\"]\n",
    "    act_dat_joined[\"event_id\"] = np.where(\n",
    "        act_dat_joined[\"event_id_report\"].isna(),\n",
    "        act_dat_joined[\"event_id_sched\"],\n",
    "        act_dat_joined[\"event_id_report\"],\n",
    "    )\n",
    "\n",
    "    #  Ensure secret_user_id available for all rows\n",
    "    act_dat_joined[\"secret_user_id\"] = np.where(\n",
    "        act_dat_joined[\"secret_user_id\"].isna(),\n",
    "        act_dat_joined[\"history_sui\"],\n",
    "        act_dat_joined[\"secret_user_id\"],\n",
    "    )\n",
    "\n",
    "    #  Ensure activity_name available for all rows\n",
    "    act_dat_joined[\"activity_name_history\"] = act_dat_joined[\"activity_name\"]\n",
    "    act_dat_joined[\"activity_name\"] = np.where(\n",
    "        act_dat_joined[\"activity_name_report\"].isna(),\n",
    "        act_dat_joined[\"activity_name_history\"],\n",
    "        act_dat_joined[\"activity_name_report\"],\n",
    "    )\n",
    "\n",
    "    #  Ensure activity_id available for all rows\n",
    "    act_dat_joined[\"activity_id_history\"] = act_dat_joined[\"activity_id\"]\n",
    "    act_dat_joined[\"activity_id\"] = np.where(\n",
    "        act_dat_joined[\"activity_id_report\"].isna(),\n",
    "        act_dat_joined[\"activity_id_history\"],\n",
    "        act_dat_joined[\"activity_id_report\"],\n",
    "    )\n",
    "\n",
    "    # Combining Headache Time separated version and time range version together\n",
    "    act_dat_joined[\"headache_time_start_dup\"] = act_dat_joined[\"headache_time_start\"]\n",
    "    act_dat_joined[\"headache_time_start\"] = np.where(\n",
    "        (act_dat_joined[\"headache_time_start_dup\"] == \"nan\")\n",
    "        | (act_dat_joined[\"headache_time_start_dup\"].isna()),\n",
    "        act_dat_joined[\"headache_time_start_old\"],\n",
    "        act_dat_joined[\"headache_time_start_dup\"],\n",
    "    )\n",
    "    act_dat_joined[\"headache_time_end_dup\"] = act_dat_joined[\"headache_time_end\"]\n",
    "    act_dat_joined[\"headache_time_end\"] = np.where(\n",
    "        (act_dat_joined[\"headache_time_end_dup\"] == \"nan\")\n",
    "        | (act_dat_joined[\"headache_time_end_dup\"].isna()),\n",
    "        act_dat_joined[\"headache_time_end_old\"],\n",
    "        act_dat_joined[\"headache_time_end_dup\"],\n",
    "    )\n",
    "\n",
    "    # Timestamp formatting - TO MAKE SURE!\n",
    "    act_dat_joined[\"schedule_Time\"] = pd.to_datetime(act_dat_joined[\"schedule_Time\"])\n",
    "    act_dat_joined[\"start_Time\"] = pd.to_datetime(act_dat_joined[\"start_Time\"])\n",
    "    act_dat_joined[\"end_Time\"] = pd.to_datetime(act_dat_joined[\"end_Time\"])\n",
    "    act_dat_joined[\"scheduled_Time_start\"] = pd.to_datetime(\n",
    "        act_dat_joined[\"scheduled_Time_start\"]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity Name Binary Column Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Creating activities binary columns\n",
    "    activities = act_dat_joined.copy()\n",
    "    activities = activities[[\"activity_id\", \"activity_name\", \"version\"]]\n",
    "    activities[\"value\"] = 1\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    activities = activities.drop_duplicates()\n",
    "\n",
    "    # Getting other binary columns from flow history to ensure all activities are listed\n",
    "    other_activities = flow.copy()\n",
    "    other_activities = flow[[\"activity_id\", \"activity_name\", \"applet_version\"]]\n",
    "    other_activities[\"value_flow\"] = 0\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    other_activities = other_activities.drop_duplicates()\n",
    "\n",
    "    # Joining the two binary column data together\n",
    "    activities_final = pd.merge(\n",
    "        other_activities,\n",
    "        activities,\n",
    "        how=\"outer\",\n",
    "        left_on=[\"applet_version\", \"activity_id\"],\n",
    "        right_on=[\"version\", \"activity_id\"],\n",
    "    )\n",
    "\n",
    "    # Filling in data with the join so all information is in one column\n",
    "    activities_final[\"values\"] = np.where(\n",
    "        activities_final[\"value\"].isna(),\n",
    "        activities_final[\"value_flow\"],\n",
    "        activities_final[\"value\"],\n",
    "    )\n",
    "    activities_final[\"activity_name\"] = np.where(\n",
    "        activities_final[\"activity_name_x\"].isna(),\n",
    "        activities_final[\"activity_name_y\"],\n",
    "        activities_final[\"activity_name_x\"],\n",
    "    )\n",
    "    activities_final[\"version_activity\"] = activities_final[\"version\"]\n",
    "    activities_final[\"version\"] = np.where(\n",
    "        activities_final[\"applet_version\"].isna(),\n",
    "        activities_final[\"version_activity\"],\n",
    "        activities_final[\"applet_version\"],\n",
    "    )\n",
    "    activities_final = activities_final[\n",
    "        [\"activity_id\", \"activity_name\", \"version\", \"values\"]\n",
    "    ]\n",
    "\n",
    "    # Making binary data wide to join and filling any NAs with 0\n",
    "    activities_wide = pd.pivot_table(\n",
    "        activities_final,\n",
    "        index=[\n",
    "            \"version\",\n",
    "            \"activity_id\",\n",
    "            \"activity_name\",\n",
    "        ],\n",
    "        columns=\"activity_name\",\n",
    "        values=\"values\",\n",
    "        aggfunc=\"first\",\n",
    "    ).reset_index()\n",
    "    activities_wide = activities_wide.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Data and Activities Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Merging Wide fomart export data with binary wide data\n",
    "    act_final = pd.merge(\n",
    "        act_dat_joined,\n",
    "        activities_wide,\n",
    "        how=\"left\",\n",
    "        left_on=[\"version\", \"activity_id\"],\n",
    "        right_on=[\"version\", \"activity_id\"],\n",
    "    )\n",
    "\n",
    "    # Ensuring schedule time shows up\n",
    "    act_final[\"schedule_Time\"] = np.where(\n",
    "        act_final[\"schedule_Time\"].isna(),\n",
    "        act_final[\"scheduled_Time_start\"],\n",
    "        act_final[\"schedule_Time\"],\n",
    "    )\n",
    "\n",
    "    # Final Timestamp formatting\n",
    "    act_final[\"schedule_Time\"] = pd.to_datetime(act_final[\"schedule_Time\"])\n",
    "    act_final[\"start_Time\"] = pd.to_datetime(act_final[\"start_Time\"])\n",
    "    act_final[\"end_Time\"] = pd.to_datetime(act_final[\"end_Time\"])\n",
    "\n",
    "    # Creating activity_flow column to match the flow final data\n",
    "    act_final[\"activity_flow\"] = np.nan\n",
    "\n",
    "    # Check to make sure you see all activity names\n",
    "    list(activities_wide.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activities Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################### PLEASE SELECT APPROPRIATE ITEMS & ORDER ############################################################################\n",
    "\n",
    "# selecting the required columns and ordering\n",
    "if act_history_exist == 1:\n",
    "    act_final = act_final[\n",
    "        [\n",
    "            \"userId\",\n",
    "            \"event_id\",\n",
    "            \"id\",\n",
    "            \"secret_user_id\",\n",
    "            \"activity_flow\",\n",
    "            \"schedule_Time\",\n",
    "            \"start_Time\",\n",
    "            \"end_Time\",\n",
    "            \"version\",\n",
    "            \" Sleep \",\n",
    "            \"Activity Watch\",\n",
    "            \"Context of Assessment\",\n",
    "            \"Diet\",\n",
    "            \"Diet Test Activity schedule\",  # TESTING ACTIVITY\n",
    "            \"Food and Drink Intake\",\n",
    "            \"Food and Drink Intake (Supplemental)\",  # Only in Testing\n",
    "            \"Life Events\",\n",
    "            \"Light Device\",\n",
    "            \"Menstrual Period\",\n",
    "            \"Mood Circumplex\",\n",
    "            \"Pain\",\n",
    "            \"Physical Activity\",\n",
    "            \"Physical Activity (with Supplement)\",  # Only in Testing\n",
    "            \"Physical Health\",\n",
    "            \"Procedure Tracking\",  # Only in Testing\n",
    "            \"Saliva Sample\",\n",
    "            \"Substance Use\",  # Only in Testing\n",
    "            \"Substance Use (AM)\",  # Only in Testing\n",
    "            \"Substance Use (Core)\",  # Only in Testing\n",
    "            \"Substance Use (Supplement)\",\n",
    "            \"saliva_instructions\",\n",
    "            \"since_activity_monitor\",\n",
    "            \"since_activity_monitor_time_start\",\n",
    "            \"since_activity_monitor_time_end\",\n",
    "            \"since_light_device\",\n",
    "            \"since_light_device_time_start\",\n",
    "            \"since_light_device_time_end\",\n",
    "            \"morning_bedtime\",\n",
    "            \"morning_lights_off\",\n",
    "            \"morning_fall_asleep\",\n",
    "            \"morning_wake_number\",\n",
    "            \"morning_awakenings_last\",\n",
    "            \"morning_waketime\",\n",
    "            \"morning_outbed\",\n",
    "            \"morning_sleep_quality\",\n",
    "            \"morning_sleep_refreshed\",\n",
    "            \"morning_sleep_problems\",\n",
    "            \"morning_sleeping_pills\",\n",
    "            \"morning_sleeping_pills_type\",\n",
    "            \"now_gps_lat\",\n",
    "            \"now_gps_long\",\n",
    "            \"now_where\",\n",
    "            \"now_inside\",\n",
    "            \"now_outside\",\n",
    "            \"since_where\",\n",
    "            \"since_inside\",\n",
    "            \"since_outside\",\n",
    "            \"now_company\",\n",
    "            # 'now_company_confirm',\n",
    "            \"since_company\",\n",
    "            \"now_activity\",\n",
    "            \"since_activity\",\n",
    "            \"since_internet\",\n",
    "            \"internet_use_duration\",\n",
    "            \"internet_use_category\",\n",
    "            \"videogame_duration\",\n",
    "            \"socialmedia_duration\",\n",
    "            \"socialmedia_activity\",\n",
    "            \"friends_communication_method\",\n",
    "            \"strangers_communication_method\",\n",
    "            \"comments_byothers\",\n",
    "            \"comments_byself\",\n",
    "            \"now_sadness\",\n",
    "            \"now_anxiousness\",\n",
    "            \"now_active\",\n",
    "            \"now_tired\",\n",
    "            \"now_distracted\",\n",
    "            \"now_irritable\",\n",
    "            \"now_quick_thinking\",\n",
    "            \"now_enjoyment\",\n",
    "            \"now_fidgety\",\n",
    "            \"now_thoughts_positive\",\n",
    "            \"now_thoughts_negative\",\n",
    "            \"now_thoughts_negative_about\",\n",
    "            \"now_thoughts_negative_severity\",\n",
    "            \"since_thoughts_negative\",\n",
    "            \"since_thoughts_negative_suicide\",\n",
    "            \"since_thoughts_negative_suicide_message\",\n",
    "            \"instructions\",  # event_instructions\n",
    "            \"event_emotion\",\n",
    "            \"event_category\",\n",
    "            \"event_people\",\n",
    "            \"event_where\",\n",
    "            \"event_health\",\n",
    "            \"event_content\",\n",
    "            \"event_issue\",\n",
    "            \"event_stress\",\n",
    "            \"since_physical_activity\",\n",
    "            \"since_sedentary\",\n",
    "            \"since_nap_rest_time\",\n",
    "            \"since_rest_duration\",\n",
    "            \"since_rest_fell_asleep\",\n",
    "            \"since_sleep_duration\",\n",
    "            \"since_vigorous_activity\",\n",
    "            \"since_moderate_activity\",\n",
    "            \"since_light_activity\",\n",
    "            \"now_thirsty\",\n",
    "            \"since_had_drink\",\n",
    "            \"not_alcohol_amount\",\n",
    "            \"since_had_drink_alcohol_type\",\n",
    "            \"since_had_drink_alcohol_quantity\",\n",
    "            \"alcohol_time\",\n",
    "            \"since_feel_drink\",\n",
    "            \"since_had_drink_caffeinated_type\",\n",
    "            \"now_hungry\",\n",
    "            \"since_times_eat\",\n",
    "            \"since_eaten_amount\",\n",
    "            \"since_eaten_when\",\n",
    "            \"since_eaten_type\",\n",
    "            \"since_food1\",\n",
    "            \"since_food2\",\n",
    "            \"since_food3\",\n",
    "            \"since_food4\",\n",
    "            \"since_food5\",\n",
    "            \"since_crave\",\n",
    "            \"craving_strong_tobacco\",\n",
    "            \"craving_strong_cannabis\",\n",
    "            \"craving_strong_otherdrug\",\n",
    "            \"craving_strong_alcohol\",\n",
    "            \"since_substances\",\n",
    "            \"substances_tobacco\",\n",
    "            \"since_substances_cigarettes\",\n",
    "            \"since_substances_cigarettes_time\",\n",
    "            \"since_substances_enicotine\",\n",
    "            \"since_substances_enicotine_time\",\n",
    "            \"since_substances_other_tobacco\",\n",
    "            \"since_substances_other_tobacco_time\",\n",
    "            \"since_cannabis_type\",\n",
    "            \"since_cannabis_time\",\n",
    "            \"since_substances_cannabis\",\n",
    "            \"since_cannabis_high\",\n",
    "            \"since_substances_other\",\n",
    "            \"since_substances_other_specify\",\n",
    "            \"since_substances_other_time\",\n",
    "            \"since_substances_other_high\",\n",
    "            \"since_substances_company\",\n",
    "            \"now_pain\",\n",
    "            \"now_pain_where\",\n",
    "            \"since_pain\",\n",
    "            \"since_pain_where\",\n",
    "            \"pain_intensity\",\n",
    "            \"headache\",\n",
    "            \"headache_prevent\",\n",
    "            \"headache_time_start\",\n",
    "            \"headache_time_end\",\n",
    "            \"headache_intensity\",\n",
    "            \"headache_location\",\n",
    "            \"headache_pulsating\",\n",
    "            \"headache_effort\",\n",
    "            \"headache_nausea\",\n",
    "            \"headache_light\",\n",
    "            \"headache_heat\",\n",
    "            \"headache_noise\",\n",
    "            \"headache_smell\",\n",
    "            \"headache_trigger\",\n",
    "            \"headache_vision_changes\",\n",
    "            \"headache_vision_change_time\",\n",
    "            \"headache_numbing\",\n",
    "            \"headache_numbing_time\",\n",
    "            \"headache_confusing\",\n",
    "            \"headache_confusing_time\",\n",
    "            \"headache_sudden\",\n",
    "            \"headache_medication\",\n",
    "            \"headache_interference\",\n",
    "            \"day_physical_health\",\n",
    "            \"day_problem_categories\",\n",
    "            \"day_bother\",\n",
    "            \"day_over_medication\",\n",
    "            \"day_over_medication_why\",\n",
    "            \"day_prescribed_medication\",\n",
    "            \"day_prescribed_medication_conditions\",\n",
    "            \"day_problems_allergies\",\n",
    "            \"day_problems_breath\",\n",
    "            \"day_problems_belly_symptoms\",\n",
    "            \"day_problems_belly\",\n",
    "            \"day_problems_muscle\",\n",
    "            \"day_problems_heart\",\n",
    "            \"dizziness_situation\",\n",
    "            \"dizziness_faint\",\n",
    "            \"day_lethargic\",\n",
    "            \"activity_planned\",\n",
    "            \"day_period\",\n",
    "            ##New items (2024 Jul 31st)\n",
    "            \"audio_test\",\n",
    "            \"headache_current\",\n",
    "            \"headache_same\",\n",
    "            \"since_activity_monitor_now\",\n",
    "            # 'since_cannabis_craving',\n",
    "            # 'since_light_activity_planned',\n",
    "            \"since_light_device_now\",\n",
    "            # 'since_moderate_activity_planned',\n",
    "            # 'since_substances_other_crave',\n",
    "            # 'since_vigorous_activity_planned',\n",
    "            # 'substances_lastnight_time',\n",
    "            # 'substances_lastnight'\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Ensuring all NAs apper \"NA\"\n",
    "    act_final_out = act_final.copy()\n",
    "    act_final_out.replace(\"nan\", \"NA\", inplace=True)\n",
    "    act_final_out.fillna(\"NA\", inplace=True)\n",
    "    act_final_out.replace(\"NA NA\", \"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see activities final output looks good.\n",
    "if act_history_exist == 1:\n",
    "    act_final_out.to_csv(\n",
    "        os.path.join(output_path, \"activity_final.csv\"),\n",
    "        index=False,\n",
    "        date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "        na_rep=\"NA\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Joining flow flow and activity final togeter\n",
    "    all_final = [final_out, act_final_out]\n",
    "    ema_out = pd.concat(all_final, ignore_index=True)\n",
    "\n",
    "    # Included similar to R script to ensure alignment with formatting\n",
    "    ema_out = ema_out.applymap(str)\n",
    "\n",
    "    # Final data output sorting\n",
    "    ema_out = ema_out.sort_values(by=[\"secret_user_id\", \"schedule_Time\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # checking for duplicates in final file. Ensure this always returns \"False\"\n",
    "    print(ema_out.duplicated().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Preview final output\n",
    "    ema_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Write the final output file to csv\n",
    "\n",
    "    ema_out.to_csv(\n",
    "        os.path.join(output_path, \"ema_output_final_v2.csv\"),\n",
    "        index=False,\n",
    "        date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "        na_rep=\"NA\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
