{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ruff: noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "\n",
    "input_path = Path(\n",
    "    \"/Users/sailaja.yenepalli/Documents/scripts/a_Export Enhancements/TestLookUp/input\"\n",
    ")\n",
    "output_path = Path(\n",
    "    \"/Users/sailaja.yenepalli/Documents/scripts/a_Export Enhancements/TestLookUp/output\"\n",
    ")\n",
    "\n",
    "# input_path = Path('/Users/minji.kang/Documents/NGDT/Data_export_management/Report_CSV_Preprocessing_Generic_Script/MiResourceExtended/input/')\n",
    "# output_path = Path('/Users/minji.kang/Documents/NGDT/Data_export_management/Report_CSV_Preprocessing_Generic_Script/MiResourceExtended/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_response_files(input_dir):\n",
    "    \"\"\"Reads and combines all CSV files starting with 'report' from the specified directory.\"\"\"\n",
    "    try:\n",
    "        # Find all files starting with 'report' in the directory\n",
    "        report_files = input_dir.glob(\"report*.csv\")\n",
    "\n",
    "        # Read and combine CSV files on the fly\n",
    "        combined_df = pd.concat(\n",
    "            (pd.read_csv(file, encoding=\"ISO-8859-1\") for file in report_files),\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        # Rename the first column\n",
    "        combined_df.rename(\n",
    "            columns={\n",
    "                combined_df.columns[0]: \"activity_submission_id\",\n",
    "                \"activity_start_time\": \"activity_start_time_utc\",\n",
    "                \"activity_end_time\": \"activity_end_time_utc\",\n",
    "                \"activity_scheduled_time\": \"activity_scheduled_time_utc\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory {input_dir} not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: One or more files are empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    # Return an empty DataFrame if an error occurs\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Process the files using input path and save to output path\n",
    "if input_path.exists():\n",
    "    response_data = load_and_merge_response_files(input_path)\n",
    "    if not response_data.empty:\n",
    "        output_file = output_path / \"report_all.csv\"\n",
    "        output_path.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "        response_data.to_csv(output_file, index=False)\n",
    "        print(f\"Combined report saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No data to combine or no matching files found.\")\n",
    "else:\n",
    "    print(f\"Input directory does not exist: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_applet_data_dict(data):\n",
    "    \"\"\"Extracts applet details and saves to a seperate file as applet dictionary\"\"\"\n",
    "    return data[\n",
    "        [\n",
    "            \"version\",\n",
    "            \"activity_flow_id\",\n",
    "            \"activity_flow_name\",\n",
    "            \"activity_id\",\n",
    "            \"activity_name\",\n",
    "            \"item_id\",\n",
    "            \"item\",\n",
    "            \"prompt\",\n",
    "            \"options\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "\n",
    "# Process the response data and save applet data dictionary to CSV\n",
    "applet_data_dict = extract_applet_data_dict(response_data)\n",
    "applet_data_dict.to_csv(output_path / \"applet_data_dict.csv\", index=False)\n",
    "print(f\"Applet data dictionary saved to: {output_path / 'applet_data_dict.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subscale_transform_long_format(data):\n",
    "    \"\"\"Transforms subscale columns into rows\"\"\"\n",
    "    # Remove 'legacy_user_id' if it exists\n",
    "    if \"legacy_user_id\" in data.columns:\n",
    "        data = data.drop(columns=[\"legacy_user_id\"])\n",
    "\n",
    "    id_vars = data[\n",
    "        [\n",
    "            \"activity_submission_id\",\n",
    "            \"activity_flow_submission_id\",\n",
    "            \"activity_scheduled_time_utc\",\n",
    "            \"activity_start_time_utc\",\n",
    "            \"activity_end_time_utc\",\n",
    "            \"flag\",\n",
    "            \"secret_user_id\",\n",
    "            \"userId\",\n",
    "            \"source_user_subject_id\",\n",
    "            \"source_user_secret_id\",\n",
    "            \"source_user_nickname\",\n",
    "            \"source_user_relation\",\n",
    "            \"source_user_tag\",\n",
    "            \"target_user_subject_id\",\n",
    "            \"target_user_secret_id\",\n",
    "            \"target_user_nickname\",\n",
    "            \"target_user_tag\",\n",
    "            \"input_user_subject_id\",\n",
    "            \"input_user_secret_id\",\n",
    "            \"input_user_nickname\",\n",
    "            \"activity_id\",\n",
    "            \"activity_name\",\n",
    "            \"activity_flow_id\",\n",
    "            \"activity_flow_name\",\n",
    "            \"version\",\n",
    "            \"reviewing_id\",\n",
    "            \"event_id\",\n",
    "            \"timezone_offset\",\n",
    "        ]\n",
    "    ].columns.to_list()\n",
    "\n",
    "    value_vars = data.columns[data.columns.get_loc(\"timezone_offset\") + 1 :].tolist()\n",
    "\n",
    "    if not value_vars:  # Check if the list is empty\n",
    "        print(\" No Subscale Scores Present\")\n",
    "    else:\n",
    "        # Reshape the DataFrame using melt for columns after 'timezone_offset'\n",
    "        reshaped_data = data.melt(\n",
    "            id_vars=id_vars,  # Columns to keep as identifiers\n",
    "            value_vars=value_vars,  # Columns to reshape\n",
    "            var_name=\"item\",  # New column to hold column names\n",
    "            value_name=\"response\",  # New column to hold corresponding values\n",
    "        ).dropna(subset=[\"response\"])\n",
    "\n",
    "        subscale_names = [\n",
    "            x.replace(\"Optional text for \", \"\")\n",
    "            for x in reshaped_data[\"item\"].unique()\n",
    "            if re.match(r\"^Optional text for \", x)\n",
    "            and x != \"Optional text for Final SubScale Score\"\n",
    "        ]\n",
    "\n",
    "        # Classify score types\n",
    "        reshaped_data[\"score_type\"] = reshaped_data[\"item\"].apply(\n",
    "            lambda x: \"finalscore\"\n",
    "            if x == \"Final SubScale Score\"\n",
    "            else \"finalscore_text\"\n",
    "            if x == \"Optional text for Final SubScale Score\"\n",
    "            #'lookup' if x in subscale_names else\n",
    "            else \"lookup_text\"\n",
    "            if re.match(r\"^Optional text for \", x)\n",
    "            else \"subscale\"\n",
    "        )\n",
    "\n",
    "        # Transform item names based on score types\n",
    "        def transform_item(row):\n",
    "            if row[\"score_type\"] == \"finalscore\":\n",
    "                return \"activity_score\"\n",
    "            if row[\"score_type\"] == \"finalscore_text\":\n",
    "                return \"activity_score_lookup_text\"\n",
    "            # elif row['score_type'] == 'lookup':\n",
    "            #    return 'subscale_lookup_' + row['item']\n",
    "            if row[\"score_type\"] == \"lookup_text\":\n",
    "                return \"subscale_lookup_text_\" + row[\"item\"].replace(\n",
    "                    \"Optional text for \", \"\"\n",
    "                )\n",
    "            return \"subscale_name_\" + row[\"item\"]\n",
    "\n",
    "        reshaped_data[\"item\"] = reshaped_data.apply(transform_item, axis=1)\n",
    "\n",
    "        # Add additional computed columns\n",
    "        reshaped_data = reshaped_data.drop(columns=[\"score_type\"]).assign(\n",
    "            item_id=\"\", prompt=\"\", options=\"\", rawScore=\"\"\n",
    "        )\n",
    "\n",
    "        # Prepare a subset of the original DataFrame for alignment\n",
    "        subset_data = data[reshaped_data.columns.tolist()]\n",
    "\n",
    "        # Combine the subset and reshaped DataFrame\n",
    "        return pd.concat([subset_data, reshaped_data], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "subscale_tranformed_data_init = subscale_transform_long_format(response_data)\n",
    "if subscale_tranformed_data_init is not None:\n",
    "    subscale_tranformed_data_init.to_csv(\n",
    "        output_path / \"subscale_long_data.csv\", index=False\n",
    "    )\n",
    "    subscale_tranformed_data = subscale_tranformed_data_init.copy()\n",
    "else:\n",
    "    subscale_tranformed_data = response_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_epochtime(data, column_name):\n",
    "    \"\"\"Convert epoch time in milliseconds to datetime.\"\"\"\n",
    "    return pd.to_datetime(\n",
    "        pd.to_numeric(data[column_name], errors=\"coerce\") / 1000, unit=\"s\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply the function to multiple columns using a loop\n",
    "for col in [\n",
    "    \"activity_start_time_utc\",\n",
    "    \"activity_end_time_utc\",\n",
    "    \"activity_scheduled_time_utc\",\n",
    "]:\n",
    "    subscale_tranformed_data[col] = format_epochtime(subscale_tranformed_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process responses to clean and format time entries\n",
    "\n",
    "\n",
    "def format_response(data):\n",
    "    formatted_responses = []\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        response = row.get(\"response\", None)\n",
    "\n",
    "        # Ensure response is a string or NaN\n",
    "        if not isinstance(response, str):\n",
    "            response = str(response) if not pd.isna(response) else np.nan\n",
    "\n",
    "        # Clean responses\n",
    "        if isinstance(response, str):\n",
    "            if \"geo:\" in response:\n",
    "                geo_match = re.search(\n",
    "                    r\"geo:\\s*lat\\s*\\((.*?)\\)\\s*/\\s*long\\s*\\((.*?)\\)\", response\n",
    "                )\n",
    "                if geo_match:\n",
    "                    lat, long = geo_match.groups()\n",
    "                    formatted_responses.append(f\"{lat.strip()}/{long.strip()}\")\n",
    "                    continue\n",
    "\n",
    "            if \"value:\" in response:\n",
    "                formatted_responses.append(re.sub(r\"value:\\s*\", \"\", response))\n",
    "                continue\n",
    "\n",
    "            if \"date:\" in response:\n",
    "                formatted_responses.append(re.sub(r\"date:\\s*\", \"\", response))\n",
    "                continue\n",
    "\n",
    "            if pd.isna(response):  # Handle NaN explicitly\n",
    "                formatted_responses.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            if \"time:\" in response:\n",
    "                time_match = re.search(r\"hr\\s*(\\d{1,2}),\\s*min\\s*(\\d{1,2})\", response)\n",
    "                if time_match:\n",
    "                    hour, minute = map(int, time_match.groups())\n",
    "                    formatted_responses.append(f\"{hour:02}:{minute:02}\")\n",
    "                    continue\n",
    "                formatted_responses.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            if \"time_range:\" in response:\n",
    "                try:\n",
    "                    clean_time = re.sub(r\"[a-zA-Z\\s+(\\)_:]\", \"\", response).replace(\n",
    "                        \",\", \":\"\n",
    "                    )\n",
    "                    time_parts = clean_time.split(\"/\")\n",
    "                    formatted_parts = [\n",
    "                        f\"{part.split(':')[0].zfill(2)}:{part.split(':')[1].zfill(2)}\"\n",
    "                        for part in time_parts\n",
    "                    ]\n",
    "                    formatted_responses.append(\"/\".join(formatted_parts))\n",
    "                except (IndexError, ValueError):\n",
    "                    formatted_responses.append(np.nan)\n",
    "                continue\n",
    "\n",
    "        # Fallback case\n",
    "        formatted_responses.append(response)\n",
    "\n",
    "    return pd.Series(formatted_responses)\n",
    "\n",
    "\n",
    "subscale_tranformed_data[\"formatted_response\"] = format_response(\n",
    "    subscale_tranformed_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = timestamp_conv_data.copy()\n",
    "# test = test[test['activity_submission_id']=='5bc2fa7a-49db-49b8-b672-685dfbb2f899'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps responses to scores based on the options column in the DataFrame.\n",
    "\n",
    "\n",
    "def response_value_score_mapping(data):\n",
    "    response_scores = []\n",
    "    response_values = []\n",
    "\n",
    "    for options, response in zip(data[\"options\"], data[\"response\"], strict=False):\n",
    "        # Ensure 'options' and 'response' are valid strings\n",
    "        if not isinstance(options, str) or not isinstance(response, str):\n",
    "            response_scores.append(np.nan)\n",
    "            response_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Check if options contain scores\n",
    "        if \"score: \" in options:\n",
    "            # Parse options and responses\n",
    "            split_options = [\n",
    "                opt.strip() for opt in options.strip().split(\"),\") if \"(score\" in opt\n",
    "            ]\n",
    "            split_response = [\n",
    "                resp.strip() for resp in response.strip().split(\": \")[1].split(\",\")\n",
    "            ]\n",
    "\n",
    "            # Build the score mapping dictionary\n",
    "            scores = {\n",
    "                opt.split(\": \")[1].split(\" \")[0]:  # Extract position part\n",
    "                opt.split(\"score: \")[1].strip(\" )\")  # Extract score part\n",
    "                for opt in split_options\n",
    "                if \"score: \" in opt\n",
    "            }\n",
    "\n",
    "            # Map responses to scores\n",
    "            response_score_mapping = [\n",
    "                scores.get(resp, \"N/A\") for resp in split_response\n",
    "            ]\n",
    "            response_scores.append(\", \".join(response_score_mapping))\n",
    "            response_values.append(np.nan)\n",
    "\n",
    "        elif \": \" in options:\n",
    "            if \"Min: 0, Max:\" in options:\n",
    "                max_value = re.sub(r\"Min: 0, Max: \", \"\", options)\n",
    "                max_value = int(max_value)\n",
    "\n",
    "                if max_value > 1:\n",
    "                    slider_response = re.sub(\"value: \", \"\", response)\n",
    "                    response_values.append(\", \".join(slider_response))\n",
    "                    response_scores.append(np.nan)\n",
    "\n",
    "            else:\n",
    "                value_options = \", \" + options + \",\"\n",
    "                split_options_text = [\n",
    "                    opt.strip() for opt in re.findall(r\",\\s(.*?):\", value_options)\n",
    "                ]\n",
    "                split_options_value = [\n",
    "                    opt.strip() for opt in re.findall(r\":\\s(\\d+),\", value_options)\n",
    "                ]\n",
    "                split_response_values = [\n",
    "                    resp.strip() for resp in response.strip().split(\": \")[1].split(\",\")\n",
    "                ]\n",
    "\n",
    "                # Build actual response mapping\n",
    "                values = {\n",
    "                    value: text  # Map position (value) to response text\n",
    "                    for text, value in zip(\n",
    "                        split_options_text, split_options_value, strict=False\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                # Map response positions to actual values\n",
    "                response_value_mapping = [\n",
    "                    values.get(resp, re.sub(\"value: \", \"\", response))\n",
    "                    for resp in split_response_values\n",
    "                ]\n",
    "                response_values.append(\", \".join(response_value_mapping))\n",
    "                response_scores.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            response_scores.append(np.nan)\n",
    "            response_values.append(np.nan)\n",
    "\n",
    "    return pd.Series(response_values), pd.Series(response_scores)\n",
    "\n",
    "\n",
    "(\n",
    "    subscale_tranformed_data[\"response_values\"],\n",
    "    subscale_tranformed_data[\"response_scores\"],\n",
    ") = response_value_score_mapping(subscale_tranformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = subscale_tranformed_data.copy()\n",
    "test_final[\"merged_responses\"] = (\n",
    "    test_final[\"response_scores\"]\n",
    "    .combine_first(test_final[\"response_values\"])\n",
    "    .combine_first(test_final[\"formatted_response\"])\n",
    ")\n",
    "check_df = test_final[\n",
    "    [\n",
    "        \"userId\",\n",
    "        \"activity_id\",\n",
    "        \"activity_name\",\n",
    "        \"item_id\",\n",
    "        \"item\",\n",
    "        \"response\",\n",
    "        \"options\",\n",
    "        \"response_scores\",\n",
    "        \"response_values\",\n",
    "        \"formatted_response\",\n",
    "        \"merged_responses\",\n",
    "    ]\n",
    "]\n",
    "check_df.to_csv(\n",
    "    os.path.join(output_path, \"CHECK_response_value_score_mapping.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column list and response column name\n",
    "mycolumn_list = [\n",
    "    \"userId\",\n",
    "    \"secret_user_id\",\n",
    "    \"source_user_secret_id\",\n",
    "    \"target_user_secret_id\",\n",
    "    \"input_user_secret_id\",\n",
    "    \"activity_start_time_utc\",\n",
    "    \"activity_end_time_utc\",\n",
    "    \"activity_scheduled_time_utc\",\n",
    "    \"activity_flow_id\",\n",
    "    \"activity_flow_name\",\n",
    "    \"activity_id\",\n",
    "    \"activity_name\",\n",
    "    \"event_id\",\n",
    "    \"version\",\n",
    "]\n",
    "\n",
    "\n",
    "def widen_data(data, column_list):\n",
    "    \"\"\"Transforms data into a wide format based on the specified column list.\"\"\"\n",
    "    # merge formatted response, values and scores created a single response field\n",
    "    data = data.copy()\n",
    "    data[\"merged_responses\"] = (\n",
    "        data[\"response_scores\"]\n",
    "        .combine_first(data[\"response_values\"])\n",
    "        .combine_first(data[\"formatted_response\"])\n",
    "    )\n",
    "\n",
    "    # Convert datetime columns to string and handle NaT\n",
    "    datetime_cols = data.select_dtypes(include=[\"datetime\"]).columns\n",
    "    data[datetime_cols] = data[datetime_cols].astype(str).replace(\"NaT\", \"\")\n",
    "\n",
    "    # Fill missing values in specified columns\n",
    "    data[column_list] = data[column_list].fillna(\"\")\n",
    "\n",
    "    # Group by the column list and combine IDs\n",
    "    answers = (\n",
    "        data.groupby(column_list)[\"activity_submission_id\"]\n",
    "        .apply(lambda x: \"|\".join(x.astype(str)))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create combined column names\n",
    "    data[\"combined_cols\"] = (\n",
    "        \"activityName[\"\n",
    "        + data[\"activity_name\"]\n",
    "        + \"]_itemName[\"\n",
    "        + data[\"item\"].astype(str)\n",
    "        + \"]_itemId[\"\n",
    "        + data[\"item_id\"].astype(str)\n",
    "        + \"]\"\n",
    "    )\n",
    "    data[\"combined_cols\"] = np.where(\n",
    "        data[\"combined_cols\"].str.contains(\"_itemId[]\", regex=False),\n",
    "        data[\"combined_cols\"].str.replace(\"_itemId[]\", \"\", regex=False),\n",
    "        data[\"combined_cols\"],\n",
    "    )\n",
    "\n",
    "    # Select relevant columns for pivoting\n",
    "    subset_columns = column_list + [\"combined_cols\", \"merged_responses\"]\n",
    "    dat_subset = data[subset_columns]\n",
    "\n",
    "    # Pivot the data into wide format\n",
    "    dat_wide = pd.pivot_table(\n",
    "        dat_subset,\n",
    "        index=column_list,\n",
    "        columns=\"combined_cols\",\n",
    "        values=\"merged_responses\",\n",
    "        aggfunc=\"last\",\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge with the combined IDs\n",
    "    dat_wide = pd.merge(dat_wide, answers, on=column_list, how=\"outer\")\n",
    "\n",
    "    return dat_wide\n",
    "\n",
    "\n",
    "# Apply the function to process data into wide format\n",
    "data_wide = widen_data(subscale_tranformed_data, mycolumn_list)\n",
    "\n",
    "# Save the output to CSV\n",
    "data_wide.to_csv(os.path.join(output_path, \"data_wide_all.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
