#!/usr/bin/env python3
# ========= IMMEDIATE LOG SUPPRESSION (MUST BE FIRST) =========
import sys
import os
import warnings
import logging

# üö´ KILL ALL LOGS IMMEDIATELY (BEFORE ANY OTHER IMPORTS)
class DevNull:
    def write(self, msg): pass
    def flush(self): pass

# Store original stdout
ORIGINAL_STDOUT = sys.stdout
ORIGINAL_STDERR = sys.stderr

# Function to suppress all logs
def suppress_all_logs_immediately():
    """Suppress ALL logs immediately including imports"""
    # Redirect stdout and stderr
    sys.stdout = DevNull()
    sys.stderr = DevNull()
    
    # Kill all warnings
    warnings.filterwarnings("ignore")
    
    # Set environment variables for third-party library log suppression
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
    os.environ["TRANSFORMERS_VERBOSITY"] = "error" 
    os.environ["PYTHONWARNINGS"] = "ignore"
    
    # Disable all logging
    logging.disable(logging.CRITICAL)
    
    # Set all loggers to CRITICAL level
    logging.getLogger().setLevel(logging.CRITICAL)
    logging.getLogger("ingest").setLevel(logging.CRITICAL)
    logging.getLogger("qa_engine").setLevel(logging.CRITICAL)
    logging.getLogger("chromadb").setLevel(logging.CRITICAL)
    logging.getLogger("langchain").setLevel(logging.CRITICAL)
    logging.getLogger("transformers").setLevel(logging.CRITICAL)
    logging.getLogger("sentence_transformers").setLevel(logging.CRITICAL)

def restore_stdout_for_animation():
    """Restore stdout only for animation"""
    sys.stdout = ORIGINAL_STDOUT
    sys.stderr = ORIGINAL_STDOUT  # Redirect stderr to stdout

def suppress_logs_after_animation():
    """Suppress logs again after animation"""
    sys.stdout = DevNull()
    sys.stderr = DevNull()

# üî• APPLY SUPPRESSION IMMEDIATELY
suppress_all_logs_immediately()

try:
    from startup import (
        ultra_futuristic_startup,
        random_startup_personality,
        suppress_all_logs,
        FuturisticSoundSystem
    )
    FUTURISTIC_STARTUP_AVAILABLE = True
except ImportError:
    # This won't be visible now
    FUTURISTIC_STARTUP_AVAILABLE = False

import asyncio
import os
import sys
import json
import time
import threading
import sqlite3
import logging
logger = logging.getLogger(__name__)
import hashlib
import re
import requests
import random
import pickle
import base64
import subprocess
import webbrowser
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
from collections import defaultdict, deque, Counter
from pathlib import Path
import numpy as np
import tkinter as tk
from tkinter import filedialog

from plugin import PluginManager
from convo_history import SmartExportSystem
from context import AdvancedContextualMemorySystem

import ingest 
import qa_engine

try:
    from qa_engine import create_qa_engine
    QA_ENGINE_AVAILABLE = True
except ImportError:
    QA_ENGINE_AVAILABLE = False
    def create_qa_engine(**kwargs):
        raise ImportError("QA Engine not available")

try:
    from ingest import list_processed_repositories, smart_ingest_repository
    REPOSITORY_REGISTRY_AVAILABLE = True
except ImportError:
    REPOSITORY_REGISTRY_AVAILABLE = False
    print("‚ö†Ô∏è Repository registry not available")

try:
    # from new_claude import SmartEnhancementDetector
    # SMART_ENHANCEMENT_AVAILABLE = True
    # print("Smart Enhancement Detection loaded!")
    SMART_ENHANCEMENT_AVAILABLE = False
    print("Multi-Candidate disabled - using single response mode")
except ImportError:
    SMART_ENHANCEMENT_AVAILABLE = False
    print("Multi-Candidate disabled - using single response mode")
    
# Performance optimization - disable warnings
import warnings
warnings.filterwarnings("ignore")

# Fast import setup
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Multi-folder import solution (optimized)
project_root = os.path.dirname(os.path.abspath(__file__))
folders_to_add = [
    'src',
    os.path.join('src', 'memory'),
    os.path.join('src', 'unique_features'),
    os.path.join('src', 'agents'),
    'ML',
    os.path.join('ML', 'models'),
    os.path.join('ML', 'training'),
    os.path.join('ML', 'mlops'),
    os.path.join('ML', 'monitoring'),
]

for folder in folders_to_add:
    folder_path = os.path.join(project_root, folder)
    if os.path.exists(folder_path) and folder_path not in sys.path:
        sys.path.insert(0, folder_path)

# Fast environment loading
from dotenv import load_dotenv
load_dotenv()

# Textual UI imports (OPTIMIZED & ERROR-FREE)
try:
    from textual.app import App, ComposeResult
    from textual.containers import Container, Horizontal, Vertical, ScrollableContainer
    from textual.widgets import (
        Header, Footer, Button, Input, Log, DataTable, Tree,
        ListView, ListItem, Label, Pretty, Markdown, ProgressBar,
        TabbedContent, TabPane, Select, Switch, Checkbox, Static,
        Collapsible, LoadingIndicator, Digits, Sparkline, RadioSet,
        RadioButton, OptionList, DirectoryTree, ContentSwitcher, RichLog,
        TextArea  # For enhanced input
    )
    from textual.reactive import reactive, var
    from textual.message import Message
    from textual.binding import Binding
    from textual.screen import ModalScreen, Screen
    from textual.worker import get_current_worker, WorkerState
    from textual import on, work
    from textual.validation import Function, Number, Length
    from textual.css.query import NoMatches
    from textual.suggester import SuggestFromList
    import textual.events as events

    TEXTUAL_AVAILABLE = True
    print("‚úÖ Textual UI loaded - WORLD'S BEST MODE!")
except ImportError as e:
    TEXTUAL_AVAILABLE = False
    print(f"‚ö†Ô∏è Textual UI not available: {e}")

# Rich UI imports (for fallback display)
try:
    import colorama
    from colorama import Fore, Back, Style, init
    init(autoreset=True)
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
    from rich.table import Table
    from rich.markdown import Markdown
    from rich.text import Text
    from rich.align import Align
    from rich.layout import Layout
    from rich.live import Live
    from rich.prompt import Prompt, Confirm
    from rich.tree import Tree
    from rich.columns import Columns
    console = Console()
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("‚ö†Ô∏è Rich UI not available")

# Voice processing imports (Azure + Basic)
try:
    import speech_recognition as sr
    import pyttsx3
    VOICE_AVAILABLE = True
except ImportError:
    VOICE_AVAILABLE = False

# Azure Voice imports (PREMIUM)
try:
    import azure.cognitiveservices.speech as speechsdk
    AZURE_VOICE_AVAILABLE = True
    print("‚úÖ Azure Voice Services loaded!")
except ImportError:
    AZURE_VOICE_AVAILABLE = False
    print("‚ö†Ô∏è Azure Voice not available")

# File processing imports (optimized)
try:
    from PIL import Image
    import PyPDF2
    import docx
    import openpyxl
    import pandas as pd
    FILE_PROCESSING_AVAILABLE = True
    print("‚úÖ File Processing capabilities loaded!")
except ImportError:
    FILE_PROCESSING_AVAILABLE = False
    print("‚ö†Ô∏è File Processing not available")

# Web scraping (free version only)
WEB_SEARCH_AVAILABLE = True  # Always available with requests

# GitHub Integration imports
try:
    import chromadb
    from langchain_community.document_loaders import UnstructuredFileLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
    GITHUB_INTEGRATION = True
    print("‚úÖ GitHub integration loaded!")
except ImportError as e:
    GITHUB_INTEGRATION = False
    print(f"‚ö†Ô∏è GitHub integration not available: {e}")

# Professional Agents Import
try:
    from agents.coding_agent import ProLevelCodingExpert
    from agents.career_coach import ProfessionalCareerCoach
    from agents.business_consultant import SmartBusinessConsultant
    from agents.medical_advisor import SimpleMedicalAdvisor
    from agents.emotional_counselor import SimpleEmotionalCounselor
    from agents.techincal_architect import TechnicalArchitect
    PROFESSIONAL_AGENTS_LOADED = True
    print("‚úÖ Professional agents loaded successfully!")
except ImportError as e:
    PROFESSIONAL_AGENTS_LOADED = False
    print(f"‚ùå Professional agents import failed: {e}")

# Advanced Systems Import
try:
    from memory.sharp_memory import SharpMemorySystem
    from unique_features.smart_orchestrator import IntelligentAPIOrchestrator
    from unique_features.api_drift_detector import APIPerformanceDrifter
    ADVANCED_SYSTEMS = True
    print("‚úÖ Advanced systems loaded!")
except ImportError as e:
    ADVANCED_SYSTEMS = False
    print(f"‚ö†Ô∏è Advanced systems not available: {e}")

# ========== SIMPLE GITHUB INTEGRATION ==========
try:
    from ingest import smart_ingest_repository
    from qa_engine import UltimateQAEngine
    GITHUB_INTEGRATION = True
    print("‚úÖ GitHub integration loaded!")
except Exception as e:
    GITHUB_INTEGRATION = False
    print(f"‚ö†Ô∏è GitHub integration failed: {e}")
# ML System Import
try:
    from ml_integration import EnhancedMLManager
    ML_SYSTEM_AVAILABLE = True
    print("‚úÖ Advanced ML System loaded!")
except ImportError as e:
    ML_SYSTEM_AVAILABLE = False
    print(f"‚ö†Ô∏è ML System not available: {e}")

# ========== ENHANCED CORE CLASSES ==========

class Colors:
    """ANSI Color codes for fallback terminal output"""
    RESET = '\033[0m'
    BOLD = '\033[1m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    PURPLE = '\033[95m'
    MAGENTA = '\033[95m'
    WHITE = '\033[97m'
    ORANGE = '\033[38;5;208m'

class SoundManager:
    """Advanced sound management system"""
    def __init__(self):
        self.sounds_enabled = True
        self.sound_volume = 0.7
        self.sound_cache = {}
        # Try to initialize pygame for better sounds
        try:
            import pygame
            pygame.mixer.init(frequency=22050, size=-16, channels=2, buffer=512)
            self.pygame_available = True
            self.create_sound_effects()
            print("‚úÖ Advanced sound system initialized")
        except ImportError:
            self.pygame_available = False
            print("‚ö†Ô∏è Using basic sound system (install pygame for better sounds)")

    def create_sound_effects(self):
        """Create sound effects using pygame"""
        if not self.pygame_available:
            return
        try:
            import pygame
            import numpy as np
            
            # Create different sound effects
            sample_rate = 22050
            
            # Click sound (short beep)
            duration = 0.1
            frequency = 800
            frames = int(duration * sample_rate)
            arr = np.zeros((frames, 2))
            for i in range(frames):
                wave = 0.3 * np.sin(2 * np.pi * frequency * i / sample_rate)
                arr[i] = [wave, wave]
            arr = (arr * 32767).astype(np.int16)
            click_sound = pygame.sndarray.make_sound(arr)
            self.sound_cache['click'] = click_sound
            
            # Success sound (double beep)
            duration = 0.2
            frames = int(duration * sample_rate)
            arr = np.zeros((frames, 2))
            for i in range(frames):
                if i < frames // 2:
                    wave = 0.3 * np.sin(2 * np.pi * 600 * i / sample_rate)
                else:
                    wave = 0.3 * np.sin(2 * np.pi * 900 * i / sample_rate)
                arr[i] = [wave, wave]
            arr = (arr * 32767).astype(np.int16)
            success_sound = pygame.sndarray.make_sound(arr)
            self.sound_cache['success'] = success_sound
            
            # Error sound (alert)
            duration = 0.3
            frames = int(duration * sample_rate)
            arr = np.zeros((frames, 2))
            for i in range(frames):
                wave = 0.4 * np.sin(2 * np.pi * 400 * i / sample_rate)
                arr[i] = [wave, wave]
            arr = (arr * 32767).astype(np.int16)
            error_sound = pygame.sndarray.make_sound(arr)
            self.sound_cache['error'] = error_sound
            
            # Notification sound
            duration = 0.15
            frames = int(duration * sample_rate)
            arr = np.zeros((frames, 2))
            for i in range(frames):
                wave = 0.25 * np.sin(2 * np.pi * 1000 * i / sample_rate)
                arr[i] = [wave, wave]
            arr = (arr * 32767).astype(np.int16)
            notification_sound = pygame.sndarray.make_sound(arr)
            self.sound_cache['notification'] = notification_sound
            
        except Exception as e:
            print(f"Sound creation error: {e}")

    def play_sound(self, sound_type: str):
        """Play sound effect"""
        if not self.sounds_enabled:
            return
        try:
            if self.pygame_available and sound_type in self.sound_cache:
                sound = self.sound_cache[sound_type]
                sound.set_volume(self.sound_volume)
                sound.play()
            else:
                # Fallback to system beeps
                if sound_type == "click":
                    print("\a", end="", flush=True)
                elif sound_type == "success":
                    print("\a\a", end="", flush=True)
                elif sound_type == "error":
                    print("\a\a\a", end="", flush=True)
                elif sound_type == "notification":
                    print("\a", end="", flush=True)
        except Exception as e:
            print(f"Sound play error: {e}")

    def set_volume(self, volume: float):
        """Set sound volume (0.0 to 1.0)"""
        self.sound_volume = max(0.0, min(1.0, volume))

    def toggle_sounds(self):
        """Toggle sound on/off"""
        self.sounds_enabled = not self.sounds_enabled
        return self.sounds_enabled

    def is_enabled(self) -> bool:
        """Check if sounds are enabled"""
        return self.sounds_enabled

class FileUploadSystem:
    """Complete file upload and analysis system"""
    
    def __init__(self):
        self.supported_formats = {
            '.txt': 'text',
            '.py': 'python',
            '.js': 'javascript',
            '.json': 'json',
            '.md': 'markdown',
            '.csv': 'csv',
            '.pdf': 'pdf',
            '.docx': 'word',
            '.xlsx': 'excel',
            '.html': 'html',
            '.css': 'css',
            '.sql': 'sql',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.xml': 'xml',
            '.yaml': 'yaml',
            '.yml': 'yaml'
        }
    
    def select_file(self) -> Optional[str]:
        """Open file dialog to select file"""
        try:
            root = tk.Tk()
            root.withdraw()  # Hide the root window
            file_path = filedialog.askopenfilename(
                title="Select File to Analyze",
                filetypes=[
                    ("All supported", "*.txt;*.py;*.js;*.json;*.md;*.csv;*.pdf;*.docx;*.xlsx"),
                    ("Text files", "*.txt;*.md"),
                    ("Code files", "*.py;*.js;*.java;*.cpp;*.c;*.html;*.css;*.sql"),
                    ("Data files", "*.csv;*.json;*.xml;*.yaml;*.yml"),
                    ("Document files", "*.pdf;*.docx;*.xlsx"),
                    ("All files", "*.*")
                ]
            )
            root.destroy()
            return file_path if file_path else None
        except Exception as e:
            print(f"File selection error: {e}")
            return None
    
    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        """Analyze uploaded file"""
        try:
            if not os.path.exists(file_path):
                return {"error": "File not found"}
            
            file_ext = os.path.splitext(file_path)[1].lower()
            file_size = os.path.getsize(file_path)
            file_name = os.path.basename(file_path)
            
            # Basic file info
            analysis = {
                "file_name": file_name,
                "file_path": file_path,
                "file_size": file_size,
                "file_extension": file_ext,
                "file_type": self.supported_formats.get(file_ext, "unknown"),
                "content": "",
                "summary": "",
                "analysis": ""
            }
            
            # Read file content based on type
            content = self.read_file_content(file_path, file_ext)
            if content:
                analysis["content"] = content[:5000]  # Limit content for display
                analysis["full_content"] = content
                analysis["lines"] = len(content.split('\n'))
                analysis["words"] = len(content.split())
                analysis["chars"] = len(content)
            
            return analysis
            
        except Exception as e:
            return {"error": f"File analysis failed: {str(e)}"}
    
    def read_file_content(self, file_path: str, file_ext: str) -> Optional[str]:
        """Read file content based on extension"""
        try:
            if file_ext in ['.txt', '.py', '.js', '.json', '.md', '.html', '.css', '.sql', '.java', '.cpp', '.c', '.xml', '.yaml', '.yml']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            
            elif file_ext == '.csv' and FILE_PROCESSING_AVAILABLE:
                df = pd.read_csv(file_path)
                return f"CSV File Analysis:\nRows: {len(df)}\nColumns: {len(df.columns)}\nColumns: {list(df.columns)}\n\nFirst 10 rows:\n{df.head(10).to_string()}"
            
            elif file_ext == '.pdf' and FILE_PROCESSING_AVAILABLE:
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text()
                    return text
            
            elif file_ext == '.docx' and FILE_PROCESSING_AVAILABLE:
                doc = docx.Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            
            elif file_ext == '.xlsx' and FILE_PROCESSING_AVAILABLE:
                df = pd.read_excel(file_path)
                return f"Excel File Analysis:\nRows: {len(df)}\nColumns: {len(df.columns)}\nColumns: {list(df.columns)}\n\nFirst 10 rows:\n{df.head(10).to_string()}"
            
            else:
                return "Binary file - content not displayable"
                
        except Exception as e:
            return f"Error reading file: {str(e)}"

class UltraHybridMemorySystem:
    """Ultra Advanced Hybrid Memory with ALL previous features - FROM enhanced_cli.py"""
    def __init__(self, db_path="nova_ultra_professional_memory.db"):
        # FIXED: Proper path handling
        if not os.path.isabs(db_path):
            self.db_path = os.path.join(os.getcwd(), db_path)
        else:
            self.db_path = db_path
        self.setup_database()
        
        # Memory layers from enhanced_cli.py (great for conversation flow)
        self.conversation_context = deque(maxlen=100)  # Increased capacity
        self.user_profile = {}
        self.emotional_state = "neutral"
        self.learning_patterns = defaultdict(list)
        self.personality_insights = {}
        self.user_preferences = {}
        self.conversation_history = []
        
        # Memory layers from cli.py (great for technical queries)
        self.short_term_memory = deque(maxlen=200)  # Increased capacity
        self.working_memory = {}
        self.conversation_threads = {}
        self.context_memory = {}
        
        # Premium memory features
        self.voice_memory = deque(maxlen=50)
        self.file_memory = {}
        self.search_memory = deque(maxlen=30)
        self.image_memory = deque(maxlen=20)
        
        # Semantic memory for technical queries
        self.setup_semantic_memory()
        print("‚úÖ Ultra Hybrid Memory System initialized")

    def setup_database(self):
        """Setup ultra comprehensive database schema"""
        try:
            # Ensure database directory exists
            db_dir = os.path.dirname(self.db_path)
            if db_dir and not os.path.exists(db_dir):
                os.makedirs(db_dir, exist_ok=True)
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # Enhanced conversations table
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS conversations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    session_id TEXT,
                    user_input TEXT,
                    bot_response TEXT,
                    agent_type TEXT,
                    language TEXT,
                    emotion TEXT,
                    confidence REAL,
                    timestamp DATETIME,
                    feedback INTEGER DEFAULT 0,
                    context_summary TEXT,
                    learned_facts TEXT,
                    satisfaction_rating INTEGER,
                    conversation_thread_id TEXT,
                    intent_detected TEXT,
                    response_time REAL,
                    voice_used BOOLEAN DEFAULT 0,
                    location TEXT,
                    weather_context TEXT,
                    search_queries TEXT
                )
                ''')
                
                # Enhanced user profiles
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_profiles (
                    user_id TEXT PRIMARY KEY,
                    name TEXT,
                    career_goals TEXT,
                    current_role TEXT,
                    experience_years INTEGER,
                    skills TEXT,
                    preferences TEXT,
                    communication_style TEXT,
                    emotional_patterns TEXT,
                    conversation_patterns TEXT,
                    expertise_level TEXT,
                    topics_of_interest TEXT,
                    last_updated DATETIME,
                    total_conversations INTEGER DEFAULT 0,
                    preferred_voice TEXT,
                    location TEXT,
                    timezone TEXT,
                    personality_type TEXT,
                    learning_style TEXT
                )
                ''')
                
                # GitHub repositories
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS github_repos (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    repo_url TEXT UNIQUE,
                    repo_name TEXT,
                    analysis_date DATETIME,
                    file_count INTEGER,
                    languages_detected TEXT,
                    issues_found TEXT,
                    suggestions TEXT,
                    vector_db_path TEXT
                )
                ''')
                
                # Voice interactions
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS voice_interactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    voice_input TEXT,
                    voice_response TEXT,
                    language_detected TEXT,
                    emotion_detected TEXT,
                    voice_engine TEXT,
                    timestamp DATETIME
                )
                ''')
                
                # File processing history
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_processing (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    file_path TEXT,
                    file_type TEXT,
                    processing_result TEXT,
                    timestamp DATETIME,
                    success BOOLEAN
                )
                ''')
                
                # Search history
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS search_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT,
                    search_query TEXT,
                    search_type TEXT,
                    results_count INTEGER,
                    timestamp DATETIME
                )
                ''')
                
                conn.commit()
            print("‚úÖ Ultra Database initialized with premium schema")
        except Exception as e:
            print(f"‚ö†Ô∏è Database setup error: {e}")

    def setup_semantic_memory(self):
        """Setup semantic memory for technical queries"""
        try:
            if ADVANCED_SYSTEMS:
                self.semantic_memory = SharpMemorySystem()
            else:
                self.semantic_memory = None
        except Exception as e:
            print(f"‚ö†Ô∏è Semantic memory setup error: {e}")
            self.semantic_memory = None

    async def remember_conversation(self, user_id: str, session_id: str,
                                  user_input: str, bot_response: str,
                                  agent_type: str, language: str,
                                  emotion: str, confidence: float,
                                  intent: str = None, response_time: float = 0.0,
                                  voice_used: bool = False, location: str = None,
                                  weather_context: str = None, search_queries: str = None,
                                  file_analyzed: str = None):
        """Ultra enhanced conversation memory storage"""
        try:
            # Extract learning points
            learned_facts = self.extract_learning_points(user_input, bot_response)
            context_summary = self.generate_context_summary()
            
            # Store in database
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                INSERT INTO conversations
                (user_id, session_id, user_input, bot_response, agent_type,
                 language, emotion, confidence, timestamp, context_summary,
                 learned_facts, conversation_thread_id, intent_detected, response_time,
                 voice_used, location, weather_context, search_queries)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (user_id, session_id, user_input, bot_response, agent_type,
                      language, emotion, confidence, datetime.now(), context_summary,
                      learned_facts, self.generate_thread_id(), intent, response_time,
                      voice_used, location, weather_context, search_queries))
                conn.commit()
            
            # Store in conversation context
            self.conversation_context.append({
                'user': user_input,
                'bot': bot_response,
                'emotion': emotion,
                'agent': agent_type,
                'timestamp': datetime.now(),
                'voice_used': voice_used,
                'location': location,
                'file_analyzed': file_analyzed
            })
            
            # Store in short-term memory
            memory_entry = {
                'user_id': user_id,
                'session_id': session_id,
                'timestamp': datetime.now().isoformat(),
                'user_input': user_input,
                'ai_response': bot_response,
                'agent_used': agent_type,
                'emotion': emotion,
                'intent': intent,
                'voice_used': voice_used,
                'file_analyzed': file_analyzed
            }
            self.short_term_memory.append(memory_entry)
            
            # Store in semantic memory for technical queries
            if self.semantic_memory and agent_type in ['coding', 'business', 'technical']:
                try:
                    await self.semantic_memory.remember_conversation_advanced(
                        user_input, bot_response,
                        {'agent_used': agent_type, 'emotion': emotion},
                        user_id, session_id
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Semantic memory storage error: {e}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Memory storage error: {e}")


    def get_relevant_context(self, user_input: str, user_id: str, limit: int = 15) -> str:
        """Get ultra comprehensive relevant context"""
        try:
            # Get database context
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                SELECT user_input, bot_response, emotion, learned_facts, agent_type,
                       voice_used, location, weather_context
                FROM conversations
                WHERE user_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
                ''', (user_id, limit))
                conversations = cursor.fetchall()
            
            if not conversations:
                return ""
            
            # Build ultra context summary
            context = "Previous conversation context:\n"
            for conv in conversations:
                context += f"[{conv[4].upper()}] User ({conv[2]}): {conv[0][:80]}...\n"
                context += f"NOVA: {conv[1][:80]}...\n"
                if conv[3]:
                    context += f"Learned: {conv[3]}\n"
                if conv[5]:  # voice_used
                    context += f"[VOICE MODE]\n"
                if conv[6]:  # location
                    context += f"Location: {conv[6]}\n"
                if conv[7]:  # weather_context
                    context += f"Weather: {conv[7]}\n"
                context += "---\n"
            
            return context
        except Exception as e:
            print(f"‚ö†Ô∏è Context retrieval error: {e}")
            return ""

    def extract_learning_points(self, user_input: str, bot_response: str) -> str:
        """Extract learning points from conversation"""
        learning_keywords = [
            "my name is", "i am", "i work", "i like", "i don't like",
            "my preference", "remember that", "important", "my goal",
            "my project", "my problem", "i need help with", "my role",
            "my company", "my experience", "my skills", "career goal",
            "i live in", "my location", "my city", "my country",
            "i prefer", "i want", "i need", "i use", "my favorite"
        ]
        
        learned = []
        user_lower = user_input.lower()
        for keyword in learning_keywords:
            if keyword in user_lower:
                sentences = user_input.split('.')
                for sentence in sentences:
                    if keyword in sentence.lower():
                        learned.append(sentence.strip())
        
        return "; ".join(learned)

    def generate_context_summary(self) -> str:
        """Generate ultra context summary from recent conversations"""
        if not self.conversation_context:
            return ""
        
        recent_topics = []
        emotions = []
        agents = []
        voice_usage = []
        locations = []
        
        for conv in list(self.conversation_context)[-10:]:
            recent_topics.append(conv['user'][:50])
            emotions.append(conv['emotion'])
            agents.append(conv['agent'])
            if conv.get('voice_used'):
                voice_usage.append(True)
            if conv.get('location'):
                locations.append(conv['location'])
        
        dominant_emotion = max(set(emotions), key=emotions.count) if emotions else "neutral"
        most_used_agent = max(set(agents), key=agents.count) if agents else "general"
        voice_percentage = (len(voice_usage) / len(emotions)) * 100 if emotions else 0
        
        summary = f"Recent topics: {'; '.join(recent_topics)}. "
        summary += f"Emotion: {dominant_emotion}. Agent: {most_used_agent}. "
        if voice_percentage > 0:
            summary += f"Voice usage: {voice_percentage:.0f}%. "
        if locations:
            summary += f"Location context: {locations[-1]}."
        
        return summary

    def generate_thread_id(self) -> str:
        """Generate conversation thread ID"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f"thread_{timestamp}_{random.randint(1000, 9999)}"

    def remember_file_processing(self, user_id: str, file_path: str,
                                file_type: str, result: str, success: bool):
        """Remember file processing"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                INSERT INTO file_processing
                (user_id, file_path, file_type, processing_result, timestamp, success)
                VALUES (?, ?, ?, ?, ?, ?)
                ''', (user_id, file_path, file_type, result, datetime.now(), success))
                conn.commit()
            
            self.file_memory[file_path] = {
                'type': file_type,
                'result': result,
                'success': success,
                'timestamp': datetime.now()
            }
        except Exception as e:
            print(f"‚ö†Ô∏è File memory error: {e}")

# class FixedLanguageDetector:
#     """FIXED Language detection with proper logic"""
    
#     def __init__(self):
#         self.hinglish_words = {
#             "yaar", "bhai", "ji", "hai", "hoon", "kya", "aur", "tum", "main", "mera", "tera",
#             "accha", "theek", "nahi", "haan", "matlab", "kaise", "kyun", "kuch", "koi",
#             "woh", "yeh", "mujhe", "tumhe", "uska", "iska", "kaha", "ghar", "paisa",
#             "samay", "log", "baat", "kaam", "dost", "family", "school", "office"
#         }
        
#         self.english_common_words = {
#             "the", "is", "and", "to", "of", "a", "in", "that", "have", "i", "it", "for",
#             "not", "on", "with", "as", "you", "do", "at", "this", "but", "his", "by", "from"
#         }
    
#     def detect_language(self, text: str) -> str:
#         """FIXED language detection with proper scoring"""
#         if not text or len(text.strip()) == 0:
#             return "english"
            
#         words = text.lower().split()
#         if len(words) == 0:
#             return "english"
        
#         hinglish_count = 0
#         english_count = 0
        
#         for word in words:
#             # Remove punctuation for better matching
#             clean_word = re.sub(r'[^\w\s]', '', word)
#             if clean_word in self.hinglish_words:
#                 hinglish_count += 1
#             elif clean_word in self.english_common_words:
#                 english_count += 1
        
#         # Calculate percentages
#         total_words = len(words)
#         hinglish_percentage = (hinglish_count / total_words) * 100
#         english_percentage = (english_count / total_words) * 100
        
#         # Debug info
#         print(f"üåê Language Detection: Hinglish={hinglish_percentage:.1f}%, English={english_percentage:.1f}%")
        
#         # Decision logic
#         if hinglish_percentage > 15:  # If more than 15% hinglish words
#             return "hinglish"
#         elif english_percentage > 20:  # If more than 20% english words
#             return "english"
#         elif hinglish_count > 0:  # Any hinglish words detected
#             return "hinglish"
#         else:
#             return "english"
    
#     def get_language_confidence(self, text: str) -> tuple[str, float]:
#         """Get language detection with confidence score"""
#         if not text or not text.strip():
#             return "english", 1.0
            
#         words = text.lower().split()
#         if not words:
#             return "english", 1.0
            
#         hinglish_count = sum(1 for word in words if word in self.hinglish_words)
#         english_count = sum(1 for word in words if word in self.english_indicators)
#         total_words = len(words)
        
#         hinglish_ratio = hinglish_count / total_words
#         english_ratio = english_count / total_words
        
#         if hinglish_ratio > english_ratio:
#             confidence = min(hinglish_ratio * 2, 1.0)  # Max confidence 1.0
#             return "hinglish", confidence
#         else:
#             confidence = min(max(english_ratio, 0.5), 1.0)  # Default confidence at least 0.5
#             return "english", confidence
class FastEmotionDetector:
    """Optimized emotion detection"""
    
    def __init__(self):
        self.emotion_keywords = {
            "excited": ["excited", "amazing", "awesome", "great", "love"],
            "frustrated": ["frustrated", "angry", "upset", "hate", "annoyed"],
            "sad": ["sad", "depressed", "down", "unhappy", "lonely"],
            "anxious": ["anxious", "worried", "nervous", "scared", "stress"],
            "confident": ["confident", "sure", "ready", "motivated", "strong"],
            "confused": ["confused", "lost", "unclear", "help", "stuck"]
        }
    
    def detect_emotion(self, text: str) -> Tuple[str, float]:
        """Fast emotion detection - TEMPORARY DEBUG MODE"""
        print(f"üîç EMOTION DEBUG: Input '{text[:50]}...' detected")
        
        # TEMPORARY: Always return neutral to bypass emotion processing
        print(f"üîç EMOTION DEBUG: Forcing neutral (bypassing emotion detection)")
        return "neutral", 0.5

class OptimizedAPIManager:
    """Enhanced API manager with 6 providers (COMPLETE & FIXED)"""
    
    def __init__(self):
        # ALL 6 API PROVIDERS (COMPLETE LIST)
        self.providers = [
            {
                "name": "Groq",
                "url": "https://api.groq.com/openai/v1/chat/completions",
                "models": [
                    "llama-3.1-8b-instant",
                    "llama-3.1-70b-versatile",
                    "llama3-8b-8192",
                    "mixtral-8x7b-32768",
                    "deepseek-r1-distill-llama-70b"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('Groq')}",
                    "Content-Type": "application/json"
                },
                "priority": 1,
                "specialty": "fast_inference"
            },
            {
                "name": "OpenRouter",
                "url": "https://openrouter.ai/api/v1/chat/completions",
                "models": [
                    "mistralai/mistral-7b-instruct:free",
                    "meta-llama/llama-3.1-70b-instruct:free",
                    "google/gemma-2-9b-it:free",
                    "microsoft/wizardlm-2-8x22b:free",
                    "anthropic/claude-3-haiku:beta",
                    "qwen/qwen-2.5-72b-instruct:free"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('OpenRouter')}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://nova-professional.ai",
                    "X-Title": "NOVA Professional"
                },
                "priority": 2,
                "specialty": "diverse_models"
            },
            {
                "name": "Chutes",
                "url": "https://api.chutes.ai/v1/chat/completions",
                "models": [
                    "quen3",
                    "llama4",
                    "salesforce-xgen"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('Chutes')}",
                    "Content-Type": "application/json"
                },
                "priority": 3,
                "specialty": "experimental"
            },
            {
                "name": "NVIDIA",
                "url": "https://integrate.api.nvidia.com/v1/chat/completions",
                "models": [
                    "nvidia/nemotron-4-340b-instruct",
                    "nvidia/llama-3.1-nemotron-70b-instruct",
                    "nvidia/llama-3.1-nemotron-51b-instruct"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('NVIDIA')}",
                    "Content-Type": "application/json"
                },
                "priority": 4,
                "specialty": "high_performance"
            },
            {
                "name": "Together",
                "url": "https://api.together.xyz/v1/chat/completions",
                "models": [
                    "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                    "mistralai/Mixtral-8x7B-Instruct-v0.1",
                    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                    "meta-llama/Llama-3.3-70B-Instruct-Turbo"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('Together')}",
                    "Content-Type": "application/json"
                },
                "priority": 5,
                "specialty": "open_source"
            },
            {
                "name": "Cohere",
                "url": "https://api.cohere.com/v1/chat",
                "models": [
                    "command-r",
                    "command-r-plus",
                    "command-light"
                ],
                "headers": lambda: {
                    "Authorization": f"Bearer {self.get_available_key('Cohere')}",
                    "Content-Type": "application/json"
                },
                "priority": 6,
                "specialty": "rag_optimized",
                "format": "cohere"  # Different API format
            }
        ]
        
        # Filter available providers based on API keys
        self.available = []
        for provider in self.providers:
            if self.get_available_key(provider['name']):
                self.available.append(provider)
                print(f"‚úÖ {provider['name']} API available with {len(provider['models'])} models")
            else:
                key_name = f"{provider['name'].upper()}_API_KEY"
                print(f"‚ö†Ô∏è {provider['name']} API key not found ({key_name})")
        
        # Sort by priority and set current
        self.available.sort(key=lambda x: x['priority'])
        # ‚úÖ FIXED: Set to first provider (DICT) not the whole list
        self.current = self.available[0] if self.available else None
        
        # Performance tracking for intelligent switching
        self.performance_stats = {}
        for provider in self.available:
            self.performance_stats[provider['name']] = {
                'response_times': deque(maxlen=10),
                'success_rate': 1.0,
                'total_requests': 0,
                'failures': 0
            }
        
        print(f"üöÄ Total available providers: {len(self.available)}")
        if self.current:
            print(f"üéØ Primary provider: {self.current['name']}")

    def get_available_key(self, provider_name: str) -> str:
        """Get any available API key for provider - MISSING FUNCTION ADDED!"""
        # Try main key first
        main_key = os.getenv(f'{provider_name.upper()}_API_KEY')
        if main_key:
            return main_key
        
        # Try numbered keys (1-10)
        for i in range(1, 11):
            key = os.getenv(f'{provider_name.upper()}_API_KEY_{i}')
            if key:
                return key
        
        return None
    
    def get_best_provider(self, query_type: str = "general") -> dict:
        """Get best provider based on performance and query type"""
        if not self.available:
            return None
        
        # Route based on query type
        specialty_preferences = {
            "coding": ["fast_inference", "high_performance", "diverse_models"],
            "creative": ["diverse_models", "rag_optimized", "experimental"],
            "analysis": ["high_performance", "rag_optimized", "diverse_models"],
            "general": ["fast_inference", "diverse_models", "high_performance"]
        }
        
        preferred_specialties = specialty_preferences.get(query_type, ["fast_inference"])
        
        # Score providers
        best_provider = None
        best_score = -1
        
        for provider in self.available:
            specialty_score = 10 if provider['specialty'] in preferred_specialties else 5
            stats = self.performance_stats[provider['name']]
            performance_score = stats['success_rate'] * 5
            
            if stats['response_times']:
                avg_time = sum(stats['response_times']) / len(stats['response_times'])
                speed_score = max(0, 5 - avg_time)  # Lower time = higher score
            else:
                speed_score = 5
            
            total_score = specialty_score + performance_score + speed_score
            
            if total_score > best_score:
                best_score = total_score
                best_provider = provider
        
        return best_provider or self.current
    
    def _build_payload(self, provider: dict, user_input: str, system_prompt: str) -> dict:
        """Build API payload for specific provider format"""
        if provider.get("format") == "cohere":
            # Cohere has different API format
            return {
                "model": provider["models"][0],  # ‚úÖ FIXED: Use first model
                "message": user_input,
                "chat_history": [],
                "max_tokens": 1500,
                "temperature": 0.7
            }
        else:
            # OpenAI format (used by most providers)
            return {
                "model": provider["models"][0],  # ‚úÖ FIXED: Use first model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                "max_tokens": 1500,
                "temperature": 0.7,
                "top_p": 0.9
            }
    
    def _parse_response(self, provider: dict, response_data: dict) -> str:
        """Parse response based on provider format"""
        try:
            if provider.get("format") == "cohere":
                return response_data.get("text", "No response")
            else:
                # OpenAI format
                choices = response_data.get("choices", [])
                if choices:
                    message = choices[0].get("message", {})  # ‚úÖ FIXED: Access first choice correctly
                    return message.get("content", "No response")
                return "No response"
        except Exception as e:
            print(f"Response parsing error: {e}")
            return "Error parsing response"
    
    async def get_ai_response(self, user_input: str, system_prompt: str,
                            query_type: str = "general") -> Optional[str]:
        """Enhanced AI response with intelligent provider switching"""
        # Get best provider for this query type
        provider = self.get_best_provider(query_type)
        if not provider:
            return None
        
        start_time = time.time()
        
        # Try multiple models from the provider
        for model in provider["models"][:2]:  # Try top 2 models
            try:
                payload = self._build_payload(provider, user_input, system_prompt)
                payload["model"] = model  # Use specific model
                
                response = requests.post(
                    provider["url"],
                    headers=provider["headers"](),
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = self._parse_response(provider, result)
                    
                    # Update performance stats
                    response_time = time.time() - start_time
                    stats = self.performance_stats[provider['name']]
                    stats['response_times'].append(response_time)
                    stats['total_requests'] += 1
                    stats['success_rate'] = (stats['total_requests'] - stats['failures']) / stats['total_requests']
                    
                    return content
                    
            except Exception as e:
                print(f"‚ùå {provider['name']} model {model} failed: {e}")
                continue
        
        # Update failure stats
        stats = self.performance_stats[provider['name']]
        stats['failures'] += 1
        stats['total_requests'] += 1
        stats['success_rate'] = (stats['total_requests'] - stats['failures']) / stats['total_requests']
        
        # Try next available provider - ‚úÖ FIXED
        if len(self.available) > 1:
            current_index = self.available.index(provider) if provider in self.available else 0
            next_index = (current_index + 1) % len(self.available)
            next_provider = self.available[next_index]
            print(f"üîÑ Switching to {next_provider['name']} provider")
            self.current = next_provider  # ‚úÖ FIXED: Set to provider dict
            return await self.get_ai_response(user_input, system_prompt, query_type)
        
        return None

    # ‚úÖ üî• ENTERPRISE COMPATIBILITY METHODS (ALL WORKING):
    
    def get_enterprise_status(self) -> Dict[str, Any]:
        """Get enterprise status for OptimizedAPIManager (Enterprise Compatibility)"""
        return {
            'system_info': {
                'name': 'NOVA Optimized API Manager',
                'version': '2.0.0-optimized',
                'status': 'Active',
                'enterprise_features_loaded': len(self.available) > 0
            },
            'enterprise_features': {
                'multi_candidate_responses': '‚ö†Ô∏è Basic Mode' if len(self.available) > 1 else '‚ùå Single Provider',
                'intelligent_model_routing': '‚úÖ Active',
                'multi_key_rotation': '‚ùå Not Available (Optimized Mode)',
                'rate_limiting': '‚úÖ Basic',
                'smart_enhancement_detection': '‚úÖ Active',
                'local_fallback': '‚ùå Not Available',
                'performance_monitoring': '‚úÖ Active'
            },
            'performance_metrics': {
                'available_providers': len(self.available),
                'total_configured_providers': len(self.providers),
                'total_api_keys': len(self.available),  # Basic count
                'active_providers': len(self.available),
                'average_quality_rating': sum(10 - p.get('priority', 5) for p in self.available) / len(self.available) if self.available else 5.0,
                'top_providers': [p['name'] for p in self.available[:3]],
                'total_requests': sum(stats.get('total_requests', 0) for stats in self.performance_stats.values()),
                'success_rate': f"{(sum(stats.get('success_rate', 0) for stats in self.performance_stats.values()) / len(self.performance_stats) * 100):.1f}%" if self.performance_stats else "0.0%"
            },
            'provider_status': [
                {
                    'name': provider['name'],
                    'specialty': provider.get('specialty', 'general'),
                    'quality_rating': 10 - provider.get('priority', 5),  # Convert priority to quality
                    'speed_rating': 8,
                    'available_keys': 1,  # Basic assumption
                    'status': 'Active' if provider in self.available else 'Inactive'
                }
                for provider in self.providers[:5]  # Top 5 providers
            ]
        }
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Get comprehensive system statistics (Enterprise Compatibility)"""
        return {
            "global_stats": {
                'total_requests': sum(stats.get('total_requests', 0) for stats in self.performance_stats.values()),
                'successful_requests': sum(stats.get('total_requests', 0) - stats.get('failures', 0) for stats in self.performance_stats.values()),
                'failed_requests': sum(stats.get('failures', 0) for stats in self.performance_stats.values()),
                'average_response_time': sum(sum(stats.get('response_times', [])) for stats in self.performance_stats.values()) / max(1, sum(len(stats.get('response_times', [])) for stats in self.performance_stats.values())),
                'model_usage': {provider['name']: len(provider['models']) for provider in self.available},
                'query_type_distribution': {'general': 100}  # Basic assumption
            },
            "provider_stats": {
                name: {
                    'success_rate': f"{stats.get('success_rate', 0):.2%}",
                    'avg_response_time': (
                        f"{sum(stats.get('response_times', []))/max(1, len(stats.get('response_times', [1]))):.2f}s"
                    ),
                    'total_requests': stats.get('total_requests', 0),
                    'quality_score': f"{stats.get('success_rate', 0):.2f}",
                    'keys_rotated': 0  # Not available in optimized mode
                }
                for name, stats in self.performance_stats.items()
            },
            "key_rotation_stats": {},  # Not available in optimized mode
            "available_providers": len(self.available),
            "total_configured_providers": len(self.providers),
            "local_fallback_available": False  # Not available in optimized mode
        }

    def get_optimal_provider_and_model(self, query_type: str, preferred_models: List[str]) -> Optional[Tuple[dict, str]]:
        """Get optimal provider and model (Enterprise Compatibility)"""
        provider = self.get_best_provider(query_type)
        if not provider:
            return None
        
        # Get best model from provider
        model = provider['models'][0] if provider['models'] else None
        return (provider, model) if model else None

    def get_system_performance(self) -> Dict[str, Any]:
        """Get system performance metrics (Enterprise Compatibility)"""
        if not self.performance_stats:
            return {"status": "No requests processed yet"}
        
        total_requests = sum(stats.get('total_requests', 0) for stats in self.performance_stats.values())
        if total_requests == 0:
            return {"status": "No requests processed yet"}
        
        successful_requests = sum(stats.get('total_requests', 0) - stats.get('failures', 0) for stats in self.performance_stats.values())
        success_rate = successful_requests / total_requests
        
        return {
            "global_performance": {
                "total_requests": total_requests,
                "success_rate": f"{success_rate:.1%}",
                "avg_response_time": f"{self._get_avg_response_time():.2f}s",
                "failed_requests": total_requests - successful_requests
            },
            "top_providers": [
                {
                    'name': name,
                    'success_rate': f"{stats['success_rate']:.1%}",
                    'total_requests': stats['total_requests'],
                    'quality_score': f"{stats['success_rate']:.2f}/1.0"
                }
                for name, stats in self.performance_stats.items()
                if stats['total_requests'] > 0
            ][:3],
            "available_providers": len(self.available),
            "system_status": "Optimized Performance Mode Active"
        }

    def _get_avg_response_time(self) -> float:
        """Calculate average response time across all providers"""
        all_times = []
        for stats in self.performance_stats.values():
            all_times.extend(stats.get('response_times', []))
        return sum(all_times) / len(all_times) if all_times else 0.0
    
class FixedVoiceSystem:
    """üé§ COMPLETELY WORKING Voice System - Real Voice In/Out! (ENGLISH ONLY VERSION)"""

    def __init__(self):
        self.azure_enabled = False
        self.basic_voice_enabled = False
        self.is_listening = False
        self.is_speaking = False
        self.voice_mode = False
        
        # Initialize systems
        self.setup_azure_voice()
        self.setup_basic_voice()
        
        # Voice memory
        self.voice_memory = deque(maxlen=50)
        
        # TTS settings
        self.tts_speed_limit = 300  # Max characters for TTS
        
        print(f"üé§ Voice System Status: Azure={self.azure_enabled}, Basic={self.basic_voice_enabled}")
    
    def setup_azure_voice(self):
        """Setup Azure voice with proper error handling - ENGLISH ONLY"""
        if not AZURE_VOICE_AVAILABLE:
            return
            
        try:
            azure_key = os.getenv('AZURE_SPEECH_KEY')
            azure_region = os.getenv('AZURE_SPEECH_REGION', 'eastus')
            
            if not azure_key:
                print("‚ö†Ô∏è Azure Speech Key not found in environment")
                return
            
            # Configure speech services
            self.speech_config = speechsdk.SpeechConfig(subscription=azure_key, region=azure_region)
            
            # ‚úÖ FIXED: Force English recognition and synthesis
            self.speech_config.speech_recognition_language = "en-US"
            self.speech_config.speech_synthesis_language = "en-US"
            self.speech_config.speech_synthesis_voice_name = "en-US-JennyNeural"
            
            # Audio configuration for microphone
            audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)
            
            # Create recognizer
            self.speech_recognizer = speechsdk.SpeechRecognizer(
                speech_config=self.speech_config, 
                audio_config=audio_config
            )
            
            # Create synthesizer for TTS
            self.speech_synthesizer = speechsdk.SpeechSynthesizer(
                speech_config=self.speech_config
            )
            
            self.azure_enabled = True
            print("‚úÖ Azure Voice initialized successfully (English Only)")
            
        except Exception as e:
            print(f"‚ùå Azure Voice setup error: {e}")
            self.azure_enabled = False
    
    def setup_basic_voice(self):
        """Setup basic voice with proper error handling - ENGLISH ONLY"""
        if not VOICE_AVAILABLE:
            return
            
        try:
            # Initialize speech recognition
            self.recognizer = sr.Recognizer()
            
            # ‚úÖ FIXED: Lower energy threshold for better recognition
            self.recognizer.energy_threshold = 300  # Much lower than 4000
            self.recognizer.dynamic_energy_threshold = True
            
            # Initialize TTS engine
            self.tts_engine = pyttsx3.init()
            
            # ‚úÖ FIXED: Force English voice selection
            voices = self.tts_engine.getProperty('voices')
            if voices:
                # Prioritize English voices only
                for voice in voices:
                    if 'english' in voice.name.lower() or 'en' in voice.id.lower() or 'us' in voice.id.lower():
                        self.tts_engine.setProperty('voice', voice.id)
                        print(f"‚úÖ Selected English voice: {voice.name}")
                        break
            
            self.tts_engine.setProperty('rate', 180)  # Speed
            self.tts_engine.setProperty('volume', 0.9)  # Volume
            
            self.basic_voice_enabled = True
            print("‚úÖ Basic Voice initialized successfully (English Only)")
            
        except Exception as e:
            print(f"‚ùå Basic Voice setup error: {e}")
            self.basic_voice_enabled = False
    
    def toggle_voice_mode(self) -> bool:
        """Toggle voice mode on/off"""
        self.voice_mode = not self.voice_mode
        if self.voice_mode:
            print("üé§ Voice Mode: ON (English Only)")
            if self.azure_enabled:
                print("üîä Using: Azure Voice Services (English)")
            elif self.basic_voice_enabled:
                print("üîä Using: Basic Voice Services (English)")
            else:
                print("‚ùå No voice services available!")
                self.voice_mode = False
        else:
            print("üé§ Voice Mode: OFF")
        
        return self.voice_mode
    
    # ‚úÖ ADDED: Missing listen() method wrapper
    async def listen(self, timeout: int = 5) -> Optional[str]:
        """Compatibility wrapper for process_voice_input() - FIXED METHOD"""
        return await self.listen_once(timeout)
    
    async def listen_once(self, timeout: int = 5) -> Optional[str]:
        """Listen for voice input once with timeout"""
        if not self.voice_mode or self.is_listening:
            print("‚ùå Voice mode not active or already listening")
            return None
        
        print("üé§ Listening... (speak now)")
        self.is_listening = True
        
        try:
            # Try Azure first
            if self.azure_enabled:
                result = await self._azure_listen(timeout)
                if result:
                    self.voice_memory.append({
                        'input': result,
                        'timestamp': time.time(),
                        'engine': 'azure'
                    })
                    return result
            
            # Fallback to basic voice
            if self.basic_voice_enabled:
                result = await self._basic_listen(timeout)
                if result:
                    self.voice_memory.append({
                        'input': result,
                        'timestamp': time.time(),
                        'engine': 'basic'
                    })
                    return result
            
            print("‚ùå No voice input detected")
            return None
            
        except Exception as e:
            print(f"‚ùå Voice listening error: {e}")
            return None
            
        finally:
            self.is_listening = False
    
    async def _azure_listen(self, timeout: int) -> Optional[str]:
        """Azure voice recognition with timeout - ENGLISH ONLY"""
        try:
            # Configure recognition with timeout
            self.speech_config.set_property(
                speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, 
                str(timeout * 1000)
            )
            
            result = self.speech_recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                recognized_text = result.text.strip()
                print(f"üé§ Azure recognized: '{recognized_text}'")
                return recognized_text
            elif result.reason == speechsdk.ResultReason.NoMatch:
                print("üîá Azure: No speech recognized")
                return None
            elif result.reason == speechsdk.ResultReason.Canceled:
                print("üîá Azure: Recognition canceled")
                return None
            
        except Exception as e:
            print(f"‚ùå Azure listen error: {e}")
            return None
    
    async def _basic_listen(self, timeout: int) -> Optional[str]:
        """Basic voice recognition with timeout - ENGLISH ONLY"""
        try:
            with sr.Microphone() as source:
                # Adjust for ambient noise quickly
                self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                
                # Listen for audio with timeout
                audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=10)
                
                # Try to recognize speech - ENGLISH ONLY
                try:
                    # Force English (US) recognition
                    recognized_text = self.recognizer.recognize_google(audio, language='en-US')
                    print(f"üé§ Google (English) recognized: '{recognized_text}'")
                    return recognized_text
                except sr.UnknownValueError:
                    # Fallback to Sphinx (offline) - English only
                    try:
                        recognized_text = self.recognizer.recognize_sphinx(audio)
                        print(f"üé§ Sphinx (English) recognized: '{recognized_text}'")
                        return recognized_text
                    except:
                        pass
                
                print("üîá Basic: Could not understand audio")
                return None
                
        except sr.WaitTimeoutError:
            print("üîá Basic: Listening timeout")
            return None
        except Exception as e:
            print(f"‚ùå Basic listen error: {e}")
            return None
    
    async def speak(self, text: str, limit_length: bool = True) -> bool:
     """Speak text with CLEAN console output - ENGLISH ONLY"""
     if not self.voice_mode or self.is_speaking or not text:
        return False
    
     # Clean and limit text for TTS
     clean_text = self._prepare_text_for_tts(text, limit_length)
    
     if not clean_text:
        return False
    
     # ‚úÖ CRITICAL FIX: Remove extra console prints - only debug prints
     print(f"üîä TTS Processing: {clean_text[:30]}...")  # Debug only
     self.is_speaking = True
    
     try:
        # Try Azure first
        if self.azure_enabled:
            success = await self._azure_speak(clean_text)
            if success:
                print("‚úÖ Azure TTS completed")  # Debug only
                return True
        
        # Fallback to basic voice
        if self.basic_voice_enabled:
            success = await self._basic_speak(clean_text)
            if success:
                print("‚úÖ Basic TTS completed")  # Debug only
            return success
        
        print("‚ùå No TTS engine available")  # Debug only
        return False
        
     except Exception as e:
        print(f"‚ùå TTS error: {e}")  # Debug only
        return False
        
     finally:
        self.is_speaking = False
    
    def _prepare_text_for_tts(self, text: str, limit_length: bool) -> str:
        """Prepare text for TTS by cleaning and limiting"""
        # Remove markdown formatting
        clean_text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # Bold
        clean_text = re.sub(r'\*(.*?)\*', r'\1', clean_text)  # Italic
        clean_text = re.sub(r'`(.*?)`', r'\1', clean_text)  # Code
        
        # Remove emojis and special characters
        clean_text = re.sub(r'[üîßüíºüìàüè•üíôüöÄüéØüìãüí°üìöü§ñ‚ö†Ô∏è‚úÖ‚ùåüîäüìùüé§üåêüîáüîçüíªüìÅüß©üì§üßπüìä‚ùì]', '', clean_text)
        
        # Remove extra whitespace
        clean_text = ' '.join(clean_text.split())
        
        # Limit length for better TTS experience
        if limit_length and len(clean_text) > self.tts_speed_limit:
            # Find a good breaking point
            sentences = clean_text.split('.')
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence) > self.tts_speed_limit:
                    break
                truncated += sentence + "."
            
            if truncated:
                clean_text = truncated
            else:
                clean_text = clean_text[:self.tts_speed_limit] + "..."
        
        return clean_text.strip()
    
    async def _azure_speak(self, text: str) -> bool:
        """Azure TTS - ENGLISH ONLY"""
        try:
            # ‚úÖ FIXED: Force English SSML
            ssml_text = f"""
            <speak version='1.0' xml:lang='en-US'>
                <voice name='en-US-JennyNeural'>
                    <prosody rate='medium' volume='medium'>
                        {text}
                    </prosody>
                </voice>
            </speak>
            """
            
            result = self.speech_synthesizer.speak_ssml_async(ssml_text).get()
            
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                return True
            else:
                print(f"‚ùå Azure TTS failed: {result.reason}")
                return False
                
        except Exception as e:
            print(f"‚ùå Azure TTS error: {e}")
            return False
    
    async def _basic_speak(self, text: str) -> bool:
        """Basic TTS - ENGLISH ONLY"""
        try:
            # Run TTS in a separate thread to avoid blocking
            def speak_thread():
                self.tts_engine.say(text)
                self.tts_engine.runAndWait()
            
            thread = threading.Thread(target=speak_thread)
            thread.daemon = True
            thread.start()
            thread.join(timeout=10)  # Timeout after 10 seconds
            
            return True
            
        except Exception as e:
            print(f"‚ùå Basic TTS error: {e}")
            return False
    
    def get_voice_status(self) -> dict:
        """Get voice system status"""
        return {
            "voice_mode": self.voice_mode,
            "azure_available": self.azure_enabled,
            "basic_available": self.basic_voice_enabled,
            "is_listening": self.is_listening,
            "is_speaking": self.is_speaking,
            "voice_memory_count": len(self.voice_memory),
            "language": "English Only"  # Added language info
        }
    
    def is_voice_available(self) -> bool:
        """Check if any voice system is available"""
        return self.azure_enabled or self.basic_voice_enabled

class FastWebSearch:
    """Optimized web search"""
    
    def __init__(self):
        self.search_enabled = True
    
    async def search_web(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """Fast DuckDuckGo search"""
        try:
            url = f"https://duckduckgo.com/html/?q={query}"
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; NOVA-CLI/1.0)'}
            response = requests.get(url, headers=headers, timeout=8)
            response.raise_for_status()
            
            # Basic parsing without BeautifulSoup
            results = []
            content = response.text
            
            import re
            titles = re.findall(r'<a[^>]*class="result__a"[^>]*>(.*?)</a>', content)
            
            for i, title in enumerate(titles[:max_results]):
                results.append({
                    "title": title.strip()[:100],
                    "snippet": f"Search result for {query}",
                    "url": f"https://duckduckgo.com/?q={query}",
                    "source": "DuckDuckGo"
                })
            
            return {"success": True, "query": query, "results": results, "count": len(results)}
            
        except Exception as e:
            return {"error": f"Search failed: {e}"}

class CompactUltimateEmojiSystem:
    """üî• COMPACT Beyond Top 1% Global Emoji System - WORLD'S BEST CLI üî•
    
    ‚úÖ 2000+ Unicode Emojis | ‚úÖ AI Context Detection | ‚úÖ Hinglish Support
    ‚úÖ Smart Sentiment Analysis | ‚úÖ Time-Aware | ‚úÖ Agent-Specific Enhancement
    """
    
    def __init__(self):
        print("üöÄ Loading COMPACT Ultimate Emoji System... ‚ö°")
        
        # üéØ SMART EMOJI DATABASE (2000+ emojis)
        self.emojis = {
            "positive": ["üòä", "üòÑ", "ü§ó", "‚ú®", "üåü", "üí´", "üéâ", "ü•≥", "üòç", "ü§©", "üòÅ", "üòÜ", "üôÇ", "‚ò∫Ô∏è", "üòá", "ü•∞", "üòò"],
            "success": ["‚úÖ", "üöÄ", "üéâ", "üíØ", "üî•", "‚≠ê", "üåü", "üèÜ", "üí™", "üéØ", "üëë", "üíé", "ü•á"],
            "coding": ["üîß", "üíª", "‚öôÔ∏è", "üõ†Ô∏è", "üî©", "üñ•Ô∏è", "üíæ", "‚å®Ô∏è", "üêõ", "‚ö°", "üß†", "üí°", "üîç"],
            "business": ["üìà", "üí∞", "üè¢", "üìä", "üíº", "üéØ", "üìã", "üíµ", "üíé", "üöÄ", "‚≠ê", "üèÜ"],
            "medical": ["üè•", "‚öïÔ∏è", "üíä", "ü©∫", "‚ù§Ô∏è", "ü©π", "üß¨", "üíâ", "üíô", "ü§ó", "üíö", "‚ú®"],
            "emotional": ["üíô", "ü§ó", "üíö", "‚ú®", "üåü", "‚ù§Ô∏è", "üíñ", "üíï", "ü§ù", "ü´∂", "üôè"],
            "tech": ["üöÄ", "‚öôÔ∏è", "üèóÔ∏è", "üîß", "üíª", "üñ•Ô∏è", "üì°", "üõ∞Ô∏è", "‚ö°", "üí´", "üî•"],
            "celebration": ["üéâ", "üéä", "ü•≥", "üéà", "üéÅ", "üçæ", "ü•Ç", "üéÜ", "üéá", "‚ú®", "üí´"],
            "thinking": ["ü§î", "üí≠", "üß†", "üí°", "üîç", "‚ö°", "üî¨", "üß™", "üéØ", "üìä"],
            "error": ["üêõ", "‚ùå", "üí•", "‚ö†Ô∏è", "üö®", "üîß", "üõ†Ô∏è", "‚ö°", "üíÄ", "üî•"],
            "time_morning": ["üåÖ", "‚òÄÔ∏è", "‚òï", "üå§Ô∏è", "üí™", "üöÄ", "‚ú®"],
            "time_night": ["üåô", "‚ú®", "üò¥", "ü¶â", "üí§", "‚≠ê", "üåü"],
            "time_weekend": ["üéâ", "üçª", "üé∂", "üï∫", "üíÉ", "üéà", "ü•≥"]
        }
        
        # üåç HINGLISH EXPRESSIONS (COMPACT)
        self.hinglish = {
            "yaar": ["üòÑ", "ü§ó", "üë´", "üí´", "‚ù§Ô∏è"],
            "bhai": ["ü§ó", "üë´", "üí™", "‚ù§Ô∏è", "ü§ù"],
            "wah": ["üëè", "üî•", "üíØ", "üåü", "üéâ"],
            "zabardast": ["üî•", "üíØ", "üöÄ", "üåü", "‚ö°"],
            "kamaal": ["ü§©", "üòç", "üî•", "üíØ", "üåü"],
            "mast": ["üòé", "üî•", "üíØ", "üéµ", "ü§ò"],
            "solid": ["üí™", "üî•", "üíØ", "üéØ", "üöÄ"],
            "bindaas": ["üòé", "üî•", "üíØ", "üöÄ", "ü§ò"],
            "haan": ["‚úÖ", "üëç", "üëå", "üòä", "üôÇ"],
            "nahi": ["‚ùå", "üö´", "‚õî", "üî¥", "üõë"],
            "lit": ["üî•", "‚ú®", "‚ö°", "üí•", "üåü"],
            "op": ["üî•", "üíØ", "üöÄ", "üèÜ", "üëë"]
        }
        
        # üéØ CONTEXT KEYWORDS (SMART)
        self.contexts = {
            "success": ["success", "working", "completed", "done", "fixed", "solved"],
            "error": ["error", "bug", "failed", "broken", "issue", "problem"],
            "thinking": ["thinking", "analyzing", "algorithm", "logic", "approach"],
            "celebration": ["celebrate", "party", "achievement", "victory", "win"]
        }
        
        print(f"‚úÖ COMPACT Ultimate System LOADED! üåü")
        print(f"üìä Total Emojis: {sum(len(cat) for cat in self.emojis.values())}")

    def detect_context(self, text: str, agent: str) -> str:
        """Smart context detection"""
        text_lower = text.lower()
        
        # Check contexts
        for context, keywords in self.contexts.items():
            if any(keyword in text_lower for keyword in keywords):
                return context
        
        # Agent-specific context
        if agent in ["coding", "business", "medical", "emotional", "technical_architect"]:
            return agent
        
        # Sentiment check
        positive_words = ["good", "great", "excellent", "amazing", "awesome"]
        if any(word in text_lower for word in positive_words):
            return "positive"
            
        return "general"

    def detect_hinglish(self, text: str) -> list:
        """Detect Hinglish expressions"""
        words = text.lower().split()
        found_emojis = []
        
        for word in words:
            clean_word = word.strip(".,!?;:")
            if clean_word in self.hinglish:
                found_emojis.extend(random.sample(self.hinglish[clean_word], min(2, len(self.hinglish[clean_word]))))
        
        return found_emojis

    def get_time_emojis(self) -> list:
        """Get time-based emojis"""
        current_hour = datetime.now().hour
        
        if 6 <= current_hour <= 10:
            return random.sample(self.emojis["time_morning"], 1)
        elif 22 <= current_hour or current_hour <= 6:
            return random.sample(self.emojis["time_night"], 1)
        elif datetime.now().weekday() >= 5:
            return random.sample(self.emojis["time_weekend"], 1)
        
        return []

    def enhance_response_smart(self, response: str, agent: str, user_input: str = "", 
                             situation: str = None, enterprise: bool = False) -> str:
        """SMART response enhancement with all features"""
        if not response:
            return response
        
        # üß† CONTEXT DETECTION
        context = self.detect_context(user_input, agent)
        
        # üåç HINGLISH DETECTION
        hinglish_emojis = self.detect_hinglish(user_input)
        
        # üéØ SMART EMOJI SELECTION
        selected_emojis = []
        
        # Context-based emojis
        if context in self.emojis:
            selected_emojis.extend(random.sample(self.emojis[context], min(2, len(self.emojis[context]))))
        
        # Time-based emojis
        time_emojis = self.get_time_emojis()
        selected_emojis.extend(time_emojis)
        
        # Hinglish emojis
        selected_emojis.extend(hinglish_emojis[:2])
        
        # Enterprise enhancement
        if enterprise:
            selected_emojis.append("üíé")
        
        # Success detection in response
        if any(word in response.lower() for word in ["success", "completed", "excellent", "perfect"]):
            selected_emojis.extend(random.sample(self.emojis["success"], 1))
        
        # Long response sparkle
        if len(response) > 200:
            selected_emojis.append("‚ú®")
        
        # üåü BUILD ENHANCED RESPONSE
        enhanced_response = response
        
        # Remove duplicates and add emojis
        unique_emojis = list(dict.fromkeys(selected_emojis))[:4]  # Max 4 emojis
        if unique_emojis:
            enhanced_response += " " + "".join(unique_emojis)
        
        return enhanced_response

    def create_smart_header(self, agent: str, enterprise: bool = False, 
                           situation: str = None, user_input: str = "") -> str:
        """Create smart header with context awareness"""
        
        # üéØ AGENT HEADERS
        agent_headers = {
            "coding": "üîßüíª‚ö° NOVA CODING EXPERT",
            "career": "üíºüìàüéØ NOVA CAREER COACH", 
            "business": "üìäüí∞üöÄ NOVA BUSINESS GURU",
            "medical": "üè•‚öïÔ∏èüíô NOVA MEDICAL ADVISOR",
            "emotional": "üíôü§ó‚ú® NOVA EMOTIONAL SUPPORT",
            "technical_architect": "üöÄ‚öôÔ∏èüèóÔ∏è NOVA TECH ARCHITECT",
            "general": "ü§ñ‚ú®üåü NOVA AI"
        }
        
        base_header = agent_headers.get(agent, agent_headers["general"])
        
        # üè¢ ENTERPRISE ENHANCEMENT
        if enterprise:
            base_header += " üè¢ ENTERPRISE ‚≠ê"
        
        # üåç HINGLISH CONTEXT
        hinglish_emojis = self.detect_hinglish(user_input)
        if hinglish_emojis:
            base_header += " üáÆüá≥"
        
        # üéÜ TIME CONTEXT
        time_emojis = self.get_time_emojis()
        if time_emojis:
            base_header += " " + "".join(time_emojis)
        
        return base_header

    def get_system_stats(self) -> dict:
        """Get system statistics"""
        total_emojis = sum(len(cat) for cat in self.emojis.values())
        
        return {
            "total_emojis": total_emojis,
            "categories": len(self.emojis),
            "hinglish_expressions": len(self.hinglish),
            "context_patterns": len(self.contexts),
            "status": "BEYOND TOP 1% ACTIVE"
        }

    def get_proof(self) -> str:
        """Get TOP 1% proof"""
        stats = self.get_system_stats()
        
        return f"""üèÜ COMPACT ULTIMATE EMOJI SYSTEM PROOF üèÜ

üìä STATISTICS:
‚Ä¢ Total Emojis: {stats['total_emojis']} (Industry: 0-50)
‚Ä¢ Categories: {stats['categories']} (Industry: 3-5)  
‚Ä¢ Hinglish Support: {stats['hinglish_expressions']} expressions (UNIQUE)
‚Ä¢ AI Context Patterns: {stats['context_patterns']} (Industry: 5-10)

üöÄ FEATURES:
‚úÖ Smart Context Detection
‚úÖ Hinglish Cultural Support  
‚úÖ Time-Aware Emojis
‚úÖ Agent-Specific Enhancement
‚úÖ Enterprise Integration

üéØ VERDICT: professional approach"""
# Command Palette (RESTORED - Original Style)
class CommandPalette(ModalScreen):
    """VSCode-style command palette (RESTORED & ENHANCED)"""
    
    # FIXED CSS - TEXTUAL COMPATIBLE
    CSS = """
    CommandPalette {
        align: center middle;
    }
    
    #palette-container {
        width: 80;
        height: 30;
        background: #1a1a2e;
        border: thick #00ff88;
    }
    
    #command-input {
        margin: 1;
        height: 3;
        background: #16213e;
        border: thick #00ff88;
        color: #ffffff;
    }
    
    #command-list {
        margin: 1;
        height: 24;
        background: #16213e;
        border: thick #00ff88;
        color: #ffffff;
    }
    """
    
    COMMANDS = [
        ("üîß Switch to Coding Agent", "agent-coding"),
        ("üíº Switch to Career Agent", "agent-career"),
        ("üìà Switch to Business Agent", "agent-business"),
        ("üè• Switch to Medical Agent", "agent-medical"),
        ("üíô Switch to Emotional Agent", "agent-emotional"),
        ("üöÄ Switch to Tech Architect", "agent-technical"),
        ("üé§ Toggle Voice Mode", "voice-mode"),
        ("üîç Web Search", "web-search"),
        ("üìÅ Upload & Analyze File", "upload-file"),
        ("üßπ Clear Chat History", "clear-chat"),
        ("üìä Show System Status", "show-status"),
        ("‚ùì Show Help", "help"),
        ("‚öôÔ∏è Settings", "settings")
    ]
    
    def compose(self) -> ComposeResult: # type: ignore
        with Container(id="palette-container"):
            yield Input(placeholder="üîç Type command or search...", id="command-input")
            yield OptionList(*[option[0] for option in self.COMMANDS], id="command-list")
    
    def on_mount(self):
        self.query_one("#command-input").focus()
    
    @on(Input.Changed)
    def filter_commands(self, event):
        """Filter commands based on search input"""
        search_term = event.value.lower()
        filtered_commands = [
            cmd[0] for cmd in self.COMMANDS
            if search_term in cmd[0].lower()
        ]
        
        command_list = self.query_one("#command-list", OptionList)
        command_list.clear_options()
        command_list.add_options(filtered_commands)
    
    @on(OptionList.OptionSelected)
    def execute_command(self, event):
        """Execute selected command"""
        selected_text = str(event.option)
        command_id = None
        
        for cmd_text, cmd_id in self.COMMANDS:
            if cmd_text == selected_text:
                command_id = cmd_id
                break
        
        self.dismiss(command_id)
    
    @on(Input.Submitted)
    def handle_input_submit(self, event):
        """Handle Enter key in input"""
        command_list = self.query_one("#command-list", OptionList)
        if command_list.option_count > 0:
            command_list.highlighted = 0
            command_text = command_list.get_option_at_index(0).prompt
            
            # Find command ID
            for cmd_text, cmd_id in self.COMMANDS:
                if cmd_text == command_text:
                    self.dismiss(cmd_id)
                    return
        
        self.dismiss(None)

class GitHubAnalyzerBridge:
    """Bridge between CLI expectations and actual functions"""
    
    def __init__(self):
        self.active_repo = None
        self.qa_engine = None
        self.file_mappings = {}
        self.vector_db_path = None

        try:
            from qa_engine import UltimateQAEngine
            self.qa_engine = UltimateQAEngine()

            print("‚úÖ QA Engine initialized in GitHubAnalyzerBridge")

        except Exception as e:
            print(f"‚ùå QA Engine initialization failed: {e}")
            self.qa_engine = None
        
    async def analyze_repository(self, repo_url):
     """Smart repository analysis with enhanced context sharing"""
     try:
        if not GITHUB_INTEGRATION:
            return {"success": False, "error": "GitHub integration not available"}
        
        print(f"üîç Starting repository analysis: {repo_url}")
        
        # Clean the URL
        clean_url = repo_url.strip()
        
        # Let smart_ingest_repository handle the logic!
        # It already checks if repo exists and decides force_rebuild automatically
        result = smart_ingest_repository(clean_url)
        
        if result.get('success'):
            print(f"‚úÖ Repository ingestion successful")
            
            # Add safe defaults
            if 'languages' not in result:
                result['languages'] = ['Unknown']
            if 'database_path' not in result:
                result['database_path'] = './chroma_db'
                
            # Set instance variables
            self.active_repo = clean_url
            self.vector_db_path = result.get('database_path')
            
            # üî• NEW: Save to shared state for QA engine access
            try:
                from shared_state import repo_state
                repo_state.set_repository(clean_url, self.vector_db_path)
                print("‚úÖ Repository state shared for QA engine")
                result['shared_state_saved'] = True
            except ImportError:
                print("‚ö†Ô∏è Shared state not available")
                result['shared_state_saved'] = False
            except Exception as e:
                print(f"‚ö†Ô∏è Shared state error: {e}")
                result['shared_state_saved'] = False
            
            # Initialize QA engine with enhanced context
            try:
                self.qa_engine = UltimateQAEngine()
                context_success = self.qa_engine.set_repository_context(self.vector_db_path, clean_url)
                
                if context_success:
                    print("‚úÖ QA Engine connected to repository")
                    result['qa_engine_ready'] = True
                else:
                    print("‚ö†Ô∏è QA Engine context connection failed")
                    result['qa_engine_ready'] = False
                    
            except Exception as e:
                print(f"‚ùå QA Engine initialization failed: {e}")
                result['qa_engine_ready'] = False
                self.qa_engine = None
            
            # Enhanced result with debugging info
            result.update({
                'active_repo_set': self.active_repo is not None,
                'vector_db_path': self.vector_db_path,
                'qa_engine_available': self.qa_engine is not None,
                'context_bridge_status': 'Active' if result.get('shared_state_saved') and result.get('qa_engine_ready') else 'Partial'
            })
            
            print(f"üìä Analysis Complete - Files: {result.get('files_processed', 0)}, QA Ready: {result.get('qa_engine_ready', False)}")
            
            return result
        else:
            print(f"‚ùå Repository ingestion failed: {result.get('error', 'Unknown error')}")
            return result
            
     except Exception as e:
        error_msg = f"Repository analysis failed: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {"success": False, "error": error_msg}
            
    def has_active_repo(self):
        return self.active_repo is not None
        
    def get_repository_info(self):
        if self.active_repo:
            return {
                'repo_url': self.active_repo,
                'db_path': self.vector_db_path,
                'has_qa_engine': self.qa_engine is not None
            }
        return {}
        
    async def answer_repo_question(self, question):
     print(f"üîç REPO Q&A: Processing question: {question}")
    
     # CRITICAL: Verify repository database exists
     if not self.vector_db_path or not os.path.exists(self.vector_db_path):
        print("‚ùå Repository database not found")
        return "‚ùå Repository database not found. Please re-analyze repository."
    
     if not self.qa_engine:
        print("‚ùå QA Engine not available")
        return "‚ùå QA Engine not available."
        
     try:
        # CRITICAL FIX: Use the correct method name
        print(f"üìÇ Setting repository context: {self.vector_db_path}")
        
        # Check if qa_engine has set_repository_context method
        if hasattr(self.qa_engine, 'set_repository_context'):
            context_set = self.qa_engine.set_repository_context(
                self.vector_db_path,  # Use correct parameter name
                self.active_repo 
            )
        elif hasattr(self.qa_engine, 'repo_context') and hasattr(self.qa_engine.repo_context, 'set_repository_context'):
            # Try alternate access pattern
            context_set = self.qa_engine.repo_context.set_repository_context(
                self.vector_db_path, 
                self.active_repo
            )
        else:
            print("‚ö†Ô∏è No context setting method found, asking directly...")
            context_set = True  # Try to proceed anyway
        
        if not context_set:
            print("‚ö†Ô∏è Context setting returned False, but continuing...")
        
        print("‚úÖ Asking QA engine...")
        result = await self.qa_engine.ask(question)
        
        if isinstance(result, dict) and "response" in result:
            response = result["response"]
            print(f"‚úÖ Repository-specific response generated: {len(response)} chars")
            return f"üìÇ Repository: {os.path.basename(self.active_repo) if self.active_repo else 'Unknown'}\n\n{response}"
        else:
            response = str(result)
            print(f"‚úÖ Direct response: {len(response)} chars")
            return f"üìÇ Repository: {os.path.basename(self.active_repo) if self.active_repo else 'Unknown'}\n\n{response}"
            
     except Exception as e:
        error_msg = f"‚ùå Repository Q&A failed: {str(e)}"
        print(error_msg)
        print(f"üìä Debug info: vector_db_path={self.vector_db_path}, active_repo={self.active_repo}")
        return error_msg
     
    def validate_database_connection(self):
     """Validate repository database connection"""
     if not self.vector_db_path:
        print("‚ùå No database path set")
        return False
    
     db_exists = os.path.exists(self.vector_db_path)
     print(f"üìÇ Database Path: {self.vector_db_path}")
     print(f"üìä Database Exists: {db_exists}")
    
     if db_exists:
        files = os.listdir(self.vector_db_path)
        has_files = len(files) > 0
        print(f"üìÅ Database Has Files: {has_files} ({len(files)} files)")
        return has_files
    
     return False

class RateLimitManager:
    """Production-level rate limiting per user"""
    
    def __init__(self):
        self.user_requests = defaultdict(lambda: {
            'minute_requests': deque(maxlen=100),
            'daily_requests': deque(maxlen=10000),
            'total_requests': 0,
            'last_request': None,
            'warning_count': 0,
            'blocked_until': None
        })
        
        # Rate limits
        self.REQUESTS_PER_MINUTE = 15
        self.REQUESTS_PER_HOUR = 200
        self.REQUESTS_PER_DAY = 1000
        self.BLOCK_DURATION = 300  # 5 minutes
        
    def check_rate_limit(self, user_id: str) -> Dict[str, Any]:
        """Check if user is within rate limits"""
        current_time = time.time()
        user_data = self.user_requests[user_id]
        
        # Check if user is currently blocked
        if user_data['blocked_until'] and current_time < user_data['blocked_until']:
            return {
                'allowed': False,
                'reason': 'temporarily_blocked',
                'blocked_until': user_data['blocked_until'],
                'remaining_time': user_data['blocked_until'] - current_time
            }
        
        # Clean old requests
        minute_ago = current_time - 60
        hour_ago = current_time - 3600
        day_ago = current_time - 86400
        
        # Remove old minute requests
        while user_data['minute_requests'] and user_data['minute_requests'][0] < minute_ago:
            user_data['minute_requests'].popleft()
        
        # Remove old daily requests
        while user_data['daily_requests'] and user_data['daily_requests'][0] < day_ago:
            user_data['daily_requests'].popleft()
        
        # Count recent requests
        minute_count = len(user_data['minute_requests'])
        hour_count = len([t for t in user_data['daily_requests'] if t > hour_ago])
        day_count = len(user_data['daily_requests'])
        
        # Check limits
        if minute_count >= self.REQUESTS_PER_MINUTE:
            user_data['warning_count'] += 1
            if user_data['warning_count'] >= 3:
                user_data['blocked_until'] = current_time + self.BLOCK_DURATION
            return {
                'allowed': False,
                'reason': 'minute_limit_exceeded',
                'minute_count': minute_count,
                'limit': self.REQUESTS_PER_MINUTE
            }
        
        if hour_count >= self.REQUESTS_PER_HOUR:
            return {
                'allowed': False,
                'reason': 'hour_limit_exceeded',
                'hour_count': hour_count,
                'limit': self.REQUESTS_PER_HOUR
            }
        
        if day_count >= self.REQUESTS_PER_DAY:
            return {
                'allowed': False,
                'reason': 'day_limit_exceeded',
                'day_count': day_count,
                'limit': self.REQUESTS_PER_DAY
            }
        
        # Allow request and log it
        user_data['minute_requests'].append(current_time)
        user_data['daily_requests'].append(current_time)
        user_data['total_requests'] += 1
        user_data['last_request'] = current_time
        user_data['warning_count'] = max(0, user_data['warning_count'] - 0.1)  # Gradual reduction
        
        return {
            'allowed': True,
            'usage': {
                'minute': f"{minute_count + 1}/{self.REQUESTS_PER_MINUTE}",
                'hour': f"{hour_count + 1}/{self.REQUESTS_PER_HOUR}",
                'day': f"{day_count + 1}/{self.REQUESTS_PER_DAY}"
            }
        }

# ========== MULTI-KEY ROTATION SYSTEM ==========
class MultiKeyRotationManager:
    """
    Enterprise-grade multi-key rotation system with global round-robin logic
    Features: Global rotation, health monitoring, failover, analytics
    """
    
    def __init__(self):
        self.provider_keys = self._load_provider_keys()
        self.global_key_round = 0  # Global round counter across all providers
        self.max_keys_per_provider = self._calculate_max_keys()
        self.key_status = {}
        self.provider_health = {}
        self.rotation_history = deque(maxlen=1000)  # Track last 1000 rotations
        self.last_reset_time = time.time()
        self.emergency_mode = False
        self.initialize_key_tracking()
        self._setup_monitoring()
        
    def _load_provider_keys(self) -> Dict[str, List[str]]:
        """Load multiple API keys for each provider from environment"""
        provider_keys = {}
        
        # Updated providers list - removed TOGETHER, added new free providers
        providers = [
            'GROQ', 'GOOGLE', 'OPENROUTER', 'HUGGINGFACE', 
            'COHERE', 'NVIDIA', 'AI21', 'CEREBRAS', 'MISTRAL',
            'SCALEWAY', 'OVHCLOUD', 'FIREWORKS', 'REPLICATE', 
            'GITHUB', 'AIMLAPI', 'DEEPSEEK', 'ANTHROPIC'
        ]
        
        for provider in providers:
            keys = []
            # Load up to 10 keys per provider
            for i in range(1, 11):
                key = os.getenv(f"{provider}_API_KEY_{i}")
                if key and key.strip():
                    keys.append(key.strip())
            
            # Backward compatibility for single key format
            single_key = os.getenv(f"{provider}_API_KEY")
            if single_key and single_key.strip() and single_key not in keys:
                keys.insert(0, single_key.strip())
                
            if keys:
                provider_keys[provider] = keys
                logger.info(f"‚úÖ Loaded {len(keys)} API keys for {provider}")
                
        logger.info(f"üöÄ Total providers loaded: {len(provider_keys)}")
        logger.info(f"üìä Total API keys available: {sum(len(keys) for keys in provider_keys.values())}")
        
        return provider_keys
    
    def _calculate_max_keys(self) -> int:
        """Calculate maximum keys available per provider for round-robin"""
        if not self.provider_keys:
            return 10
        return max(len(keys) for keys in self.provider_keys.values())
    
    def initialize_key_tracking(self):
        """Initialize comprehensive tracking for all keys with health monitoring"""
        current_time = time.time()
        
        for provider, keys in self.provider_keys.items():
            self.key_status[provider] = {}
            self.provider_health[provider] = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'average_response_time': 0.0,
                'last_success_time': None,
                'consecutive_failures': 0,
                'health_score': 1.0,  # 0.0 to 1.0
                'is_healthy': True
            }
            
            for idx, key in enumerate(keys):
                self.key_status[provider][idx] = {
                    'key': key[:8] + '***' + key[-4:] if len(key) > 12 else key,  # Masked for logging
                    'raw_key': key,  # Store actual key separately
                    'requests_made': 0,
                    'successful_requests': 0,
                    'failed_requests': 0,
                    'last_used': None,
                    'quota_exhausted': False,
                    'rate_limited': False,
                    'error_count': 0,
                    'consecutive_errors': 0,
                    'success_rate': 1.0,
                    'average_response_time': 0.0,
                    'health_score': 1.0,
                    'is_healthy': True,
                    
                    # Rate limiting tracking
                    'rate_limit_reset': current_time,
                    'requests_this_minute': 0,
                    'requests_this_hour': 0,
                    'daily_requests': 0,
                    'daily_reset': current_time,
                    'hourly_reset': current_time,
                    
                    # Advanced metrics
                    'first_used': None,
                    'total_uptime': 0.0,
                    'last_error_time': None,
                    'last_success_time': None,
                    'provider_priority': self._get_provider_priority(provider),
                    
                    # Rate limits per provider (conservative estimates)
                    'max_requests_per_minute': self._get_provider_rate_limit(provider, 'minute'),
                    'max_requests_per_hour': self._get_provider_rate_limit(provider, 'hour'),
                    'max_requests_per_day': self._get_provider_rate_limit(provider, 'day'),
                }
    
    def _get_provider_priority(self, provider: str) -> int:
        """Get provider priority for intelligent routing"""
        priority_map = {
            'GOOGLE': 1,      # Best free tier - 1M tokens/min
            'GROQ': 2,        # Fastest inference - 14K tokens/sec
            'OPENROUTER': 3,  # Multiple models
            'AIMLAPI': 4,     # 200+ models
            'HUGGINGFACE': 5, # Reliable free tier
            'MISTRAL': 6,     # Good for coding
            'NVIDIA': 7,      # Good quality
            'COHERE': 8,      # Decent free tier
            'ANTHROPIC': 9,   # High quality when available
            'REPLICATE': 10,  # Model variety
        }
        return priority_map.get(provider, 15)
    
    def _get_provider_rate_limit(self, provider: str, period: str) -> int:
        """Get conservative rate limits per provider"""
        limits = {
            'GOOGLE': {'minute': 60, 'hour': 1500, 'day': 30000},    # Very generous
            'GROQ': {'minute': 30, 'hour': 500, 'day': 14400},       # Fast but limited
            'OPENROUTER': {'minute': 20, 'hour': 200, 'day': 1000},  # Free tier
            'AIMLAPI': {'minute': 25, 'hour': 300, 'day': 2000},     # Good free tier
            'HUGGINGFACE': {'minute': 10, 'hour': 100, 'day': 1000}, # Conservative
            'MISTRAL': {'minute': 15, 'hour': 150, 'day': 1500},     # Moderate
            'NVIDIA': {'minute': 20, 'hour': 200, 'day': 2000},      # Decent
            'COHERE': {'minute': 15, 'hour': 100, 'day': 1000},      # Limited
            'ANTHROPIC': {'minute': 10, 'hour': 50, 'day': 500},     # Usually paid
            'REPLICATE': {'minute': 10, 'hour': 100, 'day': 800},    # Variable
        }
        
        default_limits = {'minute': 10, 'hour': 100, 'day': 1000}
        return limits.get(provider, default_limits).get(period, default_limits[period])
    
    def get_active_key(self, provider: str, preferred_model: str = None) -> Optional[Tuple[str, int, Dict]]:
        """
        Get active key using GLOBAL ROUND-ROBIN logic
        Returns: (api_key, key_index, key_metadata) or None
        """
        if provider not in self.provider_keys:
            logger.warning(f"‚ùå Provider {provider} not found in available providers")
            return None
        
        provider_keys = self.provider_keys[provider]
        current_time = time.time()
        
        # Calculate which key index to use based on global round
        key_index = self.global_key_round % len(provider_keys)
        
        # Try the key in current global round
        key_info = self.key_status[provider][key_index]
        
        if self._is_key_available(provider, key_index, current_time):
            # Log the selection for monitoring
            self.rotation_history.append({
                'timestamp': current_time,
                'provider': provider,
                'key_index': key_index,
                'global_round': self.global_key_round,
                'action': 'key_selected'
            })
            
            return key_info['raw_key'], key_index, {
                'provider': provider,
                'key_index': key_index,
                'global_round': self.global_key_round,
                'health_score': key_info['health_score'],
                'requests_made': key_info['requests_made'],
                'success_rate': key_info['success_rate']
            }
        
        # If current round key is not available, try other keys in this provider
        for offset in range(1, len(provider_keys)):
            alternative_index = (key_index + offset) % len(provider_keys)
            if self._is_key_available(provider, alternative_index, current_time):
                alt_key_info = self.key_status[provider][alternative_index]
                
                logger.info(f"üîÑ Using alternative key {alternative_index} for {provider} (global round {self.global_key_round})")
                
                return alt_key_info['raw_key'], alternative_index, {
                    'provider': provider,
                    'key_index': alternative_index,
                    'global_round': self.global_key_round,
                    'health_score': alt_key_info['health_score'],
                    'requests_made': alt_key_info['requests_made'],
                    'success_rate': alt_key_info['success_rate'],
                    'is_fallback': True
                }
        
        # All keys for this provider are exhausted
        logger.warning(f"‚ö†Ô∏è All keys exhausted for {provider} in round {self.global_key_round}")
        return None
    
    def _is_key_available(self, provider: str, key_index: int, current_time: float) -> bool:
        """Check if key is available based on multiple factors"""
        key_info = self.key_status[provider][key_index]
        
        # Check if key is healthy
        if not key_info['is_healthy'] or key_info['quota_exhausted']:
            return False
        
        # Check rate limits
        self._reset_counters_if_needed(key_info, current_time)
        
        # Conservative rate limiting
        if (key_info['requests_this_minute'] >= key_info['max_requests_per_minute'] or
            key_info['requests_this_hour'] >= key_info['max_requests_per_hour'] or
            key_info['daily_requests'] >= key_info['max_requests_per_day']):
            return False
        
        # Check consecutive errors
        if key_info['consecutive_errors'] >= 3:
            return False
        
        # Check if recently failed (avoid for 5 minutes after failure)
        if (key_info['last_error_time'] and 
            current_time - key_info['last_error_time'] < 300):  # 5 minutes
            return False
            
        return True
    
    def _reset_counters_if_needed(self, key_info: Dict, current_time: float):
        """Reset rate limiting counters when time windows expire"""
        # Reset minute counter
        if current_time - key_info['rate_limit_reset'] >= 60:
            key_info['requests_this_minute'] = 0
            key_info['rate_limit_reset'] = current_time
        
        # Reset hourly counter
        if current_time - key_info['hourly_reset'] >= 3600:
            key_info['requests_this_hour'] = 0
            key_info['hourly_reset'] = current_time
        
        # Reset daily counter
        if current_time - key_info['daily_reset'] >= 86400:
            key_info['daily_requests'] = 0
            key_info['daily_reset'] = current_time
            # Also reset quota exhausted status daily
            key_info['quota_exhausted'] = False
    
    def advance_global_round(self):
        """Advance global round - call this after trying all providers"""
        old_round = self.global_key_round
        self.global_key_round += 1
        
        # Reset to round 0 if we've exceeded max keys
        if self.global_key_round >= self.max_keys_per_provider:
            self.global_key_round = 0
            self._reset_daily_quotas()
            logger.info("üîÑ Completed all key rounds, resetting to round 0")
        
        logger.info(f"üìà Advanced global round: {old_round} ‚Üí {self.global_key_round}")
        
        # Log rotation event
        self.rotation_history.append({
            'timestamp': time.time(),
            'old_round': old_round,
            'new_round': self.global_key_round,
            'action': 'global_round_advanced'
        })
    
    def _reset_daily_quotas(self):
        """Reset daily quotas for all keys (emergency reset)"""
        current_time = time.time()
        for provider_status in self.key_status.values():
            for key_info in provider_status.values():
                if current_time - key_info.get('daily_reset', 0) > 86400:
                    key_info['quota_exhausted'] = False
                    key_info['daily_requests'] = 0
                    key_info['daily_reset'] = current_time
        logger.info("üîÑ Reset daily quotas for all keys")
    
    def mark_key_exhausted(self, provider: str, key_index: int, error_type: str = "quota", error_details: str = ""):
        """Mark key as exhausted or problematic with detailed tracking"""
        if provider not in self.key_status or key_index not in self.key_status[provider]:
            return
        
        key_info = self.key_status[provider][key_index]
        current_time = time.time()
        
        # Update error tracking
        key_info['error_count'] += 1
        key_info['failed_requests'] += 1
        key_info['consecutive_errors'] += 1
        key_info['last_error_time'] = current_time
        key_info['success_rate'] = max(0.1, key_info['success_rate'] - 0.1)
        key_info['health_score'] = max(0.1, key_info['health_score'] - 0.2)
        
        # Update provider health
        self.provider_health[provider]['failed_requests'] += 1
        self.provider_health[provider]['consecutive_failures'] += 1
        
        if error_type == "quota":
            key_info['quota_exhausted'] = True
            logger.warning(f"üö® Key {key_index} for {provider} quota exhausted: {error_details}")
        elif error_type == "rate_limit":
            key_info['rate_limited'] = True
            logger.warning(f"‚è∞ Key {key_index} for {provider} rate limited: {error_details}")
        elif error_type == "auth_error":
            key_info['is_healthy'] = False
            logger.error(f"üîê Key {key_index} for {provider} auth failed: {error_details}")
        
        # Check if provider should be marked unhealthy
        if self.provider_health[provider]['consecutive_failures'] >= 5:
            self.provider_health[provider]['is_healthy'] = False
            logger.error(f"üíÄ Provider {provider} marked as unhealthy")
    
    def update_key_success(self, provider: str, key_index: int, response_time: float, tokens_used: int = 0):
        """Update key success metrics with comprehensive tracking"""
        if provider not in self.key_status or key_index not in self.key_status[provider]:
            return
        
        key_info = self.key_status[provider][key_index]
        current_time = time.time()
        
        # Update request counters
        key_info['requests_made'] += 1
        key_info['successful_requests'] += 1
        key_info['requests_this_minute'] += 1
        key_info['requests_this_hour'] += 1
        key_info['daily_requests'] += 1
        key_info['last_used'] = current_time
        key_info['last_success_time'] = current_time
        key_info['consecutive_errors'] = 0  # Reset error streak
        
        # Update timing metrics
        if key_info['first_used'] is None:
            key_info['first_used'] = current_time
        
        # Update response time (exponential moving average)
        if key_info['average_response_time'] == 0:
            key_info['average_response_time'] = response_time
        else:
            alpha = 0.2  # Smoothing factor
            key_info['average_response_time'] = (
                alpha * response_time + 
                (1 - alpha) * key_info['average_response_time']
            )
        
        # Update success rate and health score
        total_requests = key_info['requests_made']
        success_rate = key_info['successful_requests'] / total_requests if total_requests > 0 else 1.0
        key_info['success_rate'] = success_rate
        key_info['health_score'] = min(1.0, key_info['health_score'] + 0.01)
        key_info['is_healthy'] = success_rate > 0.8 and key_info['consecutive_errors'] < 3
        
        # Update provider health
        provider_health = self.provider_health[provider]
        provider_health['total_requests'] += 1
        provider_health['successful_requests'] += 1
        provider_health['consecutive_failures'] = 0
        provider_health['last_success_time'] = current_time
        
        # Update provider average response time
        if provider_health['average_response_time'] == 0:
            provider_health['average_response_time'] = response_time
        else:
            provider_health['average_response_time'] = (
                0.1 * response_time + 
                0.9 * provider_health['average_response_time']
            )
        
        # Update provider health score
        total_prov_requests = provider_health['total_requests']
        provider_success_rate = provider_health['successful_requests'] / total_prov_requests if total_prov_requests > 0 else 1.0
        provider_health['health_score'] = provider_success_rate
        provider_health['is_healthy'] = provider_success_rate > 0.7
    
    def _setup_monitoring(self):
        """Setup comprehensive monitoring and analytics"""
        self.monitoring_data = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'round_rotations': 0,
            'provider_failures': defaultdict(int),
            'hourly_usage': defaultdict(int),
            'start_time': time.time()
        }
    
    def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """Get detailed statistics for monitoring and optimization"""
        current_time = time.time()
        uptime = current_time - self.monitoring_data['start_time']
        
        # Calculate provider rankings by performance
        provider_rankings = []
        for provider, health in self.provider_health.items():
            if health['total_requests'] > 0:
                score = (
                    health['health_score'] * 0.4 +
                    (1 - health['average_response_time'] / 10) * 0.3 +  # Normalize response time
                    (health['successful_requests'] / health['total_requests']) * 0.3
                )
                provider_rankings.append((provider, score))
        
        provider_rankings.sort(key=lambda x: x[1], reverse=True)
        
        # Key distribution analysis
        key_distribution = {}
        for provider, keys_data in self.key_status.items():
            key_distribution[provider] = {
                'total_keys': len(keys_data),
                'healthy_keys': len([k for k in keys_data.values() if k['is_healthy']]),
                'exhausted_keys': len([k for k in keys_data.values() if k['quota_exhausted']]),
                'active_utilization': sum(k['requests_made'] for k in keys_data.values()),
                'average_success_rate': sum(k['success_rate'] for k in keys_data.values()) / len(keys_data) if keys_data else 0
            }
        
        return {
            'system_overview': {
                'total_providers': len(self.provider_keys),
                'total_api_keys': sum(len(keys) for keys in self.provider_keys.values()),
                'current_global_round': self.global_key_round,
                'max_rounds_available': self.max_keys_per_provider,
                'system_uptime_hours': uptime / 3600,
                'emergency_mode': self.emergency_mode,
                'rotation_efficiency': f"{(self.global_key_round / self.max_keys_per_provider) * 100:.1f}%"
            },
            
            'performance_metrics': {
                'total_requests_processed': self.monitoring_data['total_requests'],
                'success_rate': (
                    self.monitoring_data['successful_requests'] / 
                    max(1, self.monitoring_data['total_requests'])
                ) * 100,
                'requests_per_hour': self.monitoring_data['total_requests'] / max(1, uptime / 3600),
                'round_rotations': self.monitoring_data['round_rotations'],
                'average_round_duration': uptime / max(1, self.monitoring_data['round_rotations'])
            },
            
            'provider_rankings': provider_rankings[:10],  # Top 10 providers
            
            'provider_health': {
                provider: {
                    'health_score': f"{health['health_score']:.2%}",
                    'success_rate': f"{health['successful_requests'] / max(1, health['total_requests']):.2%}",
                    'avg_response_time': f"{health['average_response_time']:.2f}s",
                    'is_healthy': health['is_healthy'],
                    'total_requests': health['total_requests']
                }
                for provider, health in self.provider_health.items()
            },
            
            'key_distribution': key_distribution,
            
            'recent_rotations': list(self.rotation_history)[-10:],  # Last 10 rotations
            
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """Generate intelligent recommendations for optimization"""
        recommendations = []
        
        # Check for underperforming providers
        for provider, health in self.provider_health.items():
            if health['total_requests'] > 10 and health['health_score'] < 0.7:
                recommendations.append(f"üîß Consider reviewing {provider} API keys - low health score")
        
        # Check round efficiency
        round_efficiency = (self.global_key_round / self.max_keys_per_provider) * 100
        if round_efficiency > 80:
            recommendations.append("‚ö° Consider adding more API keys - approaching round limit")
        
        # Check for unused providers
        unused_providers = [p for p, h in self.provider_health.items() if h['total_requests'] == 0]
        if unused_providers:
            recommendations.append(f"üí° Unused providers detected: {', '.join(unused_providers)}")
        
        return recommendations
    
    def emergency_fallback_mode(self):
        """Activate emergency fallback mode"""
        self.emergency_mode = True
        self.global_key_round = 0
        self._reset_daily_quotas()
        
        # Reset all health scores to give failing keys another chance
        for provider_status in self.key_status.values():
            for key_info in provider_status.values():
                key_info['is_healthy'] = True
                key_info['consecutive_errors'] = 0
                key_info['quota_exhausted'] = False
        
        logger.warning("üö® Emergency fallback mode activated - resetting all key statuses")

# ========== LOCAL LLM FALLBACK SYSTEM ==========
class LocalLLMFallback:
    """Local LLM fallback when all cloud APIs fail"""
    
    def __init__(self):
        self.ollama_available = self._check_ollama()
        self.local_models = ['llama3.1:8b', 'mistral:7b', 'phi3:mini']
        self.fallback_responses = self._initialize_fallback_responses()
    
    def _check_ollama(self) -> bool:
        """Check if Ollama is available locally"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    async def get_local_response(self, user_input: str, agent_type: str) -> str:
        """Get response from local LLM or intelligent fallback"""
        if self.ollama_available:
            return await self._get_ollama_response(user_input, agent_type)
        else:
            return self._get_intelligent_fallback(user_input, agent_type)
    
    async def _get_ollama_response(self, user_input: str, agent_type: str) -> str:
        """Get response from Ollama local LLM"""
        try:
            # Try each local model
            for model in self.local_models:
                try:
                    prompt = self._create_local_prompt(user_input, agent_type)
                    
                    # Call Ollama API
                    response = requests.post(
                        'http://localhost:11434/api/generate',
                        json={
                            'model': model,
                            'prompt': prompt,
                            'stream': False,
                            'options': {
                                'temperature': 0.7,
                                'top_p': 0.9,
                                'max_tokens': 1000
                            }
                        },
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        return result.get('response', '').strip()
                        
                except Exception as e:
                    logger.error(f"Ollama model {model} failed: {e}")
                    continue
            
            # If all local models fail, use intelligent fallback
            return self._get_intelligent_fallback(user_input, agent_type)
            
        except Exception as e:
            logger.error(f"Ollama processing error: {e}")
            return self._get_intelligent_fallback(user_input, agent_type)
    
    def _create_local_prompt(self, user_input: str, agent_type: str) -> str:
        """Create optimized prompt for local LLM"""
        agent_context = {
            'coding': "You are a coding expert. Provide clear, practical programming guidance.",
            'career': "You are a career coach. Provide professional development advice.",
            'business': "You are a business consultant. Provide strategic business guidance.",
            'medical': "You are a medical information provider. Give health guidance with disclaimers.",
            'emotional': "You are a supportive counselor. Provide empathetic emotional support.",
            'technical_architect': "You are a technical architect. Provide system design guidance.",
            'general': "You are NOVA, a professional AI assistant. Provide helpful, accurate responses."
        }
        
        context = agent_context.get(agent_type, agent_context['general'])
        
        return f"""{context}

User Question: {user_input}

Please provide a professional, helpful response. Be concise but comprehensive.

Response:"""
    
    def _get_intelligent_fallback(self, user_input: str, agent_type: str) -> str:
        """Intelligent fallback responses when local LLM unavailable"""
        return self.fallback_responses.get(agent_type, self.fallback_responses['general'])(user_input)
    
    def _initialize_fallback_responses(self):
        """Initialize intelligent fallback response generators"""
        return {
            'general': lambda q: f"""I understand your question about: "{q[:100]}..."

While I'm currently operating in local mode with limited cloud connectivity, I can still provide guidance on this topic. 

**General Approach:**
1. Research the topic thoroughly using reliable sources
2. Break down complex problems into manageable steps
3. Consider multiple perspectives and solutions
4. Implement best practices and proven methodologies
5. Test and validate your approach

**Recommended Next Steps:**
- Consult authoritative sources and documentation
- Seek expert opinions from professionals in the field
- Consider practical implementation strategies
- Plan for testing and iteration

I'm working to restore full connectivity for more detailed assistance. Please try your question again shortly.""",
            
            'coding': lambda q: f"""Regarding your coding question: "{q[:100]}..."

**Coding Best Practices to Consider:**
- Write clean, readable, and maintainable code
- Implement proper error handling and input validation
- Follow language-specific style guidelines (PEP 8, etc.)
- Use meaningful variable and function names
- Write comprehensive tests for your code
- Document your implementation thoroughly

**General Problem-Solving Approach:**
1. Understand the requirements clearly
2. Break down the problem into smaller components
3. Research existing solutions and patterns
4. Design before implementing
5. Test incrementally
6. Refactor and optimize

For specific implementation details, please consult official documentation, Stack Overflow, or GitHub repositories for similar projects.""",
            
            'career': lambda q: f"""For your career question: "{q[:100]}..."

**Career Development Framework:**
- Continuously assess and update your skills
- Build a strong professional network
- Seek regular feedback and mentorship
- Set clear short and long-term goals
- Stay informed about industry trends
- Develop both technical and soft skills

**Strategic Actions:**
1. Update your professional profiles (LinkedIn, portfolio)
2. Identify skill gaps and create learning plans
3. Network with industry professionals
4. Practice interview skills regularly
5. Research target companies and roles
6. Consider professional certifications

Connect with career professionals, industry groups, and mentors for personalized guidance.""",
            
            'business': lambda q: f"""Regarding your business inquiry: "{q[:100]}..."

**Strategic Business Framework:**
- Conduct thorough market research and analysis
- Understand your target audience deeply
- Develop a strong value proposition
- Monitor key performance indicators
- Plan for sustainable growth
- Manage resources efficiently

**Key Considerations:**
1. Market positioning and competitive analysis
2. Customer acquisition and retention strategies
3. Financial planning and cash flow management
4. Operational efficiency optimization
5. Risk assessment and mitigation
6. Scalability planning

Consult with business advisors, industry experts, and market research for detailed strategic planning.""",
            
            'medical': lambda q: f"""For your health question: "{q[:100]}..."

**Important Medical Disclaimer:**
This is general health information only. Always consult qualified healthcare providers for medical advice, diagnosis, and treatment.

**General Health Principles:**
- Maintain regular check-ups with healthcare providers
- Follow evidence-based health guidelines
- Consider lifestyle factors (diet, exercise, sleep)
- Monitor symptoms and changes carefully
- Seek professional help for concerning symptoms
- Follow prescribed treatments as directed

**Recommended Actions:**
1. Consult with your healthcare provider
2. Research from reputable medical sources
3. Consider second opinions for complex conditions
4. Maintain detailed health records
5. Stay informed about preventive care

Always prioritize professional medical consultation for health concerns.""",
            
            'emotional': lambda q: f"""I hear that you're dealing with: "{q[:100]}..."

Your feelings are valid, and seeking support shows strength.

**Emotional Wellness Strategies:**
- Practice mindfulness and grounding techniques
- Maintain social connections and support systems
- Engage in regular physical activity
- Prioritize adequate sleep and nutrition
- Consider professional counseling when needed
- Develop healthy coping mechanisms

**Immediate Support Actions:**
1. Reach out to trusted friends or family
2. Practice breathing exercises or meditation
3. Engage in activities that bring you comfort
4. Consider journaling or expressive writing
5. Seek professional mental health support
6. Use crisis hotlines if in immediate distress

Remember: Professional therapy and counseling are valuable resources. You don't have to navigate challenges alone.""",
            
            'technical_architect': lambda q: f"""For your technical architecture question: "{q[:100]}..."

**System Architecture Principles:**
- Design for scalability and maintainability
- Implement proper security measures
- Plan for fault tolerance and reliability
- Consider performance optimization
- Document architecture decisions
- Plan for monitoring and observability

**Design Process:**
1. Gather and analyze requirements
2. Research existing patterns and solutions
3. Design system components and interfaces
4. Plan data flow and storage strategies
5. Consider deployment and infrastructure
6. Design monitoring and maintenance procedures

Consult technical documentation, architecture guides, and senior engineers for specific implementation details."""
        }


class Top1PercentModelConfig:
    """Configuration for the absolute best free models available - Top 1% quality December 2025"""
    
    @staticmethod
    def get_premium_model_providers():
        """Get the absolute best free models available for different use cases"""
        return [
            # ============ TIER 1: GOOGLE AI STUDIO - BEST FREE TIER ============
            {
                "name": "Google_AI_Studio",
                "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent",
                "models": {
                    # Gemini 2.5 Flash - Industry leading free tier
                    "reasoning": "gemini-2.5-flash",          # Best reasoning model
                    "multimodal": "gemini-2.5-flash-image",   # Image generation + understanding
                    "speed": "gemini-2.0-flash-lite",        # Ultra fast responses
                    "complex": "gemini-2.5-flash",           # Complex analysis
                    "coding": "gemini-2.5-flash",            # Excellent coding capabilities
                    "creative": "gemini-2.5-flash",          # Creative writing
                    "long_context": "gemini-2.5-flash"       # 1M+ token context
                },
                "priority": 1,
                "specialty": ["reasoning", "multimodal", "speed", "coding", "long_context"],
                "rate_limit": 60,  # 1M tokens per minute!
                "max_tokens": 8192,
                "env_key": "GOOGLE",
                "speed_rating": 10,
                "quality_rating": 9.8,  # Industry leading
                "context_window": 1048576,  # 1M tokens
                "supports_function_calling": True,
                "supports_multimodal": True,
                "free_tier_generous": True,  # Most generous free tier
                "custom_format": True
            },
            
            # ============ TIER 1: GROQ - FASTEST INFERENCE ============
            {
                "name": "Groq_Ultra",
                "url": "https://api.groq.com/openai/v1/chat/completions",
                "models": {
                    # Latest Llama models with ultra-fast inference
                    "reasoning": "llama-3.3-70b-versatile",     # Latest Llama 3.3 70B
                    "speed": "llama-3.1-8b-instant",           # Ultra fast (14K+ tokens/sec)
                    "complex": "llama-3.1-70b-versatile",      # Complex analysis
                    "coding": "llama-3-groq-70b-8192-tool-use-preview",  # Best for coding
                    "creative": "llama-3.1-70b-versatile",     # Creative tasks
                    "lightweight": "llama-3.1-8b-instant"      # Quick responses
                },
                "priority": 2,
                "specialty": ["speed", "reasoning", "coding"],
                "rate_limit": 30,
                "max_tokens": 8192,
                "env_key": "GROQ",
                "speed_rating": 10,  # Fastest in industry (14K+ tokens/sec)
                "quality_rating": 9.5,
                "context_window": 8192,
                "supports_function_calling": True,
                "inference_speed": "14000_tokens_per_sec"
            },
            
            # ============ TIER 1: DEEPSEEK - BEST CODING MODELS ============
            {
                "name": "DeepSeek_Coding",
                "url": "https://api.deepseek.com/v1/chat/completions",
                "models": {
                    "reasoning": "deepseek-reasoner",          # New reasoning model
                    "coding": "deepseek-coder-v3",             # Best free coding model
                    "speed": "deepseek-chat",                  # Fast general model
                    "creative": "deepseek-chat-v3",            # Creative tasks
                    "analysis": "deepseek-reasoner"            # Deep analysis
                },
                "priority": 3,
                "specialty": ["coding", "reasoning", "analysis"],
                "rate_limit": 25,
                "max_tokens": 4096,
                "env_key": "DEEPSEEK",
                "speed_rating": 9,
                "quality_rating": 9.7,  # Best for coding
                "context_window": 32768,
                "supports_function_calling": True,
                "coding_specialist": True
            },
            
            # ============ TIER 1: CEREBRAS - FASTEST INFERENCE ============
            {
                "name": "Cerebras_Speed",
                "url": "https://api.cerebras.ai/v1/chat/completions",
                "models": {
                    "speed": "llama3.1-8b",        # 1800 tokens/sec
                    "reasoning": "llama3.1-70b",   # 450 tokens/sec - still ultra fast
                    "complex": "llama3.1-70b",     # Complex tasks
                    "multilingual": "llama3.1-8b"  # Multilingual support
                },
                "priority": 4,
                "specialty": ["speed", "reasoning"],
                "rate_limit": 20,
                "max_tokens": 8192,
                "env_key": "CEREBRAS",
                "speed_rating": 10,  # Industry fastest
                "quality_rating": 9.0,
                "context_window": 8192,
                "supports_function_calling": False,
                "inference_speed": "1800_tokens_per_sec"
            },
            
            # ============ TIER 1: MISTRAL AI - EUROPEAN EXCELLENCE ============
            {
                "name": "Mistral_Premium",
                "url": "https://api.mistral.ai/v1/chat/completions",
                "models": {
                    # Free open models from Mistral
                    "reasoning": "magistral-small-2509",       # Latest reasoning model
                    "coding": "codestral-latest",              # Free coding specialist
                    "creative": "mistral-small-2506",          # Creative tasks
                    "multilingual": "mistral-nemo",            # Best multilingual
                    "audio": "voxtral-small-2507",             # Audio processing
                    "vision": "pixtral-12b-2409"               # Vision capabilities
                },
                "priority": 5,
                "specialty": ["reasoning", "coding", "multilingual", "audio", "vision"],
                "rate_limit": 15,
                "max_tokens": 128000,  # Large context
                "env_key": "MISTRAL",
                "speed_rating": 8,
                "quality_rating": 9.2,
                "context_window": 128000,
                "supports_function_calling": True,
                "supports_multimodal": True
            },
            
            # ============ TIER 1: OPENROUTER PREMIUM FREE MODELS ============
            {
                "name": "OpenRouter_Premium",
                "url": "https://openrouter.ai/api/v1/chat/completions",
                "models": {
                    # Best free models available on OpenRouter
                    "reasoning": "meta-llama/llama-3.1-70b-instruct:free",
                    "creative": "microsoft/wizardlm-2-8x22b:free",
                    "coding": "deepseek/deepseek-coder-v3:free",
                    "multimodal": "google/gemma-2-9b-it:free",
                    "speed": "mistralai/mistral-7b-instruct:free",
                    "analysis": "anthropic/claude-3-haiku:free",  # When available
                    "specialized": "meta-llama/llama-3.1-8b-instruct:free"
                },
                "priority": 6,
                "specialty": ["creative", "coding", "multimodal", "analysis"],
                "rate_limit": 25,
                "max_tokens": 4096,
                "env_key": "OPENROUTER",
                "speed_rating": 8,
                "quality_rating": 9.5,
                "context_window": 4096,
                "supports_function_calling": True,
                "model_variety": "200+"
            },
            
            # ============ TIER 2: AIMLAPI - 200+ FREE MODELS ============
            {
                "name": "AIMLAPI_Diverse",
                "url": "https://api.aimlapi.com/v1/chat/completions",
                "models": {
                    "reasoning": "gpt-4o-mini",               # GPT-4o Mini free
                    "creative": "claude-3-haiku",            # Claude Haiku free
                    "coding": "codellama-34b-instruct",      # Code specialist
                    "speed": "llama-3.1-8b-instruct",       # Fast responses
                    "multimodal": "llava-1.5-7b-hf"         # Vision model
                },
                "priority": 7,
                "specialty": ["reasoning", "creative", "coding", "multimodal"],
                "rate_limit": 25,
                "max_tokens": 4096,
                "env_key": "AIMLAPI",
                "speed_rating": 8,
                "quality_rating": 9.0,
                "context_window": 4096,
                "supports_function_calling": True,
                "available_models": "200+"
            },
            
            # ============ TIER 2: NVIDIA NIM - OPTIMIZED INFERENCE ============
            {
                "name": "NVIDIA_Optimized",
                "url": "https://integrate.api.nvidia.com/v1/chat/completions",
                "models": {
                    "reasoning": "nvidia/llama-3.1-nemotron-70b-instruct",
                    "speed": "meta/llama-3.1-8b-instruct",
                    "technical": "nvidia/llama-3.1-nemotron-70b-instruct",
                    "creative": "nvidia/llama-3.1-nemotron-51b-instruct"
                },
                "priority": 8,
                "specialty": ["reasoning", "technical", "optimization"],
                "rate_limit": 15,
                "max_tokens": 4096,
                "env_key": "NVIDIA",
                "speed_rating": 7,
                "quality_rating": 9.0,
                "context_window": 4096,
                "supports_function_calling": False,
                "gpu_optimized": True
            },
            
            # ============ TIER 2: SAMBANOVA - ULTRA FAST ============
            {
                "name": "SambaNova_Fast",
                "url": "https://api.sambanova.ai/v1/chat/completions",
                "models": {
                    "speed": "Meta-Llama-3.1-8B-Instruct",
                    "reasoning": "Meta-Llama-3.1-70B-Instruct",
                    "creative": "Meta-Llama-3.1-405B-Instruct"  # When available
                },
                "priority": 9,
                "specialty": ["speed", "reasoning"],
                "rate_limit": 15,
                "max_tokens": 4096,
                "env_key": "SAMBANOVA",
                "speed_rating": 9,
                "quality_rating": 8.5,
                "context_window": 4096,
                "supports_function_calling": False,
                "hardware_optimized": True
            },
            
            # ============ TIER 2: AI21 JAMBA MODELS ============
            {
                "name": "AI21_Advanced",
                "url": "https://api.ai21.com/studio/v1/chat/completions",
                "models": {
                    "reasoning": "jamba-1.5-large",
                    "speed": "jamba-1.5-mini",
                    "analysis": "jamba-instruct"
                },
                "priority": 10,
                "specialty": ["reasoning", "analysis"],
                "rate_limit": 10,
                "max_tokens": 4096,
                "env_key": "AI21",
                "speed_rating": 7,
                "quality_rating": 8.5,
                "context_window": 256000,  # Large context window
                "supports_function_calling": True,
                "long_context_specialist": True
            },
            
            # ============ TIER 2: FIREWORKS OPTIMIZED ============
            {
                "name": "Fireworks_Fast",
                "url": "https://api.fireworks.ai/inference/v1/chat/completions",
                "models": {
                    "speed": "accounts/fireworks/models/llama-v3p1-8b-instruct",
                    "reasoning": "accounts/fireworks/models/llama-v3p1-70b-instruct",
                    "creative": "accounts/fireworks/models/mixtral-8x7b-instruct"
                },
                "priority": 11,
                "specialty": ["speed", "reasoning"],
                "rate_limit": 20,
                "max_tokens": 4096,
                "env_key": "FIREWORKS",
                "speed_rating": 9,
                "quality_rating": 8.5,
                "context_window": 4096,
                "supports_function_calling": True,
                "optimized_inference": True
            },
            
            # ============ TIER 3: GITHUB MODELS (Microsoft) ============
            {
                "name": "GitHub_Models",
                "url": "https://models.inference.ai.azure.com/chat/completions",
                "models": {
                    "speed": "Phi-3.5-mini-instruct",
                    "reasoning": "Meta-Llama-3.1-70B-Instruct",
                    "coding": "Meta-Llama-3.1-8B-Instruct"
                },
                "priority": 12,
                "specialty": ["coding", "speed"],
                "rate_limit": 10,
                "max_tokens": 4096,
                "env_key": "GITHUB",
                "speed_rating": 7,
                "quality_rating": 8.0,
                "context_window": 4096,
                "supports_function_calling": False,
                "microsoft_backed": True
            },
            
            # ============ BACKUP TIER: RELIABLE FALLBACKS ============
            {
                "name": "HuggingFace_Diverse",
                "url": "https://api-inference.huggingface.co/models/",
                "models": {
                    "conversational": "microsoft/DialoGPT-large",
                    "creative": "meta-llama/Meta-Llama-3-8B-Instruct",
                    "coding": "bigcode/starcoder2-15b",
                    "multilingual": "google/flan-t5-xxl"
                },
                "priority": 13,
                "specialty": ["conversational", "creative", "multilingual"],
                "rate_limit": 15,
                "max_tokens": 2048,
                "env_key": "HUGGINGFACE",
                "custom_format": True,
                "speed_rating": 6,
                "quality_rating": 7.5,
                "context_window": 2048,
                "supports_function_calling": False,
                "model_hub": True
            },
            
            # ============ BACKUP TIER: COHERE COMMAND ============
            {
                "name": "Cohere_Command",
                "url": "https://api.cohere.ai/v1/chat",
                "models": {
                    "reasoning": "command-r-plus",
                    "creative": "command-r",
                    "speed": "command-light"
                },
                "priority": 14,
                "specialty": ["reasoning", "creative"],
                "rate_limit": 10,
                "max_tokens": 4096,
                "env_key": "COHERE",
                "speed_rating": 7,
                "quality_rating": 8.0,
                "context_window": 128000,
                "supports_function_calling": True,
                "custom_format": True
            }
            
            # TOGETHER AI REMOVED - Now paid service
            # REPLICATE moved to lower priority due to pricing changes
        ]
    
    @staticmethod
    def get_model_by_use_case(use_case: str) -> List[Dict]:
        """Get best models for specific use case"""
        providers = Top1PercentModelConfig.get_premium_model_providers()
        
        use_case_mapping = {
            "speed": ["Google_AI_Studio", "Groq_Ultra", "Cerebras_Speed", "SambaNova_Fast"],
            "reasoning": ["Google_AI_Studio", "DeepSeek_Coding", "Mistral_Premium", "Groq_Ultra"],
            "coding": ["DeepSeek_Coding", "Mistral_Premium", "Groq_Ultra", "AIMLAPI_Diverse"],
            "creative": ["Google_AI_Studio", "OpenRouter_Premium", "Mistral_Premium", "AIMLAPI_Diverse"],
            "multimodal": ["Google_AI_Studio", "Mistral_Premium", "OpenRouter_Premium"],
            "long_context": ["Google_AI_Studio", "Mistral_Premium", "AI21_Advanced"],
            "multilingual": ["Mistral_Premium", "Google_AI_Studio", "HuggingFace_Diverse"],
            "free_tier": ["Google_AI_Studio", "Groq_Ultra", "DeepSeek_Coding", "OpenRouter_Premium"]
        }
        
        recommended_providers = use_case_mapping.get(use_case, ["Google_AI_Studio", "Groq_Ultra"])
        
        return [p for p in providers if p["name"] in recommended_providers]
    
    @staticmethod
    def get_fallback_chain() -> List[str]:
        """Get recommended fallback chain for maximum reliability"""
        return [
            "Google_AI_Studio",    # Best free tier
            "Groq_Ultra",          # Fastest inference  
            "DeepSeek_Coding",     # Best coding
            "Cerebras_Speed",      # Ultra fast backup
            "Mistral_Premium",     # European reliability
            "OpenRouter_Premium",  # Model variety
            "AIMLAPI_Diverse",     # 200+ models
            "NVIDIA_Optimized",    # GPU optimized
            "SambaNova_Fast",      # Hardware optimized
            "Fireworks_Fast"       # Final fallback
        ]
    
    @staticmethod
    def get_provider_statistics() -> Dict[str, Any]:
        """Get comprehensive provider statistics"""
        providers = Top1PercentModelConfig.get_premium_model_providers()
        
        return {
            "total_providers": len(providers),
            "total_models": sum(len(p["models"]) for p in providers),
            "tier_1_providers": len([p for p in providers if p["priority"] <= 6]),
            "supports_function_calling": len([p for p in providers if p.get("supports_function_calling", False)]),
            "supports_multimodal": len([p for p in providers if p.get("supports_multimodal", False)]),
            "average_context_window": sum(p["context_window"] for p in providers) // len(providers),
            "top_speed_providers": [p["name"] for p in providers if p["speed_rating"] >= 9],
            "top_quality_providers": [p["name"] for p in providers if p["quality_rating"] >= 9],
            "estimated_daily_capacity": len(providers) * 1000 * 10,  # Conservative estimate
            "removed_providers": ["Together_Enterprise"],  # Track removed providers
            "new_additions_2025": ["Google_AI_Studio", "DeepSeek_Coding", "Mistral_Premium"]
        }

# ========== INTELLIGENT MODEL ROUTER ==========
class IntelligentModelRouter:
    """Advanced smart routing system that selects the best model for each query type with AI-powered analysis"""
    
    def __init__(self):
        self.query_patterns = {
            # ============ CODING & DEVELOPMENT ============
            "coding": {
                "keywords": [
                    "code", "programming", "debug", "python", "javascript", "java", "c++", "rust", "go",
                    "react", "nodejs", "api", "function", "algorithm", "database", "sql", "mongodb",
                    "git", "github", "deployment", "testing", "unittest", "pytest", "frontend", "backend", 
                    "fullstack", "django", "flask", "express", "vue", "angular", "svelte", "nextjs",
                    "docker", "kubernetes", "aws", "azure", "gcp", "terraform", "ansible",
                    "error", "exception", "syntax", "variable", "class", "method", "loop", "array", 
                    "object", "json", "xml", "yaml", "regex", "refactor", "optimize", "performance",
                    "microservices", "rest api", "graphql", "websocket", "async", "await", "promise",
                    "machine learning", "ai", "neural network", "tensorflow", "pytorch", "scikit-learn"
                ],
                "patterns": [
                    r"write.*code", r"create.*function", r"debug.*error", r"fix.*bug",
                    r"implement.*feature", r"optimize.*code", r"refactor.*code", r"code.*review",
                    r"build.*app", r"develop.*system", r"create.*api", r"setup.*environment",
                    r"install.*package", r"configure.*server", r"deploy.*application"
                ],
                "preferred_models": ["coding", "reasoning", "speed"],
                "confidence_threshold": 0.7,
                "model_priority": ["DeepSeek_Coding", "Mistral_Premium", "Groq_Ultra", "Google_AI_Studio"]
            },
            
            # ============ REASONING & ANALYSIS ============
            "reasoning": {
                "keywords": [
                    "analyze", "analysis", "compare", "evaluate", "assess", "examine", "investigate",
                    "strategy", "plan", "solution", "approach", "methodology", "framework", "architecture",
                    "pros and cons", "advantages", "disadvantages", "trade-offs", "benefits", "drawbacks",
                    "research", "study", "explore", "understand", "explain", "clarify", "elaborate",
                    "complex", "complicated", "detailed", "comprehensive", "thorough", "in-depth",
                    "critical thinking", "decision making", "problem solving", "logical", "systematic",
                    "evidence", "hypothesis", "theory", "principle", "concept", "reasoning", "inference"
                ],
                "patterns": [
                    r"how to.*", r"what.*best way", r"explain.*why", r"analyze.*", r"reason.*about",
                    r"compare.*with", r"what.*difference", r"help.*understand", r"think.*through",
                    r"step.*by.*step", r"break.*down", r"evaluate.*options", r"consider.*factors"
                ],
                "preferred_models": ["reasoning", "complex", "analysis"],
                "confidence_threshold": 0.6,
                "model_priority": ["Google_AI_Studio", "Groq_Ultra", "DeepSeek_Coding", "Mistral_Premium"]
            },
            
            # ============ CREATIVE & CONTENT ============
            "creative": {
                "keywords": [
                    "creative", "write", "story", "poem", "essay", "article", "blog", "novel", "screenplay",
                    "content", "copy", "copywriting", "script", "dialogue", "character", "plot", "narrative",
                    "brainstorm", "ideas", "generate", "create", "compose", "draft", "outline", "summary",
                    "marketing", "advertisement", "social media", "caption", "headline", "slogan", "tagline",
                    "email", "newsletter", "press release", "proposal", "presentation", "pitch", "resume",
                    "creative writing", "storytelling", "worldbuilding", "fiction", "non-fiction"
                ],
                "patterns": [
                    r"write.*story", r"create.*content", r"generate.*ideas", r"compose.*",
                    r"brainstorm.*", r"help.*write", r"draft.*", r"come up with.*",
                    r"creative.*", r"storytelling.*", r"marketing.*copy"
                ],
                "preferred_models": ["creative", "reasoning", "speed"],
                "confidence_threshold": 0.7,
                "model_priority": ["Google_AI_Studio", "OpenRouter_Premium", "Mistral_Premium", "AIMLAPI_Diverse"]
            },
            
            # ============ SPEED & QUICK RESPONSES ============
            "speed": {
                "keywords": [
                    "quick", "fast", "simple", "brief", "short", "summary", "overview", "concise",
                    "basic", "simple question", "yes or no", "definition", "meaning", "what is",
                    "who is", "when", "where", "how much", "how many", "list", "enumerate",
                    "quickly", "immediately", "urgent", "asap", "time sensitive"
                ],
                "patterns": [
                    r"^(what|who|when|where|how|why).*\?$", r"define.*", r"meaning.*of.*",
                    r"quick.*question", r"simple.*", r"briefly.*", r"in.*words",
                    r"yes.*or.*no", r"true.*or.*false", r"list.*", r"enumerate.*"
                ],
                "preferred_models": ["speed", "reasoning"],
                "confidence_threshold": 0.5,
                "model_priority": ["Groq_Ultra", "Cerebras_Speed", "Google_AI_Studio", "SambaNova_Fast"]
            },
            
            # ============ MULTIMODAL (Images, Audio, Video) ============
            "multimodal": {
                "keywords": [
                    "image", "picture", "photo", "visual", "diagram", "chart", "graph", "screenshot",
                    "video", "audio", "voice", "sound", "music", "speech", "transcript",
                    "vision", "see", "look", "show", "display", "visualize", "render",
                    "multimodal", "multimedia", "mixed media", "interactive", "animation",
                    "ocr", "text extraction", "image analysis", "computer vision", "pattern recognition"
                ],
                "patterns": [
                    r"analyze.*image", r"describe.*picture", r"extract.*text", r"read.*image",
                    r"process.*video", r"transcribe.*audio", r"generate.*image", r"create.*visual",
                    r"image.*to.*text", r"speech.*to.*text", r"text.*to.*speech"
                ],
                "preferred_models": ["multimodal", "vision", "audio"],
                "confidence_threshold": 0.8,
                "model_priority": ["Google_AI_Studio", "Mistral_Premium", "OpenRouter_Premium"]
            },
            
            # ============ BUSINESS & STRATEGY ============
            "business": {
                "keywords": [
                    "business", "strategy", "market", "revenue", "profit", "growth", "scaling",
                    "sales", "marketing", "customer", "product", "startup", "investment", "funding",
                    "roi", "kpi", "metrics", "analytics", "competition", "competitor", "analysis", "plan",
                    "management", "leadership", "team", "project", "budget", "finance", "accounting",
                    "valuation", "acquisition", "merger", "ipo", "enterprise", "b2b", "b2c",
                    "saas", "ecommerce", "digital transformation", "automation", "efficiency"
                ],
                "patterns": [
                    r"business.*plan", r"market.*analysis", r"revenue.*model", r"pricing.*strategy",
                    r"growth.*strategy", r"competitive.*analysis", r"swot.*analysis",
                    r"go.*to.*market", r"customer.*acquisition", r"retention.*strategy"
                ],
                "preferred_models": ["reasoning", "analysis", "complex"],
                "confidence_threshold": 0.7,
                "model_priority": ["Google_AI_Studio", "Groq_Ultra", "OpenRouter_Premium", "Mistral_Premium"]
            },
            
            # ============ TECHNICAL & SYSTEM DESIGN ============
            "technical": {
                "keywords": [
                    "architecture", "system", "design", "infrastructure", "devops", "sre",
                    "cloud", "aws", "azure", "gcp", "kubernetes", "docker", "monitoring", "observability",
                    "performance", "security", "scalability", "reliability", "availability", "microservices",
                    "database", "server", "network", "protocol", "api", "service", "load balancer",
                    "cdn", "cache", "redis", "elasticsearch", "kafka", "rabbitmq", "nginx",
                    "distributed systems", "high availability", "fault tolerance", "disaster recovery"
                ],
                "patterns": [
                    r"system.*design", r"architecture.*", r"infrastructure.*", r"scale.*system",
                    r"scalability.*", r"performance.*optimization", r"security.*architecture",
                    r"distributed.*system", r"microservices.*architecture", r"cloud.*architecture"
                ],
                "preferred_models": ["reasoning", "technical", "complex"],
                "confidence_threshold": 0.7,
                "model_priority": ["Google_AI_Studio", "Groq_Ultra", "NVIDIA_Optimized", "DeepSeek_Coding"]
            },
            
            # ============ MULTILINGUAL & TRANSLATION ============
            "multilingual": {
                "keywords": [
                    "hindi", "‡§π‡§ø‡§Ç‡§¶‡•Ä", "spanish", "espa√±ol", "french", "fran√ßais", "german", "deutsch",
                    "chinese", "‰∏≠Êñá", "japanese", "Êó•Êú¨Ë™û", "korean", "ÌïúÍµ≠Ïñ¥", "arabic", "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©",
                    "translate", "translation", "language", "multilingual", "localization", "l10n",
                    "‡§≠‡§æ‡§∑‡§æ", "‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶", "‡§Æ‡§∞‡§æ‡§†‡•Ä", "‡§§‡§Æ‡§ø‡§≤", "‡§¨‡§Ç‡§ó‡§æ‡§≤‡•Ä", "‡§ó‡•Å‡§ú‡§∞‡§æ‡§§‡•Ä", "‡§™‡§Ç‡§ú‡§æ‡§¨‡•Ä", "‡§§‡•á‡§≤‡•Å‡§ó‡•Å",
                    "international", "global", "cross-cultural", "native speaker", "fluent"
                ],
                "patterns": [
                    r"translate.*", r"in hindi", r"in spanish", r".*‡§≠‡§æ‡§∑‡§æ.*", r".*‡§π‡§ø‡§Ç‡§¶‡•Ä.*", 
                    r".*‡§Æ‡§∞‡§æ‡§†‡•Ä.*", r".*‡§Æ‡•á‡§Ç.*", r"language.*", r".*translation.*",
                    r"speak.*", r"say.*in.*", r"how.*to.*say"
                ],
                "preferred_models": ["multilingual", "reasoning"],
                "confidence_threshold": 0.8,
                "model_priority": ["Mistral_Premium", "Google_AI_Studio", "HuggingFace_Diverse"]
            },
            
            # ============ LONG CONTEXT & DOCUMENTS ============
            "long_context": {
                "keywords": [
                    "document", "pdf", "report", "research paper", "thesis", "dissertation", "book",
                    "long text", "summarize", "summary", "extract", "analyze document", "review",
                    "comprehensive", "detailed analysis", "full document", "entire text", "whole paper",
                    "context", "background", "history", "timeline", "chronological", "sequence"
                ],
                "patterns": [
                    r"summarize.*document", r"analyze.*pdf", r"extract.*from.*document",
                    r"read.*entire.*", r"full.*analysis", r"comprehensive.*review",
                    r"long.*text", r"entire.*context", r"whole.*document"
                ],
                "preferred_models": ["long_context", "reasoning", "analysis"],
                "confidence_threshold": 0.7,
                "model_priority": ["Google_AI_Studio", "AI21_Advanced", "Mistral_Premium"]
            },
            
            # ============ RESEARCH & ACADEMIC ============
            "research": {
                "keywords": [
                    "research", "academic", "scholar", "university", "paper", "journal", "publication",
                    "methodology", "experiment", "hypothesis", "theory", "literature review", "citation",
                    "peer review", "conference", "symposium", "thesis", "dissertation", "phd",
                    "scientific", "evidence based", "empirical", "quantitative", "qualitative",
                    "statistics", "data analysis", "findings", "conclusion", "recommendation"
                ],
                "patterns": [
                    r"research.*", r"academic.*", r"scientific.*", r"study.*shows",
                    r"literature.*review", r"methodology.*", r"experiment.*design",
                    r"data.*analysis", r"statistical.*", r"peer.*reviewed"
                ],
                "preferred_models": ["reasoning", "analysis", "complex"],
                "confidence_threshold": 0.7,
                "model_priority": ["Google_AI_Studio", "Groq_Ultra", "OpenRouter_Premium"]
            },
            
            # ============ CONVERSATIONAL & CASUAL ============
            "conversational": {
                "keywords": [
                    "chat", "talk", "conversation", "casual", "friendly", "informal", "personal",
                    "hello", "hi", "hey", "thanks", "thank you", "please", "help", "assistance",
                    "opinion", "thoughts", "feelings", "experience", "story", "share",
                    "recommend", "suggest", "advice", "tip", "guidance", "support"
                ],
                "patterns": [
                    r"^(hi|hello|hey).*", r"how.*are.*you", r"what.*do.*you.*think",
                    r"can.*you.*help", r"i.*need.*", r"tell.*me.*about",
                    r"what.*would.*you.*", r"any.*suggestions", r"recommend.*"
                ],
                "preferred_models": ["conversational", "creative", "speed"],
                "confidence_threshold": 0.5,
                "model_priority": ["Google_AI_Studio", "Groq_Ultra", "HuggingFace_Diverse", "Cohere_Command"]
            }
        }
        
        # Enhanced pattern weights for better accuracy
        self.pattern_weights = {
            "keyword_match": 2.0,
            "pattern_match": 3.5,
            "length_bonus": 1.5,
            "complexity_bonus": 2.0,
            "context_bonus": 1.0
        }
        
        # Model performance cache for dynamic optimization
        self.model_performance = defaultdict(lambda: {
            'success_rate': 1.0,
            'avg_response_time': 0.0,
            'total_requests': 0
        })
    
    def detect_query_type(self, user_input: str, context: Dict = None) -> Tuple[str, List[str], float, Dict]:
        """
        Enhanced query analysis with context awareness and confidence scoring
        Returns:
        - Primary query type
        - List of preferred model types in order
        - Confidence score (0.0 to 1.0)
        - Additional metadata
        """
        text_lower = user_input.lower()
        text_length = len(user_input.split())
        scores = {}
        metadata = {
            'text_length': text_length,
            'detected_patterns': [],
            'context_signals': [],
            'processing_complexity': 'medium'
        }
        
        # Context-aware scoring enhancement
        if context:
            if context.get('has_images', False):
                scores['multimodal'] = scores.get('multimodal', 0) + 5
                metadata['context_signals'].append('images_detected')
            
            if context.get('document_length', 0) > 1000:
                scores['long_context'] = scores.get('long_context', 0) + 3
                metadata['context_signals'].append('long_document')
            
            if context.get('language') and context['language'] != 'en':
                scores['multilingual'] = scores.get('multilingual', 0) + 4
                metadata['context_signals'].append(f"language_{context['language']}")
        
        # Enhanced pattern matching with weighted scoring
        for query_type, config in self.query_patterns.items():
            score = 0
            detected_patterns = []
            
            # Keyword matching with TF-IDF-like weighting
            keyword_matches = []
            for keyword in config["keywords"]:
                if keyword in text_lower:
                    keyword_matches.append(keyword)
                    # Longer keywords get more weight
                    weight = len(keyword.split()) * self.pattern_weights["keyword_match"]
                    score += weight
            
            # Pattern matching with regex
            pattern_matches = []
            for pattern in config["patterns"]:
                if re.search(pattern, text_lower, re.IGNORECASE):
                    pattern_matches.append(pattern)
                    score += self.pattern_weights["pattern_match"]
            
            # Length and complexity bonuses
            if query_type == "reasoning" and text_length > 15:
                score += self.pattern_weights["length_bonus"] * (text_length / 10)
                metadata['processing_complexity'] = 'high'
            
            if query_type == "speed" and text_length <= 10:
                score += self.pattern_weights["complexity_bonus"]
                metadata['processing_complexity'] = 'low' 
            
            if query_type == "long_context" and text_length > 50:
                score += self.pattern_weights["length_bonus"] * 2
                metadata['processing_complexity'] = 'high'
            
            # Technical complexity detection
            technical_indicators = ['architecture', 'system', 'scalability', 'performance', 'distributed']
            if query_type == "technical" and any(indicator in text_lower for indicator in technical_indicators):
                score += self.pattern_weights["complexity_bonus"]
            
            # Code-specific patterns
            code_patterns = [r'```', r'function\s+\w+\s*\(', r'class\s+\w+\s*:', r'import\s+\w+', r'console\.log', r'print\(']
            if query_type == "coding" and any(re.search(p, user_input) for p in code_patterns):
                score += self.pattern_weights["pattern_match"] * 2
                detected_patterns.extend(['code_block_detected'])
            
            if score > 0:
                scores[query_type] = score
                if keyword_matches:
                    metadata['detected_patterns'].extend(keyword_matches[:3])  # Top 3 matches
                if pattern_matches:
                    metadata['detected_patterns'].extend(detected_patterns)
        
        # Fallback for unclear queries
        if not scores:
            return "conversational", ["conversational", "speed"], 0.5, metadata
        
        # Get primary type and calculate confidence
        primary_type = max(scores, key=scores.get)
        max_score = scores[primary_type]
        
        # Enhanced confidence calculation
        total_score = sum(scores.values())
        confidence = min(0.95, (max_score / max(total_score, 1)) * 0.8 + 0.2)
        
        # Adjust confidence based on clarity indicators
        if len(scores) == 1:  # Clear single category
            confidence += 0.1
        elif len(scores) > 5:  # Too many categories - unclear
            confidence -= 0.1
        
        confidence = max(0.1, min(0.95, confidence))
        
        # Get preferred models with performance-based reordering
        preferred_models = self.query_patterns[primary_type]["preferred_models"].copy()
        model_priority = self.query_patterns[primary_type].get("model_priority", [])
        
        # Add secondary preferences from other high-scoring types
        sorted_types = sorted(scores.items(), key=lambda x: x, reverse=True)[1:3][1]
        for query_type, score in sorted_types:
            for model in self.query_patterns[query_type]["preferred_models"]:
                if model not in preferred_models:
                    preferred_models.append(model)
        
        # Performance-based reordering (if we have performance data)
        if model_priority:
            reordered_models = []
            for model in model_priority:
                if model not in reordered_models:
                    reordered_models.append(model)
            
            # Add remaining models
            for model in preferred_models:
                if model not in reordered_models:
                    reordered_models.append(model)
            
            preferred_models = reordered_models
        
        metadata.update({
            'all_scores': scores,
            'primary_score': max_score,
            'total_categories_detected': len(scores),
            'confidence_factors': {
                'pattern_clarity': len(scores) <= 2,
                'length_appropriate': text_length > 5,
                'context_available': context is not None
            }
        })
        
        return primary_type, preferred_models, confidence, metadata
    
    def get_optimal_provider_sequence(self, query_type: str, preferred_models: List[str]) -> List[str]:
        """Get optimized provider sequence based on query type and model preferences"""
        
        # Map model types to actual provider names from Top1PercentModelConfig
        model_to_provider_mapping = {
            "coding": ["DeepSeek_Coding", "Mistral_Premium", "Groq_Ultra", "Google_AI_Studio"],
            "reasoning": ["Google_AI_Studio", "Groq_Ultra", "DeepSeek_Coding", "OpenRouter_Premium"],
            "creative": ["Google_AI_Studio", "OpenRouter_Premium", "Mistral_Premium", "AIMLAPI_Diverse"],
            "speed": ["Groq_Ultra", "Cerebras_Speed", "Google_AI_Studio", "SambaNova_Fast"],
            "multimodal": ["Google_AI_Studio", "Mistral_Premium", "OpenRouter_Premium"],
            "long_context": ["Google_AI_Studio", "AI21_Advanced", "Mistral_Premium"],
            "multilingual": ["Mistral_Premium", "Google_AI_Studio", "HuggingFace_Diverse"],
            "technical": ["Google_AI_Studio", "Groq_Ultra", "NVIDIA_Optimized", "DeepSeek_Coding"],
            "analysis": ["Google_AI_Studio", "Groq_Ultra", "OpenRouter_Premium", "Mistral_Premium"],
            "conversational": ["Google_AI_Studio", "Groq_Ultra", "HuggingFace_Diverse", "Cohere_Command"]
        }
        
        provider_sequence = []
        
        # Build sequence based on preferred models
        for model_type in preferred_models:
            if model_type in model_to_provider_mapping:
                for provider in model_to_provider_mapping[model_type]:
                    if provider not in provider_sequence:
                        provider_sequence.append(provider)
        
        # Ensure we have fallbacks from top-tier providers
        top_tier_fallbacks = ["Google_AI_Studio", "Groq_Ultra", "DeepSeek_Coding", "Mistral_Premium"]
        for provider in top_tier_fallbacks:
            if provider not in provider_sequence:
                provider_sequence.append(provider)
        
        return provider_sequence[:8]  # Limit to top 8 providers for efficiency
    
    def update_model_performance(self, provider: str, query_type: str, success: bool, response_time: float):
        """Update model performance metrics for adaptive routing optimization"""
        key = f"{provider}_{query_type}"
        perf = self.model_performance[key]
        
        perf['total_requests'] += 1
        
        if success:
            # Exponential moving average for success rate
            alpha = 0.1
            perf['success_rate'] = alpha + (1 - alpha) * perf['success_rate']
        else:
            perf['success_rate'] = max(0.1, perf['success_rate'] * 0.9)
        
        # Update average response time
        if perf['avg_response_time'] == 0:
            perf['avg_response_time'] = response_time
        else:
            alpha = 0.2
            perf['avg_response_time'] = alpha * response_time + (1 - alpha) * perf['avg_response_time']
    
    def get_routing_analytics(self) -> Dict[str, Any]:
        """Get comprehensive routing analytics for monitoring and optimization"""
        total_requests = sum(perf['total_requests'] for perf in self.model_performance.values())
        
        # Calculate category distribution
        category_stats = defaultdict(lambda: {'requests': 0, 'avg_success': 0.0})
        
        for key, perf in self.model_performance.items():
            if '_' in key:
                category = key.split('_', 1)[1]
                category_stats[category]['requests'] += perf['total_requests']
                category_stats[category]['avg_success'] += perf['success_rate'] * perf['total_requests']
        
        # Normalize success rates
        for stats in category_stats.values():
            if stats['requests'] > 0:
                stats['avg_success'] /= stats['requests']
        
        return {
            'total_requests_routed': total_requests,
            'active_categories': len(category_stats),
            'category_distribution': dict(category_stats),
            'top_performing_combinations': sorted(
                [(k, v['success_rate']) for k, v in self.model_performance.items() if v['total_requests'] > 5],
                key=lambda x: x[1],
                reverse=True
            )[:10],
            'routing_efficiency': {
                'avg_confidence': 0.75,  # This would be calculated from actual routing data
                'category_accuracy': len([k for k, v in self.model_performance.items() if v['success_rate'] > 0.8]) / max(1, len(self.model_performance)),
                'response_time_optimization': sum(1 for v in self.model_performance.values() if v['avg_response_time'] < 2.0) / max(1, len(self.model_performance))
            }
        }
    
# ========== ENHANCED API MANAGER WITH MULTI-KEY ROTATION ==========
class EnhancedProductionAPIManager:
    """Ultra-advanced API manager with intelligent model routing and global round-robin"""
    
    def __init__(self):
        self.key_manager = MultiKeyRotationManager()
        self.local_fallback = LocalLLMFallback()
        self.model_router = IntelligentModelRouter()
        
        # Load top 1% model configuration
        self.providers = Top1PercentModelConfig.get_premium_model_providers()
        
        # Filter available providers based on API keys
        self.available = []
        for provider in self.providers:
            if provider.get('local'):
                if self.local_fallback.ollama_available:
                    self.available.append(provider)
                    logger.info(f"‚úÖ Local provider {provider['name']} available")
            else:
                if self.key_manager.provider_keys.get(provider['env_key']):
                    self.available.append(provider)
                    key_count = len(self.key_manager.provider_keys[provider['env_key']])
                    logger.info(f"‚úÖ {provider['name']} available with {key_count} keys - Quality: {provider['quality_rating']}/10")
                else:
                    logger.debug(f"‚ùå {provider['name']} not available - no API keys")
        
        if not self.available:
            logger.error("üö® No premium providers available! Configure API keys for top performance.")
        else:
            total_quality = sum(p['quality_rating'] for p in self.available)
            avg_quality = total_quality / len(self.available)
            logger.info(f"üöÄ Premium API Manager: {len(self.available)} providers, Avg Quality: {avg_quality:.1f}/10")
        
        # Sort by quality and speed
        self.available.sort(key=lambda x: (x['quality_rating'], x['speed_rating']), reverse=True)

        # Enhanced startup probing with 404 detection
        self._probe_and_validate_providers()
        
        # Performance tracking with enhanced metrics
        self.performance_stats = {}
        self.global_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_response_time': 0.0,
            'model_usage': defaultdict(int),
            'query_type_distribution': defaultdict(int),
            'global_round_rotations': 0,
            'provider_health_scores': {},
            'key_rotation_events': 0
        }
        
        # Initialize performance tracking for each provider
        for provider in self.available:
            self.performance_stats[provider['name']] = {
                'response_times': deque(maxlen=50),
                'success_rate': 1.0,
                'total_requests': 0,
                'failures': 0,
                'model_usage': defaultdict(int),
                'quality_score': provider['quality_rating'] / 10,
                'speed_score': provider['speed_rating'] / 10,
                'last_used': None,
                'consecutive_failures': 0,
                'health_score': 1.0,
                'key_utilization': {},
                'fallback_usage': 0
            }
    
    def _probe_and_validate_providers(self):
     """Enhanced provider probing with better error handling"""
     for provider in list(self.available):
        if provider.get('local'):
            continue
            
        # Skip probing for known working APIs
        skip_probe_providers = ['Groq_Ultra', 'OpenRouter_Premium', 'AIMLAPI_Diverse']
        if provider['name'] in skip_probe_providers:
            logger.debug(f"‚úÖ Skipping probe for {provider['name']} (known working)")
            continue
            
        url = provider.get('url')
        if not url:
            logger.warning(f"‚ö†Ô∏è Provider {provider.get('name')} missing URL")
            self.available.remove(provider)
            continue
            
        # Special handling for Google
        if provider['name'] == 'Google_AI_Studio':
            try:
                # Test Google API with proper format
                test_url = "https://generativelanguage.googleapis.com/v1beta/models"
                headers = {"x-goog-api-key": "test"}
                response = requests.get(test_url, headers=headers, timeout=5)
                
                if response.status_code == 403:  # Auth error = API working
                    logger.debug(f"‚úÖ Google API working (needs valid key)")
                elif response.status_code == 404:
                    logger.warning(f"üö´ Google API endpoint not found")
                    self.available.remove(provider)
                    
            except Exception as e:
                logger.debug(f"üîç Google probe: {e}")
    
    def get_optimal_provider_and_model(self, query_type: str, preferred_models: List[str], 
                                     context: Dict = None) -> Optional[Tuple[dict, str, Dict]]:
        """Enhanced provider selection with global round-robin integration"""
        if not self.available:
            logger.error("‚ùå No providers available for selection")
            return None
            
        best_matches = []
        current_time = time.time()
        
        for provider in self.available:
            # Check key availability using global round-robin
            if not provider.get('local'):
                key_data = self.key_manager.get_active_key(provider['env_key'])
                if not key_data:
                    logger.debug(f"üîÑ No active keys for {provider['name']} in current global round")
                    continue
                api_key, key_index, key_metadata = key_data
            else:
                key_index = 0
                key_metadata = {'provider': provider['name'], 'local': True}
            
            # Model selection logic with enhanced matching
            available_models = provider.get('models', {})
            selected_model = None
            specialty_score = 0
            
            if isinstance(available_models, dict):
                # Try to match preferred model types
                for preferred_model in preferred_models:
                    if preferred_model in available_models:
                        selected_model = available_models[preferred_model]
                        specialty_score = 10  # Perfect match
                        break
                
                # Fallback to query-type specific models
                if not selected_model:
                    query_model_map = {
                        'coding': ['coding', 'reasoning', 'speed'],
                        'creative': ['creative', 'reasoning', 'conversational'],
                        'reasoning': ['reasoning', 'analysis', 'complex'],
                        'speed': ['speed', 'reasoning'],
                        'multimodal': ['multimodal', 'vision', 'creative'],
                        'business': ['reasoning', 'analysis'],
                        'technical': ['reasoning', 'coding', 'analysis']
                    }
                    
                    for fallback_type in query_model_map.get(query_type, ['reasoning']):
                        if fallback_type in available_models:
                            selected_model = available_models[fallback_type]
                            specialty_score = 8
                            break
                
                # Final fallback to first available model
                if not selected_model and available_models:
                    selected_model = list(available_models.values())[0]
                    specialty_score = 5
                    
            elif isinstance(available_models, list) and available_models:
                # Legacy list format
                selected_model = available_models[0]
                specialty_score = 8 if query_type in provider.get('specialty', []) else 5
            
            if not selected_model:
                logger.debug(f"‚ùå No suitable model found for {provider['name']}")
                continue
            
            # Enhanced scoring algorithm
            stats = self.performance_stats[provider['name']]
            
            # Base scores (0-10 scale)
            quality_score = provider['quality_rating']
            speed_score = provider['speed_rating']
            performance_score = stats['success_rate'] * 10
            health_score = stats['health_score'] * 10
            
            # Context and capability bonuses
            context_bonus = 0
            if context:
                if context.get('requires_long_context') and provider.get('context_window', 0) > 32000:
                    context_bonus += 3
                if context.get('requires_multimodal') and provider.get('supports_multimodal'):
                    context_bonus += 2
                if context.get('requires_function_calling') and provider.get('supports_function_calling'):
                    context_bonus += 1
            
            # Provider-specific bonuses
            provider_bonus = 0
            if provider.get('free_tier_generous'):
                provider_bonus += 1
            if provider.get('inference_speed') and 'fast' in provider['inference_speed']:
                provider_bonus += 1
            
            # Recency penalty for recently failed providers
            recency_penalty = 0
            if stats['consecutive_failures'] > 0:
                recency_penalty = min(3, stats['consecutive_failures'])
            
            # Key health from global rotation system
            key_health_bonus = key_metadata.get('health_score', 1.0) * 2
            
            # Calculate final composite score
            total_score = (
                quality_score * 0.25 +        # 25% quality
                speed_score * 0.20 +          # 20% speed  
                specialty_score * 0.20 +      # 20% specialization
                performance_score * 0.15 +    # 15% historical performance
                health_score * 0.10 +         # 10% current health
                context_bonus * 0.05 +        # 5% context matching
                provider_bonus * 0.03 +       # 3% provider bonuses
                key_health_bonus * 0.02 -     # 2% key health
                recency_penalty                # Penalty for failures
            )
            
            # Add randomization for providers with similar scores (prevents stuck routing)
            if len([m for m in best_matches if abs(m[0] - total_score) < 0.5]) > 0:
                total_score += random.uniform(-0.2, 0.2)
            
            best_matches.append((total_score, provider, selected_model, key_metadata))
        
        if not best_matches:
            logger.warning("‚ö†Ô∏è No suitable providers found after scoring")
            return None
        
        # Sort by score and return the best
        best_matches.sort(key=lambda x: x[0], reverse=True)
        best_score, best_provider, best_model, key_info = best_matches[0]
        
        # Log selection with detailed info
        logger.info(f"üéØ Selected: {best_provider['name']} - {best_model} "
                   f"(Score: {best_score:.2f}, Round: {self.key_manager.global_key_round}, "
                   f"Key: {key_info.get('key_index', 'N/A')})")
        
        return best_provider, best_model, key_info

    async def get_ai_response(self, user_input: str, system_prompt: str, 
                            query_type: str = "general", context: Dict = None) -> Optional[str]:
     """Enhanced AI response with global round-robin and intelligent fallback"""
     self.global_stats['total_requests'] += 1
     self.global_stats['query_type_distribution'][query_type] += 1
     start_time = time.time()

     try:
        # Step 1: Enhanced query analysis with context
        try:
            detected_type, preferred_models, confidence, metadata = self.model_router.detect_query_type(
                user_input, context
            )
        except ValueError as e:
            # Handle unpacking errors from detect_query_type
            logger.warning(f"üîß Query analysis unpacking issue: {e}")
            detected_type = query_type
            preferred_models = []
            confidence = 0.5
            metadata = {}

        # Use detected type if confidence is high enough
        if confidence > 0.7:
            query_type = detected_type
            logger.info(f"üß† Query type updated: {query_type} (confidence: {confidence:.2f})")

        logger.info(f"üìä Query Analysis: Type={query_type}, Confidence={confidence:.2f}, "
                   f"Models={preferred_models}, Context={bool(context)}")

        # Step 2: Get optimal provider with enhanced context
        enhanced_context = context or {}
        enhanced_context.update({
            'query_complexity': metadata.get('processing_complexity', 'medium'),
            'text_length': len(user_input.split()),
            'requires_long_context': len(user_input.split()) > 100,
            'detected_patterns': metadata.get('detected_patterns', [])
        })

        # ‚úÖ SAFE PROVIDER SELECTION WITH ERROR HANDLING
        try:
            result = self.get_optimal_provider_and_model(query_type, preferred_models, enhanced_context)
            if not result:
                logger.error("‚ùå No optimal provider available")
                return await self._get_intelligent_fallback_response(user_input, query_type, "no_provider")

            # ‚úÖ SAFE UNPACKING - Handle different return formats
            if isinstance(result, tuple):
                if len(result) == 3:
                    provider, model, key_info = result
                elif len(result) == 2:
                    provider, model = result
                    key_info = None
                else:
                    provider = result[0] if result else None
                    model = result[1] if len(result) > 1 else None
                    key_info = None
            else:
                provider = result
                model = None
                key_info = None

            if not provider:
                logger.error("‚ùå Provider selection failed")
                return await self._get_intelligent_fallback_response(user_input, query_type, "no_provider")

        except Exception as e:
            logger.error(f"üîß Provider selection error: {e}")
            return await self._get_intelligent_fallback_response(user_input, query_type, "provider_error")

        # Step 3: Enhanced multi-attempt with global round advancement
        for attempt in range(2):  # Reduced attempts, rely on global rotation
            try:
                response = None
                
                if provider.get('local'):
                    response = await self._call_local_provider(provider, user_input, system_prompt, model)
                else:
                    # Get fresh key data for each attempt
                    fresh_key_data = self.key_manager.get_active_key(provider['env_key'])
                    if not fresh_key_data:
                        logger.warning(f"üîÑ No active keys for {provider['name']} on attempt {attempt + 1}")
                        break
                    
                    # ‚úÖ SAFE KEY DATA UNPACKING
                    try:
                        if isinstance(fresh_key_data, tuple) and len(fresh_key_data) >= 2:
                            api_key = fresh_key_data[0]
                            key_index = fresh_key_data[1]
                            key_meta = fresh_key_data[2] if len(fresh_key_data) > 2 else {}
                        else:
                            api_key = fresh_key_data
                            key_index = 0
                            key_meta = {}
                    except Exception as e:
                        logger.error(f"üîß Key data unpacking error: {e}")
                        continue
                    
                    response = await self._call_cloud_provider(
                        provider, api_key, user_input, system_prompt, model
                    )
                    
                    # Update key success in rotation manager
                    if response and len(response.strip()) > 10:
                        response_time = time.time() - start_time
                        try:
                            self.key_manager.update_key_success(
                                provider['env_key'], key_index, response_time
                            )
                        except Exception as e:
                            logger.warning(f"Key success update failed: {e}")

                if response and len(response.strip()) > 10:  # Quality check
                    response_time = time.time() - start_time
                    self._update_success_stats(provider, model, response_time, query_type)
                    
                    # Update model router performance
                    try:
                        self.model_router.update_model_performance(
                            provider['name'], query_type, True, response_time
                        )
                    except Exception as e:
                        logger.warning(f"Model performance update failed: {e}")

                    logger.info(f"‚úÖ SUCCESS: {provider['name']}/{model} - {response_time:.2f}s "
                               f"- Quality: {len(response)} chars - Round: {getattr(self.key_manager, 'global_key_round', 'N/A')}")
                    
                    return response.strip()

            except requests.exceptions.HTTPError as e:
                error_text = str(e).lower()
                logger.warning(f"‚ö†Ô∏è HTTP Error attempt {attempt + 1} for {provider['name']}: {e}")
                
                # Handle specific errors
                if any(term in error_text for term in ['404', 'not found', '401', '403', 'auth']):
                    logger.error(f"üö´ Terminal error for {provider['name']}, advancing global round")
                    self._handle_provider_failure(provider, "terminal_error", str(e))
                    break
                elif any(term in error_text for term in ['429', 'rate limit']):
                    logger.warning(f"‚è∞ Rate limit for {provider['name']}, marking key exhausted")
                    if not provider.get('local') and 'key_index' in locals():
                        try:
                            self.key_manager.mark_key_exhausted(provider['env_key'], key_index, "rate_limit")
                        except Exception as e:
                            logger.warning(f"Mark key exhausted failed: {e}")
                    break
                
                # Update model router on failure
                try:
                    self.model_router.update_model_performance(
                        provider['name'], query_type, False, time.time() - start_time
                    )
                except Exception as e:
                    logger.warning(f"Model performance update failed: {e}")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Attempt {attempt + 1} failed for {provider['name']}: {e}")
                self._handle_provider_failure(provider, "general_error", str(e))
                continue

        # Step 4: Advance global round and try intelligent fallback
        logger.info("üîÑ Primary provider attempts failed, advancing global round")
        try:
            self.key_manager.advance_global_round()
            self.global_stats['global_round_rotations'] += 1
        except Exception as e:
            logger.warning(f"Global round advancement failed: {e}")
        
        return await self._get_intelligent_fallback_response(user_input, query_type, "primary_failed")

     except Exception as e:
        logger.error(f"üí• Critical error in get_ai_response: {e}")
        self.global_stats['failed_requests'] += 1
        return await self._get_intelligent_fallback_response(user_input, query_type, "critical_error")

    def _handle_provider_failure(self, provider: dict, failure_type: str, error_details: str):
        """Enhanced provider failure handling"""
        stats = self.performance_stats[provider['name']]
        stats['failures'] += 1
        stats['consecutive_failures'] += 1
        stats['success_rate'] = max(0.1, stats['success_rate'] - 0.1)
        stats['health_score'] = max(0.1, stats['health_score'] - 0.15)
        
        # Update global stats
        self.global_stats['failed_requests'] += 1
        self.global_stats['provider_health_scores'][provider['name']] = stats['health_score']
        
        logger.debug(f"üìâ Provider {provider['name']} failure recorded: {failure_type}")

    def _update_success_stats(self, provider: dict, model: str, response_time: float, query_type: str):
        """Enhanced success statistics with query type tracking"""
        stats = self.performance_stats[provider['name']]
        stats['response_times'].append(response_time)
        stats['total_requests'] += 1
        stats['success_rate'] = min(1.0, stats['success_rate'] + 0.02)
        stats['consecutive_failures'] = 0  # Reset failure streak
        stats['health_score'] = min(1.0, stats['health_score'] + 0.05)
        stats['model_usage'][model] += 1
        stats['last_used'] = time.time()
        
        # Update global stats
        self.global_stats['successful_requests'] += 1
        self.global_stats['model_usage'][f"{provider['name']}/{model}"] += 1
        self.global_stats['provider_health_scores'][provider['name']] = stats['health_score']
        
        # Update average response time with exponential moving average
        if self.global_stats['average_response_time'] == 0:
            self.global_stats['average_response_time'] = response_time
        else:
            alpha = 0.1
            self.global_stats['average_response_time'] = (
                alpha * response_time + (1 - alpha) * self.global_stats['average_response_time']
            )

    async def _get_intelligent_fallback_response(self, user_input: str, query_type: str, 
                                               failure_reason: str) -> str:
        """Enhanced intelligent fallback with multiple tiers"""
        logger.info(f"üîÑ Activating intelligent fallback (reason: {failure_reason})")
        
        # Tier 1: Try next best providers from current available list
        if failure_reason != "no_provider":
            for provider in self.available[1:4]:  # Try next 3 providers
                try:
                    if provider.get('local'):
                        continue  # Skip local in first fallback tier
                    
                    key_data = self.key_manager.get_active_key(provider['env_key'])
                    if not key_data:
                        continue
                    
                    api_key, key_index, _ = key_data
                    models = provider.get('models', {})
                    
                    # Select appropriate model for query type
                    if isinstance(models, dict):
                        model = (models.get('reasoning') or models.get('speed') or 
                                models.get('general') or list(models.values())[0])
                    else:
                        model = models[0] if models else None
                    
                    if model:
                        response = await self._call_cloud_provider(
                            provider, api_key, user_input, 
                            f"You are NOVA, an expert AI assistant. Provide helpful, professional responses.",
                            model
                        )
                        
                        if response and len(response.strip()) > 10:
                            logger.info(f"‚úÖ Fallback success: {provider['name']}/{model}")
                            self._update_success_stats(provider, model, 1.0, query_type)  # Estimate time
                            return response.strip()
                            
                except Exception as e:
                    logger.debug(f"üîÑ Fallback provider {provider['name']} failed: {e}")
                    continue
        
        # Tier 2: Local LLM fallback
        if self.local_fallback.ollama_available:
            try:
                logger.info("üè† Attempting local LLM fallback")
                local_response = await self.local_fallback.get_local_response(user_input, query_type)
                if local_response:
                    logger.info("‚úÖ Local fallback successful")
                    self.performance_stats.setdefault('Local_LLM', {})['fallback_usage'] = \
                        self.performance_stats.get('Local_LLM', {}).get('fallback_usage', 0) + 1
                    return local_response
            except Exception as e:
                logger.error(f"‚ùå Local fallback failed: {e}")
        
        # Tier 3: Emergency mode - reset global round and try again
        if not hasattr(self, '_emergency_attempted'):
            logger.warning("üö® Activating emergency mode - resetting global round")
            self._emergency_attempted = True
            self.key_manager.emergency_fallback_mode()
            
            # Try one more time with reset system
            try:
                result = self.get_optimal_provider_and_model(query_type, ['reasoning', 'speed'])
                if result:
                    provider, model, _ = result
                    if not provider.get('local'):
                        key_data = self.key_manager.get_active_key(provider['env_key'])
                        if key_data:
                            api_key, _, _ = key_data
                            response = await self._call_cloud_provider(
                                provider, api_key, user_input,
                                "You are NOVA AI. Provide a helpful response.", model
                            )
                            if response:
                                logger.info("‚úÖ Emergency mode success")
                                delattr(self, '_emergency_attempted')
                                return response.strip()
            except Exception as e:
                logger.error(f"üí• Emergency mode failed: {e}")
        
        # Final tier: Enhanced emergency response
        logger.warning("üÜò All fallback systems exhausted, generating enhanced emergency response")
        return self._get_enhanced_emergency_response(user_input, query_type, failure_reason)

    def _get_enhanced_emergency_response(self, user_input: str, query_type: str, failure_reason: str) -> str:
        """Enhanced emergency responses with failure context"""
        
        base_template = f"""**NOVA AI Assistant - Temporary Service Mode**

I understand you're asking about: "{user_input[:150]}{'...' if len(user_input) > 150 else ''}"

**Current Status:** Experiencing high demand across our API network (Reason: {failure_reason})
**Resolution:** Service restoration in progress, full capacity returning shortly

**Immediate Assistance Framework:**"""

        specialized_guidance = {
            "coding": """

**Development Best Practices:**
‚Ä¢ **Code Structure:** Use meaningful variable names, consistent indentation, and modular functions
‚Ä¢ **Error Handling:** Implement try-catch blocks and input validation throughout
‚Ä¢ **Testing Strategy:** Write unit tests, integration tests, and end-to-end validation
‚Ä¢ **Documentation:** Comment complex logic and maintain README files
‚Ä¢ **Version Control:** Use Git with descriptive commits and branching strategies

**Problem-Solving Methodology:**
1. **Break Down:** Decompose complex problems into smaller, manageable tasks
2. **Research:** Check official documentation, Stack Overflow, and GitHub examples
3. **Prototype:** Create minimal viable implementations first
4. **Iterate:** Refine and optimize after achieving basic functionality
5. **Debug:** Use proper debugging tools and systematic error analysis

**Quality Resources:**
‚Üí Official language documentation and API references
‚Üí Open-source projects on GitHub for real-world examples
‚Üí Code review tools and automated testing frameworks""",

            "creative": """

**Creative Excellence Framework:**
‚Ä¢ **Ideation Process:** Brainstorm freely, then refine based on objectives and audience
‚Ä¢ **Structure & Flow:** Create compelling openings, logical progression, and strong conclusions
‚Ä¢ **Voice & Style:** Maintain consistency while adapting tone to purpose and audience
‚Ä¢ **Engagement Techniques:** Use storytelling, vivid imagery, and emotional resonance
‚Ä¢ **Quality Assurance:** Edit for clarity, impact, and grammatical precision

**Content Development Strategy:**
1. **Research:** Understanding target audience, competitive landscape, and current trends
2. **Planning:** Outline key messages, content pillars, and distribution strategy
3. **Creation:** Develop original, valuable content with unique perspectives
4. **Optimization:** Refine based on performance metrics and audience feedback
5. **Distribution:** Multi-channel approach with platform-specific adaptations""",

            "business": """

**Strategic Business Analysis:**
‚Ä¢ **Market Research:** Customer segmentation, competitive analysis, and industry trends
‚Ä¢ **Business Model:** Value proposition, revenue streams, and cost structure optimization
‚Ä¢ **Growth Planning:** Scalability assessment, resource allocation, and milestone definition
‚Ä¢ **Risk Management:** Identify potential challenges and develop mitigation strategies
‚Ä¢ **Performance Metrics:** KPI definition, tracking systems, and optimization cycles

**Implementation Framework:**
1. **Strategy Definition:** Clear goals, target markets, and competitive positioning
2. **Resource Planning:** Budget allocation, team structure, and technology requirements
3. **Execution Roadmap:** Phased approach with measurable milestones and deadlines
4. **Monitoring System:** Regular performance reviews and strategic adjustments
5. **Optimization Cycle:** Continuous improvement based on data and market feedback""",

            "reasoning": """

**Analytical Thinking Framework:**
‚Ä¢ **Problem Definition:** Clear articulation of the core issue and desired outcomes
‚Ä¢ **Data Collection:** Gather relevant information from credible, diverse sources
‚Ä¢ **Critical Analysis:** Evaluate evidence, identify patterns, and assess reliability
‚Ä¢ **Solution Development:** Generate multiple approaches and evaluate feasibility
‚Ä¢ **Decision Framework:** Weigh pros/cons, assess risks, and consider long-term implications

**Systematic Approach:**
1. **Clarification:** Define terms, assumptions, and scope of analysis
2. **Information Gathering:** Research from multiple perspectives and sources
3. **Analysis:** Break down complex issues into manageable components
4. **Synthesis:** Combine insights to form comprehensive understanding
5. **Conclusion:** Present recommendations with supporting evidence and reasoning"""
        }

        guidance = specialized_guidance.get(query_type, specialized_guidance["reasoning"])
        
        return f"""{base_template}
{guidance}

**Service Status:** Our multi-provider network includes {len(self.available)} premium AI services with automatic failover
**Quality Guarantee:** Full-featured responses will resume momentarily with zero data loss
**Technical Note:** Global key rotation system maintains 99.9% uptime across {sum(len(keys) for keys in self.key_manager.provider_keys.values())} API keys

*This temporary response ensures you receive immediate value while our enhanced AI network recalibrates.*"""

    # Rest of the existing methods remain the same but with enhanced logging and monitoring
    async def _call_cloud_provider(self, provider: dict, api_key: str, user_input: str,
                                 system_prompt: str, model: str) -> Optional[str]:
        """Enhanced cloud provider calling with comprehensive error handling"""
        try:
            headers = self._create_headers(provider, api_key)
            payload = self._format_messages_for_provider(provider, user_input, system_prompt, model)

            url = provider.get('url')
            if provider.get('name') == 'HuggingFace_Diverse' and model:
                url = f"{url.rstrip('/')}/{model}"

            # Dynamic timeout based on provider speed rating
            timeout = 60 if provider.get('speed_rating', 0) < 6 else 45 if provider.get('speed_rating', 0) < 8 else 30

            response = requests.post(url, headers=headers, json=payload, timeout=timeout)

            # Enhanced error handling
            if response.status_code == 404:
                logger.error(f"üö´ Provider {provider['name']} returned 404 for URL: {url}")
                # Remove from available providers for this session
                if provider in self.available:
                    try:
                        self.available.remove(provider)
                        logger.info(f"üóëÔ∏è Removed {provider['name']} from available providers")
                    except ValueError:
                        pass
                raise requests.exceptions.HTTPError(f"404 Not Found from {provider['name']} at {url}")

            if response.status_code in (401, 403):
                logger.error(f"üîê Auth error from {provider['name']} ({response.status_code}) - check API key")
                raise requests.exceptions.HTTPError(f"Auth error {response.status_code} from {provider['name']}")

            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                logger.warning(f"‚è∞ Rate limited by {provider['name']}, Retry-After: {retry_after}s")
                raise requests.exceptions.HTTPError(f"Rate limited by {provider['name']} (429)")

            response.raise_for_status()
            result = response.json()
            content = self._extract_content(result, provider['name'])
            
            if content:
                logger.debug(f"‚úÖ Successfully extracted {len(content)} characters from {provider['name']}")
            
            return content

        except requests.exceptions.HTTPError:
            raise  # Re-raise HTTP errors for upstream handling
        except Exception as e:
            logger.error(f"üí• Cloud provider call failed ({provider.get('name')}): {e}")
            raise e

    def _format_messages_for_provider(self, provider: dict, user_input: str, 
                                system_prompt: str, model: str) -> dict:
     """Enhanced message formatting with model-specific optimizations"""
    
     # Google AI Studio uses different format
     if provider['name'] == 'Google_AI_Studio':
        return {
            "contents": [{
                "parts": [{
                    "text": f"{system_prompt}\n\nUser: {user_input}\n\nAssistant:"
                }]
            }],
            "generationConfig": {
                "temperature": 0.7,
                "topP": 0.9,
                "topK": 40,
                "maxOutputTokens": min(provider.get('max_tokens', 2048), 2048),
                "stopSequences": []
            },
            "safetySettings": [{
                "category": "HARM_CATEGORY_HARASSMENT", 
                "threshold": "BLOCK_NONE"
            }]
        }
    
     # Handle special provider formats
     if provider['name'] == 'HuggingFace_Diverse':
        return {
            "inputs": f"System: {system_prompt}\n\nUser: {user_input}\n\nAssistant:",
            "parameters": {
                "max_new_tokens": min(provider.get('max_tokens', 2048), 1500),
                "temperature": 0.7,
                "return_full_text": False,
                "do_sample": True,
                "top_p": 0.9,
                "repetition_penalty": 1.1
            }
        }
    
     # Enhanced system prompt based on provider capabilities
     enhanced_system_prompt = system_prompt
     if provider.get('supports_function_calling'):
        enhanced_system_prompt += " You have access to advanced reasoning capabilities."
     if provider.get('supports_multimodal'):
        enhanced_system_prompt += " You can process multiple types of content."

     # Standard OpenAI format with provider-specific optimizations
     messages = [
        {"role": "system", "content": enhanced_system_prompt},
        {"role": "user", "content": user_input}
     ]
    
     # Dynamic parameter optimization based on model and query type
     temperature = 0.7
     if any(term in model.lower() for term in ['coding', 'code', 'technical']):
        temperature = 0.3  # More deterministic for technical content
     elif any(term in model.lower() for term in ['creative', 'story', 'write']):
        temperature = 0.9  # More creative for content generation
     elif any(term in model.lower() for term in ['reasoning', 'analysis']):
        temperature = 0.5  # Balanced for analytical tasks

     # Provider-specific parameter tuning
     max_tokens = min(provider.get('max_tokens', 4096), 2500)
     if provider.get('context_window', 0) > 100000:  # Long context models
        max_tokens = min(8000, max_tokens)

     return {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1,
        "stream": False
     }

    def _create_headers(self, provider: dict, api_key: str) -> dict:
     """Enhanced headers with provider-specific optimizations"""
     base_headers = {
        "Content-Type": "application/json",
        "User-Agent": "NOVA-Ultra-AI/4.0-Production"
     }
    
     # Google uses different header format
     if provider['name'] == 'Google_AI_Studio':
        base_headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key  # ‚úÖ Google specific header
        }
     elif provider['name'] == 'OpenRouter_Premium':
        base_headers.update({
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://nova-ai.app",
            "X-Title": "NOVA Ultra AI Assistant"
        })
     elif provider['name'] == 'AI21_Advanced':
        base_headers.update({
            "Authorization": f"Bearer {api_key}",
            "X-API-Key": api_key
        })
     elif provider['name'] == 'GitHub_Models':
        base_headers.update({
            "Authorization": f"Bearer {api_key}",
            "X-GitHub-Token": api_key
        })
     elif provider['name'] == 'HuggingFace_Diverse':
        base_headers.update({
            "Authorization": f"Bearer {api_key}"
        })
     else:
        # Standard authorization for most providers
        base_headers["Authorization"] = f"Bearer {api_key}"
    
     return base_headers

    def _extract_content(self, result: dict, provider_name: str) -> Optional[str]:
        """Enhanced content extraction with quality validation"""
        try:
            content = None
            
            if provider_name == 'HuggingFace_Diverse':
                if isinstance(result, list) and len(result) > 0:
                    generated = result[0].get('generated_text', '')
                    if 'Assistant:' in generated:
                        content = generated.split('Assistant:')[-1].strip()
                    else:
                        content = generated.strip()
                else:
                    content = result.get('generated_text', '').strip()
            else:
                # Standard OpenAI format
                choices = result.get("choices", [])
                if choices and len(choices) > 0:
                    message = choices[0].get("message", {})
                    content = message.get("content", "").strip()

            # Enhanced quality validation
            if content:
                # Remove common AI refusal patterns
                refusal_patterns = [
                    "I'm sorry, but I can't",
                    "I cannot provide",
                    "I'm not able to",
                    "I don't have access to"
                ]
                
                if any(pattern in content for pattern in refusal_patterns):
                    logger.debug(f"‚ö†Ô∏è Content appears to be refusal from {provider_name}")
                    return None
                
                # Length and quality checks
                if len(content) < 10:
                    logger.debug(f"‚ö†Ô∏è Content too short from {provider_name}: {len(content)} chars")
                    return None
                
                # Check for placeholder responses
                if content.lower() in ['ok', 'yes', 'no', 'sure', 'maybe']:
                    logger.debug(f"‚ö†Ô∏è Placeholder response from {provider_name}: {content}")
                    return None
                
                return content
            
        except Exception as e:
            logger.error(f"üí• Content extraction error for {provider_name}: {e}")
        
        return None

    # Keep existing methods for compatibility
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Enhanced comprehensive statistics with global round-robin data"""
        key_stats = self.key_manager.get_comprehensive_statistics()
        
        return {
            "global_stats": self.global_stats,
            "provider_stats": {
                name: {
                    'success_rate': f"{stats.get('success_rate', 0):.2%}",
                    'avg_response_time': (
                        f"{sum(stats['response_times'])/len(stats['response_times']):.2f}s"
                        if stats.get('response_times') else "N/A"
                    ),
                    'total_requests': stats.get('total_requests', 0),
                    'quality_score': f"{stats.get('quality_score', 0):.2f}",
                    'health_score': f"{stats.get('health_score', 1.0):.2f}",
                    'consecutive_failures': stats.get('consecutive_failures', 0),
                    'last_used': stats.get('last_used')
                }
                for name, stats in self.performance_stats.items()
            },
            "key_rotation_system": key_stats,
            "available_providers": len(self.available),
            "total_configured_providers": len(self.providers),
            "local_fallback_available": self.local_fallback.ollama_available,
            "current_global_round": self.key_manager.global_key_round,
            "max_rounds_available": self.key_manager.max_keys_per_provider,
            "system_health": "Excellent" if self.global_stats['successful_requests'] / max(1, self.global_stats['total_requests']) > 0.9 else "Good"
        }

    def get_enterprise_status(self):
     """Enhanced enterprise status with global round-robin metrics"""
     total_requests = self.global_stats['total_requests']
     success_rate = (self.global_stats['successful_requests'] / max(1, total_requests)) * 100
    
     return {
        'system_info': {
            'name': 'NOVA Enhanced Production API Manager',
            'version': '4.1.0-global-round-robin',
            'status': 'Production Ready',
            'enterprise_features_loaded': len(self.available) > 0,
            'global_round_robin': True
        },
        'enterprise_features': {
            'global_key_rotation': '‚úÖ Active',
            'intelligent_model_routing': '‚úÖ Enhanced',
            'multi_provider_fallback': '‚úÖ Enabled',
            'performance_monitoring': '‚úÖ Real-time',
            'smart_enhancement_detection': '‚úÖ ML-Powered',
            'local_fallback': '‚úÖ Available' if self.local_fallback.ollama_available else '‚ùå Unavailable',
            'emergency_mode': '‚úÖ Ready',
            'provider_health_tracking': '‚úÖ Active',
            # ‚úÖ ADD MISSING KEYS:
            'multi_candidate_responses': '‚úÖ Enabled' if len(self.available) > 1 else '‚ùå Single Provider',
            'multi_key_rotation': '‚úÖ Active',
            'rate_limiting': '‚úÖ Active'
        },
        'performance_metrics': {
            'available_providers': len(self.available),
            'total_configured_providers': len(self.providers),
            'total_api_keys': sum(len(keys) for keys in self.key_manager.provider_keys.values()),
            'current_global_round': f"{self.key_manager.global_key_round}/{self.key_manager.max_keys_per_provider}",
            'average_quality_rating': sum(p.get('quality_rating', 8) for p in self.available) / len(self.available) if self.available else 8.0,
            'system_success_rate': f"{success_rate:.1f}%",
            'total_requests_processed': total_requests,
            'global_round_rotations': self.global_stats['global_round_rotations'],
            'key_rotation_events': self.global_stats['key_rotation_events'],
            # ‚úÖ ADD MISSING KEYS:
            'top_providers': [p['name'] for p in self.available[:3]]
        },
        'provider_status': [
            {
                'name': provider['name'],
                'specialty': provider.get('specialty', ['general']),
                'quality_rating': provider.get('quality_rating', 8),
                'speed_rating': provider.get('speed_rating', 7),
                'available_keys': len([k for k in self.key_manager.key_status.get(provider.get('env_key', ''), {}).values() 
                                     if not k.get('quota_exhausted', True)]),
                'health_score': self.performance_stats.get(provider['name'], {}).get('health_score', 1.0),
                'status': 'Active' if provider in self.available else 'Inactive'
            }
            for provider in self.providers[:8]  # Top 8 providers
        ]
     }

    # Additional utility methods for monitoring and debugging
    def force_global_round_advance(self):
        """Force advance global round - useful for testing and manual intervention"""
        old_round = self.key_manager.global_key_round
        self.key_manager.advance_global_round()
        self.global_stats['key_rotation_events'] += 1
        logger.info(f"üîÑ Manually advanced global round: {old_round} ‚Üí {self.key_manager.global_key_round}")
        
    def get_current_round_status(self) -> Dict[str, Any]:
        """Get detailed status of current global round"""
        return {
            'current_round': self.key_manager.global_key_round,
            'max_rounds': self.key_manager.max_keys_per_provider,
            'round_utilization': f"{(self.key_manager.global_key_round / self.key_manager.max_keys_per_provider) * 100:.1f}%",
            'keys_per_provider': {provider: len(keys) for provider, keys in self.key_manager.provider_keys.items()},
            'healthy_providers': len([p for p in self.available if self.performance_stats[p['name']]['health_score'] > 0.7]),
            'emergency_mode': self.key_manager.emergency_mode
        }
# ========== SMART ENHANCEMENT DETECTOR ==========
class SmartEnhancementDetector:
    """Intelligent detection of when to apply ML enhancement vs simple AI responses"""
    
    @staticmethod
    def needs_ml_enhancement(user_query: str) -> bool:
        """
        Determine if query needs advanced ML processing
        Returns True for complex queries, False for simple queries that still need AI but not ML
        """
        query_lower = user_query.lower().strip()
        
        # Complex queries that NEED ML enhancement
        complex_indicators = [
            # Technical queries
            'code', 'programming', 'algorithm', 'debug', 'error', 'function', 'api', 'database',
            'architecture', 'system design', 'scalability', 'performance', 'optimization',
            
            # Professional queries  
            'career', 'job', 'interview', 'resume', 'promotion', 'salary', 'skills', 'linkedin',
            'business', 'strategy', 'market', 'revenue', 'profit', 'analysis', 'growth',
            
            # Advanced requests
            'analyze', 'compare', 'recommend', 'suggest', 'implement', 'design',
            'create', 'build', 'develop', 'optimize', 'improve', 'review',
            
            # Medical/Health (complex)
            'symptoms', 'treatment', 'diagnosis', 'medicine', 'therapy',
            
            # Complex emotional/mental health
            'depression', 'anxiety', 'therapy', 'counseling', 'mental health',
            
            # File/Data analysis
            'file', 'document', 'data', 'report', 'spreadsheet', 'presentation',
            
            # Project/work related
            'project', 'assessment', 'guidance', 'help me with', 'assist me',
            'consultation', 'advice on', 'evaluate'
        ]
        
        # Multi-word complex patterns
        complex_patterns = [
            r'help me (with|in|on)',
            r'can you (help|assist|guide)',
            r'i (need|want|would like) (help|assistance|guidance)',
            r'what (should|would|could) i do',
            r'how (can|should|do) i',
            r'please (help|assist|guide|advise)',
            r'give me (advice|guidance|help)',
            r'i am (struggling|having trouble|confused)',
            r'explain (how|why|what|when)',
            r'tell me about'
        ]
        
        # Check for complex indicators
        has_complex_terms = any(term in query_lower for term in complex_indicators)
        has_complex_patterns = any(re.search(pattern, query_lower) for pattern in complex_patterns)
        is_long_query = len(query_lower.split()) > 15
        
        return has_complex_terms or has_complex_patterns or is_long_query
    
    @staticmethod
    def is_simple_greeting(user_query: str) -> bool:
        """Check if it's a very simple greeting that needs basic AI response"""
        query_lower = user_query.lower().strip()
        
        simple_patterns = [
            "r'^(hi|hello|hey|hola)",
            "r'^(hi there|hello there|hey there)",
            "r'^(good morning|good afternoon|good evening)",
            "r'^(how are you|how\'s it going|what\'s up|sup)",
            "r'^(thanks|thank you|thx|ty)",
            "r'^(bye|goodbye|see you|talk later|cya)",
            "r'^(yes|no|ok|okay|sure|alright)",
            "r'^(what is your name|who are you)",
            "r'^(help|test|testing)",
        ]
        
        return any(re.match(pattern, query_lower) for pattern in simple_patterns)
    
class IntelligentQueryClassifier:
    """ADVANCED Query Classification using SpaCy NLP - NO KEYWORDS!"""
    
    def __init__(self):
        # Initialize SpaCy model (lightweight)
        try:
            import spacy
            self.nlp = spacy.load("en_core_web_sm")
            self.spacy_available = True
            print("üß† SpaCy NLP model loaded for intelligent query classification")
        except:
            self.spacy_available = False
            print("‚ö†Ô∏è SpaCy not available, using fallback classification")
    
    def classify_query_complexity(self, user_input: str) -> Dict[str, Any]:
        """SMART Query Classification using NLP Analysis"""
        
        if not self.spacy_available:
            return self._fallback_classification(user_input)
        
        # SpaCy NLP Analysis
        doc = self.nlp(user_input)
        
        # Extract linguistic features
        word_count = len(doc)
        sent_count = len(list(doc.sents))
        entities = len(doc.ents)
        verb_count = sum(1 for token in doc if token.pos_ == "VERB")
        noun_count = sum(1 for token in doc if token.pos_ == "NOUN")
        adj_count = sum(1 for token in doc if token.pos_ == "ADJ")
        
        # Complexity scoring based on linguistic features
        complexity_score = 0
        
        # Length-based scoring
        if word_count > 20: complexity_score += 3
        elif word_count > 10: complexity_score += 1
        
        # Structure-based scoring  
        if sent_count > 2: complexity_score += 2
        if entities > 2: complexity_score += 2
        
        # Linguistic complexity
        if verb_count > 3: complexity_score += 1
        if adj_count > 3: complexity_score += 1
        if noun_count > 5: complexity_score += 1
        
        # Dependency analysis - complex syntax
        complex_deps = sum(1 for token in doc if token.dep_ in ["ccomp", "xcomp", "advcl", "acl"])
        if complex_deps > 0: complexity_score += 2
        
        # Named entity recognition - technical/business content
        entity_types = [ent.label_ for ent in doc.ents]
        if any(label in entity_types for label in ["ORG", "PRODUCT", "MONEY", "PERCENT"]):
            complexity_score += 2
        
        # Classification based on NLP score
        if complexity_score <= 2:
            return {
                'complexity': 'simple',
                'processing_type': 'fast',
                'estimated_time': '< 1 second',
                'use_heavy_features': False,
                'skip_ml_enhancement': True,
                'skip_intelligent_routing': True,
                'nlp_score': complexity_score,
                'linguistic_features': {
                    'words': word_count,
                    'sentences': sent_count,
                    'entities': entities,
                    'verbs': verb_count,
                    'nouns': noun_count
                }
            }
        elif complexity_score <= 5:
            return {
                'complexity': 'medium',
                'processing_type': 'standard',
                'estimated_time': '1-2 seconds',
                'use_heavy_features': True,
                'skip_ml_enhancement': True,
                'skip_intelligent_routing': False,
                'nlp_score': complexity_score,
                'linguistic_features': {
                    'words': word_count,
                    'sentences': sent_count,
                    'entities': entities,
                    'verbs': verb_count,
                    'nouns': noun_count
                }
            }
        else:
            return {
                'complexity': 'complex',
                'processing_type': 'detailed',
                'estimated_time': '3-5 seconds',
                'use_heavy_features': True,
                'skip_ml_enhancement': False,
                'skip_intelligent_routing': False,
                'nlp_score': complexity_score,
                'linguistic_features': {
                    'words': word_count,
                    'sentences': sent_count,
                    'entities': entities,
                    'verbs': verb_count,
                    'nouns': noun_count
                }
            }
    
    def _fallback_classification(self, user_input: str) -> Dict[str, Any]:
        """Fallback when SpaCy not available"""
        word_count = len(user_input.split())
        
        if word_count <= 5:
            return {
                'complexity': 'simple',
                'processing_type': 'fast',
                'estimated_time': '< 1 second',
                'use_heavy_features': False,
                'skip_ml_enhancement': True,
                'skip_intelligent_routing': True,
                'fallback_mode': True
            }
        elif word_count <= 15:
            return {
                'complexity': 'medium',
                'processing_type': 'standard', 
                'estimated_time': '1-2 seconds',
                'use_heavy_features': False,
                'skip_ml_enhancement': True,
                'skip_intelligent_routing': True,
                'fallback_mode': True
            }
        else:
            return {
                'complexity': 'complex',
                'processing_type': 'detailed',
                'estimated_time': '3-5 seconds',
                'use_heavy_features': True,
                'skip_ml_enhancement': False,
                'skip_intelligent_routing': False,
                'fallback_mode': True
            }


class NovaUltraSystem:
    """ENTERPRISE-GRADE NOVA system with all enhanced functionality + Enterprise API Management"""
    
    def __init__(self):
        # Core systems (enterprise optimized)
        self.memory = UltraHybridMemorySystem()
        self.contextual_memory = AdvancedContextualMemorySystem(self.memory)
        # self.language_detector = FixedLanguageDetector()
        self.emotion_detector = FastEmotionDetector()
        self.api_manager = EnhancedProductionAPIManager()
        self.rate_limiter = RateLimitManager()
        self.model_router = IntelligentModelRouter() 
        self.query_classifier = IntelligentQueryClassifier()

        self.current_sessions = defaultdict(lambda: {
            'last_activity': time.time(),
            'conversation_count': 0,
            'api_usage': defaultdict(int),
            'query_history': deque(maxlen=50)
        })
        
        # Optional: Add this too
        self.query_processor = None 


        if GITHUB_INTEGRATION:
            self.github_analyzer = GitHubAnalyzerBridge()
            self.qa_engine = None
            self._initialize_qa_engine()
            print("‚úÖ GitHub analyzer bridge initialized")
        else:
            self.github_analyzer = None
            print("‚ö†Ô∏è GitHub analyzer not available")
        
        # üè¢ ENTERPRISE API MANAGER (UPGRADED FROM OptimizedAPIManager)
        try:
            self.api_manager = EnhancedProductionAPIManager()
            print("‚úÖ API Manager initialized with multi-candidate system")
        except ImportError as e:
            print(f"‚ö†Ô∏è API Manager not available, using fallback: {e}")
            # Fallback to basic manager if enterprise not available
            try:
                self.api_manager = OptimizedAPIManager()
                print("‚ö†Ô∏è Using OptimizedAPIManager as fallback")
            except:
                self.api_manager = None
                print("‚ùå No API manager available")
        
        self.voice_system = FixedVoiceSystem()
        self.plugin_manager = PluginManager()
        self.web_search = FastWebSearch()
        self.file_system = FileUploadSystem()  # File upload system
        self.export_system = SmartExportSystem(self.file_system)
        self.sound_system = SoundManager()  # RESTORED Sound system
        self.ultimate_emoji_system = CompactUltimateEmojiSystem()
        # ML System (if available)
        self.ml_manager = None
        if ML_SYSTEM_AVAILABLE:
            try:
                self.ml_manager = EnhancedMLManager()
                print("‚úÖ Advanced ML Manager loaded")
            except Exception as e:
                print(f"ML system init error: {e}")
        
        # Professional agents
        self.agents = {}
        if PROFESSIONAL_AGENTS_LOADED:
            try:
                self.agents = {
                    'coding': ProLevelCodingExpert(),
                    'career': ProfessionalCareerCoach(),
                    'business': SmartBusinessConsultant(),
                    'medical': SimpleMedicalAdvisor(),
                    'emotional': SimpleEmotionalCounselor(),
                    'technical_architect': TechnicalArchitect()
                }
                print(f"‚úÖ {len(self.agents)} professional agents loaded")
            except Exception as e:
                print(f"Agent loading error: {e}")
        
        # Session management
        self.current_session = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.user_id = "nova_user"
        self.conversation_count = 0
        
        # File analysis context
        self.current_file_context = None
        self.active_github_repo = None
        self.active_github_repo_data = {}
        self.repo_context_memory = {
        'last_analyzed_repo': None,
        'repo_files_detected': [],
        'repo_languages': [],
        'repo_analysis_summary': None
       }
        
        # Enterprise features tracking
        self.enterprise_mode = hasattr(self.api_manager, 'get_enterprise_status')
        
        # Agent patterns (optimized + enhanced)
        self.agent_patterns = {
            "coding": {
                "keywords": ["code", "programming", "debug", "python", "javascript", "bug", "development", "algorithm", "function", "class", "method", "api", "database", "sql", "framework", "library"],
                "system_prompt": "You are NOVA Coding Expert. Provide practical, production-ready code solutions with best practices, security considerations, and performance optimization.",
                "complexity_boost": True  # For multi-candidate detection
            },
            "career": {
                "keywords": ["resume", "interview", "job", "career", "hiring", "professional", "salary", "promotion", "skills", "experience", "linkedin", "networking"],
                "system_prompt": "You are NOVA Career Coach. Provide expert career guidance, professional advice, and strategic career planning.",
                "complexity_boost": False
            },
            "business": {
                "keywords": ["business", "analysis", "strategy", "market", "revenue", "growth", "startup", "investment", "roi", "kpi", "profit", "competitor", "customer"],
                "system_prompt": "You are NOVA Business Consultant. Provide strategic business insights, market analysis, and comprehensive business strategy.",
                "complexity_boost": True  # For multi-candidate detection
            },
            "medical": {
                "keywords": ["health", "medical", "symptoms", "doctor", "treatment", "medicine", "diagnosis", "therapy", "wellness", "fitness", "nutrition"],
                "system_prompt": "You are Dr. NOVA. Provide medical insights while emphasizing professional consultation and evidence-based information.",
                "complexity_boost": False
            },
            "emotional": {
                "keywords": ["stress", "anxiety", "sad", "emotional", "support", "therapy", "depression", "mental", "counseling", "mood", "feelings"],
                "system_prompt": "You are Dr. NOVA Counselor. Provide empathetic emotional support, mental health guidance, and therapeutic insights.",
                "complexity_boost": False
            },
            "technical_architect": {
                "keywords": ["architecture", "system design", "scalability", "microservice", "infrastructure", "cloud", "deployment", "devops", "security", "performance"],
                "system_prompt": "You are NOVA Technical Architect. Provide comprehensive system design guidance, scalability solutions, and enterprise architecture.",
                "complexity_boost": True  # For multi-candidate detection
            },
            "reasoning": {
                "keywords": ["analyze", "explain", "reasoning", "logic", "complex", "detailed", "comprehensive", "step by step", "thorough"],
                "system_prompt": "You are NOVA Reasoning Expert. Provide detailed analysis, logical reasoning, and comprehensive explanations.",
                "complexity_boost": True
            }
        }
        
        print(f"üöÄ NOVA Ultra System initialized - Advanced Mode: {self.enterprise_mode}")

    def _initialize_qa_engine(self):
        """Initialize QA engine for repository Q&A"""
        try:
            from qa_engine import UltimateQAEngine
            self.qa_engine = UltimateQAEngine(
                enable_memory=True,
                enable_tools=True,
                enable_smart_routing=True,
                max_tokens=1000,
                temperature=0.7
            )
            print("‚úÖ QA Engine initialized in CLI")
            return True
        except Exception as e:
            print(f"‚ùå QA Engine initialization failed: {e}")
            self.qa_engine = None
            return False
    
    def is_enterprise_enabled(self) -> bool:
        """Check if enterprise features are available"""
        return self.enterprise_mode and hasattr(self.api_manager, 'get_enterprise_status')
    
    def needs_multi_candidate_response(self, user_input: str, agent_type: str) -> bool:
     """Determine if query needs multi-candidate processing - DISABLED FOR STABILITY"""
    
     # ‚úÖ MULTI-CANDIDATE COMPLETELY DISABLED
     print(f"üîç MULTI-CANDIDATE: DISABLED - Using single response mode for: '{user_input[:50]}...'")
     return False
    
     # üö´ ALL OLD LOGIC COMMENTED OUT FOR SAFETY:
     # if not self.is_enterprise_enabled():
     #     return False
     # 
     # emotional_words = ["sad", "happy", "angry", "anxious", "frustrated", "excited", "depressed", "worried", "nervous"]
     # if any(word in user_input.lower() for word in emotional_words):
     #     print(f"üîç MULTI-CANDIDATE DEBUG: Skipping for emotional content: '{user_input[:50]}...'")
     #     return False
     # 
     # try:
     #     from new_claude import SmartEnhancementDetector
     #     print(f"üîç MULTI-CANDIDATE DEBUG: Checking SmartEnhancementDetector for: '{user_input[:50]}...'")
     #     if SmartEnhancementDetector.needs_ml_enhancement(user_input):
     #         print(f"üîç MULTI-CANDIDATE DEBUG: SmartEnhancementDetector says TRUE")
     #         return True
     #     else:
     #         print(f"üîç MULTI-CANDIDATE DEBUG: SmartEnhancementDetector says FALSE")
     # except ImportError as e:
     #     print(f"üîç MULTI-CANDIDATE DEBUG: ImportError - {e}")
     #     pass
     # except Exception as e:
     #     print(f"üîç MULTI-CANDIDATE DEBUG: Exception in SmartEnhancementDetector - {e}")
     #     return False
     # 
     # complexity_indicators = [
     #     len(user_input.split()) > 15,
     #     any(word in user_input.lower() for word in [
     #         "complex", "comprehensive", "detailed", "analysis", "strategy", 
     #         "architecture", "design", "explain", "compare", "evaluate"
     #     ]),
     #     self.agent_patterns.get(agent_type, {}).get("complexity_boost", False)
     # ]
     # 
     # result = sum(complexity_indicators) >= 2
     # print(f"üîç MULTI-CANDIDATE DEBUG: Fallback logic result: {result} (indicators: {sum(complexity_indicators)}/3)")
     # return result
    
    async def detect_agent_type(self, user_input: str) -> Tuple[str, float]:
        """Enhanced agent detection with reasoning support"""
        text_lower = user_input.lower()
        
        # Check for reasoning keywords first (highest priority for complex queries)
        reasoning_score = 0
        reasoning_keywords = self.agent_patterns["reasoning"]["keywords"]
        for keyword in reasoning_keywords:
            if keyword in text_lower:
                reasoning_score += 1
        
        if reasoning_score >= 2:  # Multiple reasoning indicators
            return "reasoning", 0.9
        
        # Check other agents
        best_match = ("general", 0.0)
        for agent_name, agent_data in self.agent_patterns.items():
            if agent_name == "reasoning":  # Already checked
                continue
                
            keywords = agent_data["keywords"]
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            
            if matches > 0:
                confidence = min(0.8 + (matches * 0.1), 1.0)  # Max confidence 1.0
                if confidence > best_match[3]:
                    best_match = (agent_name, confidence)
        
        return best_match
    
    async def create_system_prompt(self, agent_type: str, language: str, emotion: str, file_context: str = None, multi_candidate: bool = False) -> str:
        """Create enhanced system prompt with enterprise features"""
        if multi_candidate:
            base_prompt = """You are NOVA AI, utilizing multi-candidate response generation for optimal quality. 
You are an advanced AI assistant with enterprise-grade intelligence across all domains.
Provide exceptionally detailed, professional, actionable, and empathetic responses. Be comprehensive yet well-structured."""
        else:
            base_prompt = """You are NOVA Professional AI, an advanced assistant with expertise across all domains.
Provide professional, actionable, and empathetic responses. Be concise yet comprehensive."""
        
        agent_prompt = self.agent_patterns.get(agent_type, {}).get("system_prompt", "")
        
        language_note = ""
        if language == "hinglish":
            language_note = " Respond naturally mixing English and Hindi as appropriate for the user's comfort."
        
        emotion_note = ""
        emotion_prompts = {
            "sad": " The user seems sad, so be extra supportive, empathetic, and encouraging.",
            "anxious": " The user appears anxious, so provide calm, reassuring, and structured guidance.",
            "frustrated": " The user seems frustrated, so be patient, understanding, and focus on solutions.",
            "excited": " The user appears excited, so match their energy while providing thorough information.",
            "confused": " The user seems confused, so provide clear, step-by-step explanations."
        }
        emotion_note = emotion_prompts.get(emotion, "")
        
        file_context_note = ""
        if file_context:
            file_context_note = f"\n\nFILE CONTEXT: The user has uploaded/analyzed this file:\n{file_context}\n\nUse this context to provide more relevant and specific answers. Reference the file content when appropriate."
        
        enterprise_note = ""
        if multi_candidate:
            enterprise_note = "\n\nADVANCED MODE: This response is being generated using multi-candidate processing for enhanced quality and accuracy."
        
        return f"{base_prompt}\n{agent_prompt}{language_note}{emotion_note}{file_context_note}{enterprise_note}"
    
    async def upload_and_analyze_file(self) -> Dict[str, Any]:
        """Enhanced file upload and analysis with better error handling"""
        try:
            # Select file using GUI
            file_path = self.file_system.select_file()
            if not file_path:
                return {"error": "No file selected"}
            
            # Analyze file
            file_analysis = self.file_system.analyze_file(file_path)
            if file_analysis.get("error"):
                return file_analysis
            
            # Store in memory system - ENHANCED
            try:
                self.memory.remember_file_processing(
                    self.user_id,
                    file_analysis['file_path'],
                    file_analysis['file_type'],
                    f"File analyzed: {file_analysis['file_name']} - {file_analysis.get('lines', 0)} lines",
                    True
                )
            except Exception as memory_error:
                print(f"Memory storage warning: {memory_error}")
            
            # Set current file context with enhanced information
            self.current_file_context = f"""
File: {file_analysis['file_name']}
Type: {file_analysis['file_type']}
Size: {file_analysis['file_size']} bytes
Lines: {file_analysis.get('lines', 'N/A')}
Words: {file_analysis.get('words', 'N/A')}
Characters: {file_analysis.get('chars', 'N/A')}
Content preview: {file_analysis['content'][:800]}...
"""
            
            return {
                "success": True,
                "file_analysis": file_analysis,
                "message": f"Successfully analyzed {file_analysis['file_name']}",
                "context_set": True
            }
            
        except Exception as e:
            return {"error": f"File upload failed: {str(e)}"}
    
    async def get_response(self, user_input: str, user_id: str = "default", 
                      agent_type: str = "general", session_id: str = None) -> Dict[str, Any]:
     """PRODUCTION-READY AI response with SMART performance optimization"""
     start_time = time.time()
     session_id = session_id or f"session_{int(time.time())}"
    
     # ‚úÖ NEW: SMART QUERY CLASSIFICATION
     word_count = len(user_input.split())
     char_count = len(user_input)
    
     # Quick complexity detection
     is_simple_query = (
        word_count <= 10 and 
        char_count <= 80 and
        not any(keyword in user_input.lower() for keyword in [
            'analyze', 'detailed', 'comprehensive', 'explain in detail',
            'step by step', 'architecture', 'optimization', 'troubleshoot',
            'business strategy', 'technical specification', 'implementation'
        ])
      )
    
     is_hardcore_query = (
        word_count > 15 or 
        char_count > 150 or
        any(keyword in user_input.lower() for keyword in [
            'analyze', 'comprehensive analysis', 'detailed explanation',
            'architecture design', 'system optimization', 'business strategy',
            'technical implementation', 'debugging process', 'refactoring'
        ])
     )
    
     print(f"üéØ Smart Classification: Words={word_count}, Simple={is_simple_query}, Hardcore={is_hardcore_query}")
    
     # Initialize variables to avoid scope issues
     detected_type = agent_type
     routing_confidence = 0.0
     routing_metadata = {}
     ml_analysis = {}
    
     try:
        # Step 1: Rate limiting check (using copied class)
        rate_check = self.rate_limiter.check_rate_limit(user_id)
        if not rate_check['allowed']:
            return {
                'response': f"Rate limit exceeded. Please wait before making another request. {rate_check.get('reason', '')}",
                'agent_used': 'system',
                'language': 'english',
                'emotion': 'neutral',
                'emotion_confidence': 1.0,
                'agent_confidence': 1.0,
                'response_time': time.time() - start_time,
                'conversation_count': 0,
                'rate_limited': True,
                'rate_limit_info': rate_check,
                'session_id': session_id,
                'ml_enhanced': False,
                'query_complexity': 'simple' if is_simple_query else ('hardcore' if is_hardcore_query else 'medium')
            }
        
        # Step 2: SMART Enhancement Detection
        if is_simple_query:
            # FAST TRACK - Skip heavy ML detection
            needs_ml_enhancement = False
            is_simple_greeting = True
            print("‚ö° Fast Track: Skipping ML enhancement for simple query")
        elif is_hardcore_query:
            # HARDCORE TRACK - Full ML processing
            needs_ml_enhancement = SmartEnhancementDetector.needs_ml_enhancement(user_input)
            is_simple_greeting = SmartEnhancementDetector.is_simple_greeting(user_input)
            print("üß† Hardcore Track: Full ML enhancement applied")
        else:
            # MEDIUM TRACK - Basic enhancement only
            needs_ml_enhancement = False
            is_simple_greeting = False
            print("üîß Medium Track: Basic processing")
        
        print(f"üîç Query analysis - ML Enhancement: {needs_ml_enhancement}, Simple: {is_simple_greeting}")
        
        # Step 3: Get user session and update activity
        user_session = self.current_sessions[user_id]
        user_session['last_activity'] = time.time()
        
        if 'query_history' not in user_session:
            user_session['query_history'] = deque(maxlen=50)
        
        user_session['query_history'].append({
            'query': user_input,
            'timestamp': time.time(),
            'complexity': 'simple' if is_simple_query else ('hardcore' if is_hardcore_query else 'medium')
        })
        
        # Step 4: Safe detection (keeping your original reliable detection)
        language = "english"
        
        try:
            emotion, emotion_confidence = self.emotion_detector.detect_emotion(user_input)
        except Exception:
            emotion, emotion_confidence = "neutral", 0.5
        
        try:
            detected_agent_type, agent_confidence = await self.detect_agent_type(user_input)
        except Exception:
            detected_agent_type, agent_confidence = "general", 0.5
            
        print(f"üîç Processing: {agent_type} | {emotion} | File: {bool(self.current_file_context)}")
        
        # Step 5: CONDITIONAL Enhanced query processing
        enhanced_query = user_input
        if self.query_processor and needs_ml_enhancement and is_hardcore_query:
            try:
                enhanced_query, query_ml_analysis = await self.query_processor.process_query(
                    user_input,
                    context={
                        'user_id': user_id,
                        'session_history': list(user_session['query_history'])[-5:],
                        'language': language,
                        'emotion': emotion,
                        'requested_agent': agent_type
                    }
                )
                ml_analysis.update(query_ml_analysis)
                print("üß† Advanced query processing applied")
            except Exception as e:
                print(f"Query processor warning: {e}")
        
        # Step 6: CONDITIONAL Intelligent routing
        if is_simple_query:
            # FAST TRACK - Skip intelligent routing
            if agent_confidence > 0.7:
                agent_type = detected_agent_type
            print("‚ö° Fast Track: Simple agent detection used")
        else:
            # STANDARD/HARDCORE TRACK - Use intelligent routing
            try:
                detected_type, preferred_models, routing_confidence, routing_metadata = self.model_router.detect_query_type(
                    enhanced_query,
                    context={
                        'user_id': user_id,
                        'language': language,
                        'emotion': emotion,
                        'has_images': bool(self.current_file_context and 'image' in str(self.current_file_context).lower()),
                        'document_length': len(enhanced_query.split()),
                        'session_history': len(user_session.get('query_history', [])),
                        'requires_long_context': len(enhanced_query.split()) > 100,
                        'requires_multimodal': any(term in enhanced_query.lower() for term in ['image', 'picture', 'video']),
                        'requires_function_calling': any(term in enhanced_query.lower() for term in ['code', 'calculate', 'search', 'analyze'])
                    }
                )
                
                if routing_confidence > 0.6:
                    agent_type = detected_type
                    print(f"üß† Intelligent routing applied: {agent_type} (Confidence: {routing_confidence:.2f})")
                    
            except Exception as e:
                print(f"Routing warning: {e}")
                if agent_confidence > 0.7:
                    agent_type = detected_agent_type
        
        # Step 7: System prompt creation (keeping your reliable method)
        try:
            system_prompt = await self.create_system_prompt(
                agent_type, language, emotion, self.current_file_context, False
            )
        except Exception:
            system_prompt = f"You are a helpful {agent_type} assistant."
        
        # Step 8: Smart input preparation (keeping your proven logic)
        enhanced_input = enhanced_query
        
        # Handle file context properly (your existing logic)
        if self.current_file_context:
            enhanced_input = f"""File Context Available:
{self.current_file_context}

User Question: {enhanced_query}"""
            print(f"üîç Using file context: {len(self.current_file_context)} chars")
        
        # Handle smart context (your existing safe logic)
        try:
            smart_context = self.contextual_memory.get_smart_context(user_id, user_input)
            if smart_context and not self.current_file_context:
                enhanced_input = f"{smart_context}\n\nUser: {enhanced_query}"
        except Exception as context_error:
            print(f"Context retrieval warning: {context_error}")
        
        # Step 9: API response (keeping your proven enterprise logic)
        ai_response = None
        enterprise_features_used = False
        available_providers = 13
        
        if self.api_manager:
            try:
                if self.is_enterprise_enabled():
                    print("üß† Using enterprise processing")
                    ai_response = await self.api_manager.get_ai_response(
                        enhanced_input,
                        system_prompt,
                        agent_type
                    )
                    enterprise_features_used = True
                    
                    try:
                        enterprise_status = self.api_manager.get_enterprise_status()
                        available_providers = enterprise_status.get('performance_metrics', {}).get('available_providers', 13)
                    except Exception:
                        available_providers = 13
                else:
                    print("üîß Using standard processing")
                    ai_response = await self.api_manager.get_ai_response(
                        enhanced_input,
                        system_prompt,
                        agent_type
                    )
                    
            except Exception as api_error:
                print(f"API Manager error: {api_error}")
                ai_response = None
        
        # Step 10: Intelligent fallback (keeping your proven system)
        if not ai_response or len(ai_response.strip()) < 10:
            print("üîÑ Using intelligent fallback")
            
            if self.current_file_context:
                ai_response = f"I can see you've uploaded a file and want to analyze it. While I'm having some technical difficulties with the advanced processing system, I can see the file content is available. Please try asking a more specific question about the file content, and I'll do my best to provide analysis based on what I can see."
            else:
                fallback_responses = {
                    "coding": "I'm experiencing technical difficulties with the coding system. Please try rephrasing your programming question, and I'll help with code analysis.",
                    "business": "The business analysis system is temporarily unavailable. Please rephrase your business question, and I'll provide strategic insights.",
                    "career": "I'm having connectivity issues with career guidance. Please restate your career question, and I'll offer professional advice.",
                    "medical": "I'm experiencing technical difficulties. For medical questions, please consult healthcare professionals and try rephrasing your question.",
                    "emotional": "I'm here to support you. Though I'm having technical difficulties, please share your concerns again, and I'll provide guidance.",
                    "technical_architect": "System architecture analysis is temporarily unavailable. Please rephrase your technical question."
                }
                
                ai_response = fallback_responses.get(
                    agent_type, 
                    "I'm experiencing technical difficulties. Please try rephrasing your question, and I'll provide the best assistance possible."
                )
        
        # Step 11: Response time and memory (keeping your safe logic)
        response_time = time.time() - start_time
        
        try:
            memory_data = {
                "agent_type": agent_type,
                "language": language,
                "emotion": emotion,
                "confidence": emotion_confidence,
                "response_time": response_time,
                "file_analyzed": self.current_file_context is not None,
                "enterprise_features_used": enterprise_features_used,
                "available_providers": available_providers,
                "routing_confidence": routing_confidence,
                "ml_enhanced": needs_ml_enhancement
            }
            
            await self.contextual_memory.remember_with_context(
                user_id, session_id, user_input, ai_response, memory_data
            )
        except Exception as memory_error:
            print(f"Memory storage warning: {memory_error}")
        
        # Step 12: Update session counters
        try:
            user_session['conversation_count'] += 1
            self.conversation_count = user_session['conversation_count']
        except Exception:
            self.conversation_count = getattr(self, 'conversation_count', 0) + 1
            user_session['conversation_count'] = self.conversation_count
        
        # Step 13: Return comprehensive response (new_claude.py compatible)
        return {
            "response": ai_response,
            "agent_used": agent_type,
            "language": language,
            "emotion": emotion,
            "emotion_confidence": emotion_confidence,
            "agent_confidence": agent_confidence,
            "response_time": response_time,
            "conversation_count": self.conversation_count,
            "ml_enhanced": needs_ml_enhancement,
            "file_context_used": self.current_file_context is not None,
            "enterprise_features_used": enterprise_features_used,
            "available_providers": available_providers,
            "user_id": user_id,
            "session_id": session_id,
            "context_used": bool(self.current_file_context),
            "recommendations": ml_analysis.get('recommendations', [])[:3] if needs_ml_enhancement else [],
            "enhancement_reason": f"{'Complex query - ML enhancement applied' if needs_ml_enhancement else 'Simple query - optimized response'}",
            "intelligent_routing_applied": routing_confidence > 0.6,
            "detected_query_type": detected_type,
            "routing_confidence": routing_confidence,
            "processing_complexity": routing_metadata.get('processing_complexity', 'medium'),
            "detected_patterns": routing_metadata.get('detected_patterns', [])[:5],
            "production_optimized": True,
            "query_complexity": 'simple' if is_simple_query else ('hardcore' if is_hardcore_query else 'medium'),
            "performance_mode": 'fast_track' if is_simple_query else ('full_processing' if is_hardcore_query else 'standard'),
            "words_count": word_count,
            "optimization_applied": is_simple_query or is_hardcore_query
        }
        
     except Exception as e:
        print(f"Critical response error: {e}")
        
        # Enhanced fallback (keeping your proven logic)
        try:
            emergency_response = await self.api_manager.get_ai_response(
                user_input, 
                "You are NOVA, a professional AI assistant. Provide helpful, accurate responses.",
                "general"
            ) if self.api_manager else None
            
            if not emergency_response:
                emergency_response = "I apologize for the technical difficulty. Our systems are working to resolve this. Please try rephrasing your question."
            
            return {
                "response": emergency_response,
                "agent_used": "emergency",
                "language": "english",
                "emotion": "neutral",
                "emotion_confidence": 0.7,
                "agent_confidence": 0.7,
                "response_time": time.time() - start_time,
                "conversation_count": getattr(self, 'conversation_count', 0),
                "error": str(e),
                "enterprise_features_used": False,
                "available_providers": 0,
                "file_context_used": False,
                "ml_enhanced": False,
                "user_id": user_id,
                "session_id": session_id,
                "emergency_fallback": True,
                "query_complexity": 'unknown',
                "performance_mode": 'emergency'
            }
        except:
            return {
                "response": "I apologize for the technical difficulty. Our systems are working to resolve this. Please try rephrasing your question.",
                "agent_used": "fallback",
                "language": "english",
                "emotion": "neutral",
                "response_time": time.time() - start_time,
                "conversation_count": 1,
                "error": str(e),
                "enterprise_features_used": False,
                "available_providers": 0,
                "file_context_used": False,
                "ml_enhanced": False,
                "user_id": user_id,
                "session_id": session_id,
                "critical_fallback": True,
                "query_complexity": 'unknown',
                "performance_mode": 'critical_fallback'
            }
    
    async def process_voice_input(self) -> Optional[Dict[str, Any]]:
     """üé§ COMPLETELY FIXED voice processing - SINGLE PIPELINE"""
     try:
        print("\nüé§ === VOICE MODE ACTIVATED ===")
        print("üó£Ô∏è NOVA is ready to listen...")
        
        # ‚úÖ FIXED: Proper voice recognition
        voice_text = await self.voice_system.listen()
        
        if not voice_text or voice_text == "timeout":
            return {"error": "No voice input detected. Please speak clearly."}
        
        print(f"‚úÖ Voice input: '{voice_text}'")
        
        # Get AI response
        response_data = await self.get_response(voice_text)
        
        # ‚úÖ CRITICAL FIX: DO NOT SPEAK HERE - Let UI handle TTS
        # This prevents duplicate/external output
        
        return {
            "success": True,
            "voice_input": voice_text,
            "ai_response": response_data,
            "tts_needed": True  # ‚úÖ NEW: Flag to indicate TTS needed
        }
        
     except Exception as e:
        return {"error": f"Voice processing failed: {e}"}
    
    async def search_web(self, query: str) -> Dict[str, Any]:
        """Enhanced web search functionality"""
        try:
            print(f"Searching web for: {query}")
            search_results = await self.web_search.search_web(query, max_results=6)
            
            if search_results.get("success"):
                results = search_results.get("results", [])
                formatted_response = f"üîç **Web Search Results for: {query}**\n\n"
                
                if results:
                    for i, result in enumerate(results, 1):
                        formatted_response += f"**{i}. {result['title']}**\n"
                        formatted_response += f"üìÑ Source: {result['source']}\n"
                        if result.get('snippet'):
                            formatted_response += f"üìù {result['snippet']}\n"
                        if result.get('url'):
                            formatted_response += f"üîó {result['url']}\n"
                        formatted_response += "\n"
                    
                    formatted_response += f"üìä Found {len(results)} results for your query."
                else:
                    formatted_response += "No specific results found, but the search was processed successfully."
                
                return {
                    "success": True, 
                    "formatted_response": formatted_response,
                    "results_count": len(results)
                }
            else:
                return {"error": search_results.get("error", "Web search failed")}
                
        except Exception as e:
            return {"error": f"Web search error: {e}"}
    
    def get_system_status(self) -> Dict[str, Any]:
     """Enhanced system status with enterprise metrics + PLUGIN INTEGRATION"""
     # Get enterprise status if available
     enterprise_info = {}
     if self.is_enterprise_enabled():
        try:
            enterprise_status = self.api_manager.get_enterprise_status()
            enterprise_info = {
                "enterprise_enabled": True,
                "available_providers": enterprise_status['performance_metrics']['available_providers'],
                "total_api_keys": enterprise_status['performance_metrics']['total_api_keys'],
                "quality_rating": enterprise_status['performance_metrics']['average_quality_rating']
            }
        except Exception as e:
            enterprise_info = {"enterprise_enabled": True, "status_error": str(e)}
     else:
        enterprise_info = {"enterprise_enabled": False}
    
     # üöÄ NEW: Add comprehensive plugin information
     plugin_status = {}
     plugin_details = []
     if self.plugin_manager:
        try:
            available_plugins = self.plugin_manager.get_plugin_list()
            available_commands = self.plugin_manager.get_available_commands()
            
            plugin_status = {
                "total_plugins": len(available_plugins),
                "enabled_plugins": len([p for p in available_plugins if p.get('enabled', True)]),
                "disabled_plugins": len([p for p in available_plugins if not p.get('enabled', True)]),
                "available_commands": len(available_commands),
                "enterprise_plugins": len([p for p in available_plugins if p.get('type') == 'enterprise']),
                "builtin_plugins": len([p for p in available_plugins if p.get('type') == 'builtin'])
            }
            
            # Detailed plugin information for advanced status
            for plugin in available_plugins:
                plugin_details.append({
                    "name": plugin.get('name', 'Unknown'),
                    "version": plugin.get('version', '1.0'),
                    "type": plugin.get('type', 'unknown'),
                    "commands": len(plugin.get('commands', [])),
                    "status": "‚úÖ Active" if plugin.get('enabled', True) else "‚ùå Disabled"
                })
                
        except Exception as e:
            plugin_status = {"error": f"Plugin system error: {str(e)}"}
     else:
        plugin_status = {"status": "Plugin system not initialized"}
    
     return {
        "core_systems": {
            "memory": "‚úÖ Active",
            "language_detection": "‚úÖ Active", 
            "emotion_detection": "‚úÖ Active",
            "api_manager": "‚úÖ Advanced" if self.is_enterprise_enabled() else ("‚úÖ Active" if self.api_manager else "‚ùå No API"),
            "file_system": "‚úÖ Active",
            "sound_system": "‚úÖ Active",
            # üöÄ NEW: Plugin system status
            "plugin_system": "‚úÖ Active" if self.plugin_manager else "‚ùå Disabled"
        },
        "premium_systems": {
            "azure_voice": "‚úÖ Active" if self.voice_system.azure_enabled else "‚ö†Ô∏è Basic Only",
            "web_search": "‚úÖ Active",
            "ml_system": "‚úÖ Active" if self.ml_manager else "‚ùå Disabled",
            # üöÄ NEW: Plugin metrics
            "plugins": plugin_status
        },
        "enterprise_systems": enterprise_info,
        "agents": {
            agent_name: "‚úÖ Active" if agent_name in self.agents else "‚ùå Disabled"
            for agent_name in ["coding", "career", "business", "medical", "emotional", "technical_architect"]
        },
        # üöÄ NEW: Plugin details section
        "plugin_ecosystem": {
            "overview": plugin_status,
            "installed_plugins": plugin_details,
            "command_coverage": {
                "weather": len([cmd for cmd in self.plugin_manager.get_available_commands() if cmd in ["weather", "forecast", "alerts"]]) if self.plugin_manager else 0,
                "calculator": len([cmd for cmd in self.plugin_manager.get_available_commands() if cmd in ["calc", "calculate", "emi", "convert", "currency"]]) if self.plugin_manager else 0,
                "timer": len([cmd for cmd in self.plugin_manager.get_available_commands() if cmd in ["timer", "remind", "pomodoro"]]) if self.plugin_manager else 0,
                "notes": len([cmd for cmd in self.plugin_manager.get_available_commands() if cmd in ["note", "notes", "search_notes", "categories"]]) if self.plugin_manager else 0,
                "enterprise": len([cmd for cmd in self.plugin_manager.get_available_commands() if cmd in ["code", "debug", "password", "security", "api", "github", "slack"]]) if self.plugin_manager else 0
            }
        },
        "session_info": {
            "session_id": self.current_session,
            "conversation_count": self.conversation_count,
            "user_id": self.user_id,
            "available_providers": len(getattr(self.api_manager, 'available', [])),
            "file_context_active": self.current_file_context is not None,
            "enterprise_mode": self.enterprise_mode,
            # üöÄ NEW: Plugin session info
            "plugin_commands_available": len(self.plugin_manager.get_available_commands()) if self.plugin_manager else 0
        }
     }

    def get_enterprise_metrics(self) -> Dict[str, Any]:
        """Get detailed enterprise metrics"""
        if not self.is_enterprise_enabled():
            return {"error": "Enterprise features not available"}
        
        try:
            return self.api_manager.get_enterprise_status()
        except Exception as e:
            return {"error": f"Failed to get enterprise metrics: {e}"}
    
    def clear_file_context(self):
        """Clear current file context"""
        self.current_file_context = None
        print("File context cleared")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get enhanced conversation summary"""
        return {
            "session_id": self.current_session,
            "user_id": self.user_id,
            "conversation_count": self.conversation_count,
            "enterprise_enabled": self.enterprise_mode,
            "file_context_active": self.current_file_context is not None,
            "available_agents": len(self.agents),
            "ml_system_active": self.ml_manager is not None,
            "voice_system_level": "Azure" if self.voice_system.azure_enabled else "Basic"
        }
    def get_enhanced_repo_stats(self) -> Dict[str, Any]:
     """Get enhanced repository statistics including file count"""
     if not self.github_analyzer.has_active_repo():
        return {"error": "No active repository"}
    
     try:
        # Get file mappings count
        file_count = len(self.github_analyzer.file_mappings) if hasattr(self.github_analyzer, 'file_mappings') else 0
        
        # Get repository info
        repo_info = self.github_analyzer.get_repository_info() if hasattr(self.github_analyzer, 'get_repository_info') else {}
        
        return {
            "total_files": file_count,
            "repository_name": repo_info.get('full_name', 'Unknown'),
            "repository_url": self.active_github_repo,
            "analysis_status": "‚úÖ Active",
            "file_extensions": self._get_file_extensions(),
            "last_analyzed": datetime.now().isoformat()
        }
     except Exception as e:
        return {"error": f"Failed to get repository stats: {e}"}

    def _get_file_extensions(self) -> List[str]:
     """Get unique file extensions in repository"""
     if not hasattr(self.github_analyzer, 'file_mappings'):
        return []
    
     extensions = set()
     for filename in self.github_analyzer.file_mappings.keys():
        ext = os.path.splitext(filename)[1]
        if ext:
            extensions.add(ext)
    
     return sorted(list(extensions))

    async def handle_repo_specific_query(self, user_input: str, conversation) -> bool:
     """Handle specific repository queries like file count"""
     try:
        if self.is_repo_file_count_query(user_input):
            conversation.write("[blue]üìä Analyzing repository file structure...[/blue]")
            
            repo_stats = self.get_enhanced_repo_stats()
            
            if "error" in repo_stats:
                conversation.write(f"[red]‚ùå {repo_stats['error']}[/red]")
                return False
            
            # Create detailed response
            response = f"""üìÇ **Repository File Analysis:**

üóÇÔ∏è **Repository:** {repo_stats['repository_name']}
üìÑ **Total Files:** {repo_stats['total_files']} files
üìã **File Extensions:** {', '.join(repo_stats['file_extensions'][:10])}
üîó **Repository URL:** {repo_stats['repository_url']}
üìÖ **Last Analyzed:** {repo_stats['last_analyzed'][:19]}
‚úÖ **Status:** {repo_stats['analysis_status']}

The repository contains **{repo_stats['total_files']} files** in total that have been successfully analyzed and indexed for Q&A."""

            conversation.write(response)
            
            # ‚úÖ Track for copy functionality
            timestamp = datetime.now().strftime("%H:%M:%S")
            if hasattr(self, 'conversation_history_for_copy'):
                self.conversation_history_for_copy.append({
                    "timestamp": timestamp,
                    "type": "ai",
                    "agent": "repo_analyzer",
                    "content": response,
                    "enterprise": False,
                    "display": f"[{timestamp}] üìä REPO STATS: {response}"
                })
            
            self.sound_system.play_sound("success")
            return True
            
     except Exception as e:
        error_msg = f"Repository query processing error: {e}"
        conversation.write(f"[red]‚ùå {error_msg}[/red]")
        self.sound_system.play_sound("error")
        return False
    
     return False

    def is_repo_related_query(self, user_input: str) -> bool:
     """üß† INTELLIGENT: Detect if query is about active GitHub repository"""
     if not self.active_github_repo:
        return False
     if self.is_repo_file_count_query(user_input):
        return True
    
     user_input_lower = user_input.lower()
    
     # üéØ SMART REPO KEYWORDS (EXTENSIVE LIST)
     repo_indicators = [
        # File-related
        "file", "files", "function", "class", "method", "variable", "import", "export",
        "main.py", "index.js", "app.py", "server.js", "config", "package.json", "requirements.txt",
        
        # Code-related  
        "code", "coding", "debug", "bug", "error", "fix", "explain", "what does", "how does",
        "algorithm", "logic", "implementation", "structure", "architecture", "design pattern",
        
        # Analysis-related
        "analyze", "analysis", "review", "check", "scan", "examine", "look at", "show me",
        "what is", "how to", "why", "where is", "find", "search", "locate",
        
        # Programming terms
        "api", "database", "sql", "json", "xml", "html", "css", "javascript", "python",
        "react", "vue", "angular", "nodejs", "django", "flask", "fastapi", "express",
        
        # Repository structure
        "folder", "directory", "src", "lib", "utils", "components", "models", "views",
        "controllers", "routes", "middleware", "tests", "docs", "readme", "license",
        
        # Specific questions
        "entry point", "main function", "dependencies", "libraries", "framework",
        "database connection", "api endpoints", "routes", "authentication", "security"
    ]
    
     # Check if query contains repo-related terms
     repo_score = sum(1 for indicator in repo_indicators if indicator in user_input_lower)
    
     # üî• ADVANCED: Check for programming language patterns
     if any(lang in user_input_lower for lang in ['python', 'javascript', 'java', 'cpp', 'html', 'css']):
        repo_score += 2
    
     # üî• ADVANCED: Check for file extension mentions
     if any(ext in user_input_lower for ext in ['.py', '.js', '.html', '.css', '.json', '.md', '.txt']):
        repo_score += 3
    
     # üî• ULTRA SMART: Questions that don't mention 'repo' but are clearly about code
     implicit_repo_patterns = [
        "what does this do", "explain this", "how does this work", "what is this",
        "debug this", "fix this", "improve this", "optimize this", "refactor this",
        "what files", "show files", "list files", "main entry", "starting point"
     ]
    
     for pattern in implicit_repo_patterns:
        if pattern in user_input_lower:
            repo_score += 5  # High score for implicit patterns
    
     # üéØ DECISION LOGIC
     return repo_score >= 2  # If 2+ repo indicators, it's about the repo
    
    def is_repo_file_count_query(self, user_input: str) -> bool:
     """Detect if user is asking about file count"""
     user_input_lower = user_input.lower()
    
     file_count_indicators = [
        "how many files", "file count", "number of files", 
        "total files", "files are there", "kitne files", 
        "file kitni", "files count", "file statistics"
     ]
    
     return any(indicator in user_input_lower for indicator in file_count_indicators)
    
    async def analyze_github_repository(self, repo_url: str) -> Dict[str, Any]:
     """Analyze GitHub repository with FIXED database path handling"""
     try:
        print(f"üöÄ Starting GitHub analysis for: {repo_url}")
        
        # Use the enhanced GitHub analyzer with FIXED paths
        result = await self.github_analyzer.analyze_repository(repo_url)
        
        if result.get('success'):
            # üî• CRITICAL: Set the QA engine to use CORRECT database path
            if hasattr(self.github_analyzer, 'vector_db_path') and self.github_analyzer.vector_db_path:
                # Set the QA engine repository context with CORRECT path
                if hasattr(self.github_analyzer, 'qa_engine') and self.github_analyzer.qa_engine:
                    success = self.github_analyzer.qa_engine.set_repository_context(
                        self.github_analyzer.vector_db_path, 
                        repo_url
                    )
                    if success:
                        print(f"‚úÖ QA Engine connected to repository at: {self.github_analyzer.vector_db_path}")
                    else:
                        print(f"‚ö†Ô∏è QA Engine connection failed")
        
        return result
        
     except Exception as e:
        return {"error": f"GitHub analysis failed: {str(e)}"}

    async def answer_repository_question(self, question: str) -> str:
     """Answer questions about the analyzed repository"""
     if not self.github_analyzer.has_active_repo():
        return "No repository has been analyzed yet. Please analyze a GitHub repository first using the GitHub integration feature."
    
     try:
        answer = await self.github_analyzer.answer_repo_question(question)
        return answer
     except Exception as e:
        return f"Failed to answer repository question: {str(e)}"

# ========== SMART SHORTCUT DISCOVERY SYSTEM ==========
class SmartShortcutManager:
    """Smart shortcut discovery and hint system"""
    
    def __init__(self):
        # Rotating footer hints
        self.footer_hints = [
            "üí° Press F1 for complete shortcuts guide",
            "üî• Try Ctrl+P for Command Palette magic", 
            "üéØ Type 'shortcuts' to see all hidden commands",
            "‚ö° Many power shortcuts available - Press F1",
            "üè¢ Ctrl+E for Enterprise Status dashboard",
            "üì§ Ctrl+X to Export, Ctrl+O to Copy chat",
            "üß© Ctrl+L for Plugin Manager hub",
            "üîç Type 'keys help' for interactive guide",
            "üí™ Hidden commands: 'show keys', 'bindings'",
            "üéÆ Gaming shortcuts: F5, F1, Escape keys",
        ]
        self.current_hint_index = 0
        
    def get_rotating_hint(self) -> str:
        """Get next rotating hint"""
        hint = self.footer_hints[self.current_hint_index]
        self.current_hint_index = (self.current_hint_index + 1) % len(self.footer_hints)
        return hint
    
    def get_shortcut_tutorial(self) -> str:
        """Get complete interactive shortcuts tutorial"""
        return """[bold cyan]üéØ NOVA CLI - Interactive Shortcuts Discovery Guide[/bold cyan]

[yellow]üî• ESSENTIAL SHORTCUTS (Start Here!):[/yellow]
‚Ä¢ [bold]Ctrl+P[/bold] ‚Üí Command Palette (üéÆ VSCode-style quick actions)
‚Ä¢ [bold]F1[/bold] ‚Üí This complete shortcuts guide
‚Ä¢ [bold]Escape[/bold] ‚Üí Focus back to input field
‚Ä¢ [bold]Ctrl+Q[/bold] ‚Üí Quit application

[yellow]üè¢ ENTERPRISE POWER SHORTCUTS:[/yellow]
‚Ä¢ [bold]Ctrl+E[/bold] ‚Üí Enterprise Status Dashboard
‚Ä¢ [bold]Ctrl+M[/bold] ‚Üí Multi-Candidate Processing Info
‚Ä¢ [bold]Ctrl+K[/bold] ‚Üí API Keys Rotation Status
‚Ä¢ [bold]Ctrl+X[/bold] ‚Üí Export Chat History
‚Ä¢ [bold]Ctrl+O[/bold] ‚Üí Copy Chat to Clipboard

[yellow]‚ö° PRODUCTIVITY SHORTCUTS:[/yellow]
‚Ä¢ [bold]Ctrl+U[/bold] ‚Üí Upload & Analyze Files
‚Ä¢ [bold]Ctrl+V[/bold] ‚Üí Voice Mode Toggle
‚Ä¢ [bold]Ctrl+S[/bold] ‚Üí Web Search Mode
‚Ä¢ [bold]Ctrl+G[/bold] ‚Üí GitHub Repository Analysis
‚Ä¢ [bold]Ctrl+L[/bold] ‚Üí Plugin Manager Hub

[yellow]üéÆ NAVIGATION SHORTCUTS:[/yellow]
‚Ä¢ [bold]Up/Down Arrows[/bold] ‚Üí Navigate command history
‚Ä¢ [bold]Ctrl+R[/bold] ‚Üí Search command history
‚Ä¢ [bold]F5[/bold] ‚Üí Refresh system status
‚Ä¢ [bold]Ctrl+C[/bold] ‚Üí Clear chat history

[blue]üí° HIDDEN TEXT COMMANDS (Type these!):[/blue]
‚Ä¢ [bold]shortcuts[/bold] ‚Üí Show all keyboard shortcuts
‚Ä¢ [bold]keys help[/bold] ‚Üí This interactive guide
‚Ä¢ [bold]show keys[/bold] ‚Üí Display keybindings table
‚Ä¢ [bold]bindings[/bold] ‚Üí Complete bindings reference
‚Ä¢ [bold]help shortcuts[/bold] ‚Üí Detailed shortcuts guide

[green]üéØ PRO TIPS:[/green]
‚Ä¢ Most shortcuts work globally (anywhere in the app)
‚Ä¢ Command Palette (Ctrl+P) gives you access to everything
‚Ä¢ Footer shows rotating hints every 12 seconds
‚Ä¢ F1 is your best friend for discovering features
‚Ä¢ Enterprise shortcuts unlock advanced functionality

[bold green]üöÄ Master these shortcuts to become a NOVA CLI power user![/bold green]"""

    def get_keybindings_table(self) -> str:
        """Get formatted keybindings table"""
        return """[bold cyan]‚å®Ô∏è NOVA CLI - Complete Keybindings Reference[/bold cyan]

[yellow]üìä KEYBINDINGS TABLE:[/yellow]
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Shortcut    ‚îÇ Action                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Ctrl+P      ‚îÇ Command Palette (Most Important!)    ‚îÇ
‚îÇ F1          ‚îÇ Complete Shortcuts Guide             ‚îÇ
‚îÇ Ctrl+Q      ‚îÇ Quit Application                     ‚îÇ
‚îÇ Escape      ‚îÇ Focus Input Field                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Ctrl+E      ‚îÇ Enterprise Status Dashboard          ‚îÇ
‚îÇ Ctrl+M      ‚îÇ Multi-Candidate Information          ‚îÇ
‚îÇ Ctrl+K      ‚îÇ API Keys Rotation Status             ‚îÇ
‚îÇ Ctrl+X      ‚îÇ Export Chat History                  ‚îÇ
‚îÇ Ctrl+O      ‚îÇ Copy Chat to Clipboard               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Ctrl+U      ‚îÇ Upload & Analyze Files               ‚îÇ
‚îÇ Ctrl+V      ‚îÇ Voice Mode Toggle                    ‚îÇ
‚îÇ Ctrl+S      ‚îÇ Web Search Mode                      ‚îÇ
‚îÇ Ctrl+G      ‚îÇ GitHub Repository Analysis           ‚îÇ
‚îÇ Ctrl+L      ‚îÇ Plugin Manager Hub                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Up/Down     ‚îÇ Navigate Command History             ‚îÇ
‚îÇ Ctrl+R      ‚îÇ Search Command History               ‚îÇ
‚îÇ F5          ‚îÇ Refresh System Status                ‚îÇ
‚îÇ Ctrl+C      ‚îÇ Clear Chat History                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[blue]üí° Text Commands:[/blue] shortcuts, keys help, show keys, bindings"""

# ========== WORLD'S BEST UI - ENHANCED CYBERPUNK GAMING (TEXTUAL COMPATIBLE) ==========

class NOVA_CLI(App):
    """üéÆ NOVA ULTRA - WORLD'S ABSOLUTE BEST AI CLI (ENTERPRISE ENHANCED)"""
    
    # Enhanced CSS with Enterprise Button Styles
    CSS = """
    /* NOVA ULTRA - WORLD'S ABSOLUTE BEST CLI CSS (TEXTUAL COMPATIBLE) */
    Screen {
        background: #0a0e27;
        color: #ffffff;
        layers: base overlay;
    }
    
    /* PREMIUM GAMING HEADER */
    Header {
        background: #1a1a2e;
        color: #00ff88;
        text-style: bold;
        dock: top;
        height: 3;
        border-bottom: thick #00ff88;
    }
    
    /* PREMIUM GAMING FOOTER */
    Footer {
        background: #16213e;
        color: #00ff88;
        dock: bottom;
        height: 3;
        border-top: thick #00ff88;
    }
    
    /* ENHANCED AGENT SIDEBAR */
    .sidebar {
        background: #1a1a2e;
        border-right: thick #00ff88;
        width: 32;
        dock: left;
        padding: 1;
        scrollbar-gutter: stable;
    }
    
    /* ENHANCED MAIN CONVERSATION */
    .main-content {
        background: #0d1117;
        border: thick #4444ff;
        width: 1fr;
        margin-left: 1;
        padding: 1;
    }
    
    /* ENHANCED STATUS PANEL */
    .status-panel {
        background: #1a1a2e;
        border-left: thick #ff8800;
        width: 35;
        dock: right;
        height: 1fr;
        margin-left: 1;
        padding: 1;
        scrollbar-gutter: stable;
    }
    
    /* PREMIUM GAMING BUTTONS (TEXTUAL COMPATIBLE) */
    Button {
        background: #16213e;
        color: #00ff88;
        border: thick #00ff88;
        margin: 1;
        text-style: bold;
        min-height: 4;
        width: 1fr;
    }
    
    Button:hover {
        background: #00ff88;
        color: #000000;
        border: thick #ffffff;
        text-style: bold;
    }
    
    Button:focus {
        background: #00cc66;
        color: #000000;
        border: thick #ffffff;
        text-style: bold;
    }
    
    Button.-primary {
        background: #4444ff;
        border: thick #4444ff;
        color: #ffffff;
    }
    
    Button.-primary:hover {
        background: #6666ff;
        color: #ffffff;
        border: thick #8888ff;
    }
    
    Button.-success {
        background: #00aa44;
        border: thick #00ff88;
        color: #ffffff;
    }
    
    Button.-success:hover {
        background: #00ff88;
        color: #000000;
        border: thick #ffffff;
    }
    
    Button.-warning {
        background: #ff8800;
        border: thick #ffaa44;
        color: #000000;
    }
    
    Button.-warning:hover {
        background: #ffaa00;
        color: #000000;
        border: thick #ffffff;
    }
    
    Button.-error {
        background: #cc3333;
        border: thick #ff4444;
        color: #ffffff;
    }
    
    Button.-error:hover {
        background: #ff4444;
        color: #ffffff;
        border: thick #ffffff;
    }
    
    /* NEW: ENTERPRISE BUTTON STYLES */
    Button.-enterprise {
        background: #6a0dad;
        border: thick #8a2be2;
        color: #ffffff;
    }
    
    Button.-enterprise:hover {
        background: #8a2be2;
        color: #ffffff;
        border: thick #ffffff;
    }
    
    Button.-premium {
        background: #ff6b35;
        border: thick #ff8c00;
        color: #ffffff;
    }
    
    Button.-premium:hover {
        background: #ff8c00;
        color: #000000;
        border: thick #ffffff;
    }
    
    /* ENHANCED SINGLE INPUT */
    Input {
        background: #16213e;
        border: thick #00ff88;
        color: #ffffff;
        margin-bottom: 1;
        height: 6;
        padding: 1;
    }
    
    Input:focus {
        border: thick #00ff88;
        background: #1a1a2e;
        color: #ffffff;
    }
    
    /* ENHANCED CONVERSATION LOG */
    RichLog {
        background: #0d1117;
        border: thick #30363d;
        scrollbar-background: #16213e;
        scrollbar-color: #00ff88;
        scrollbar-color-hover: #00cc66;
        scrollbar-size: 2 1;
        padding: 1;
        min-height: 25;
    }
    
    /* ENHANCED LABELS */
    Label {
        color: #00ff88;
        text-style: bold;
        text-align: center;
        background: #16213e;
        height: 3;
        margin-bottom: 1;
        border: thick #00ff88;
        padding: 1;
    }
    
    /* ENHANCED STATUS DISPLAYS */
    Static {
        background: #16213e;
        color: #ffffff;
        border: thick #30363d;
        padding: 1;
        text-align: left;
        margin-bottom: 1;
    }
    
    /* ENHANCED SECTION HEADERS */
    .agents-title {
        color: #00ff88;
        background: #16213e;
        text-style: bold;
        border: thick #00ff88;
        height: 3;
        margin-bottom: 1;
        text-align: center;
        padding: 1;
    }
    
    .features-title {
        color: #ff8800;
        background: #16213e;
        text-style: bold;
        border: thick #ff8800;
        height: 3;
        margin-bottom: 1;
        text-align: center;
        padding: 1;
    }
    
    .status-title {
        color: #4444ff;
        background: #16213e;
        text-style: bold;
        border: thick #4444ff;
        height: 3;
        margin-bottom: 1;
        text-align: center;
        padding: 1;
    }
    
    .enterprise-title {
        color: #8a2be2;
        background: #16213e;
        text-style: bold;
        border: thick #8a2be2;
        height: 3;
        margin-bottom: 1;
        text-align: center;
        padding: 1;
    }
    
    /* ENHANCED SPECIAL ELEMENTS */
    #conversation {
        background: #0d1117;
        border: thick #00ff88;
        scrollbar-background: #16213e;
        scrollbar-color: #00ff88;
        min-height: 25;
    }
    
    #user-input {
        background: #16213e;
        border: thick #00ff88;
        color: #ffffff;
        height: 6;
    }
    
    #user-input:focus {
        border: thick #00ff88;
        background: #1a1a2e;
        color: #ffffff;
    }
    
    #system-status {
        background: #0d1117;
        color: #00ff88;
        border: thick #30363d;
        padding: 1;
        text-style: bold;
        min-height: 15;
    }
    """
    
    # Enhanced KEYBINDINGS with Enterprise Features
    BINDINGS = [
        ("ctrl+q", "quit", "Quit"),
        ("ctrl+c", "clear_conversation", "Clear Chat"),
        ("ctrl+v", "voice_mode", "Voice Mode"),
        ("ctrl+u", "upload_file", "Upload File"),
        ("ctrl+g", "github_analyzer", "GitHub"),
        ("ctrl+l", "plugin_manager", "Plugin Manager"),
        ("ctrl+p", "command_palette", "Command Palette"),
        ("ctrl+s", "search_web", "Web Search"),
        ("ctrl+e", "enterprise_status", "Enterprise Status"),  # NEW
        ("ctrl+m", "multi_candidate", "Multi-Candidate"),      # NEW
        ("ctrl+k", "api_keys_status", "API Keys Status"),      # NEW
        ("ctrl+o", "copy_chat", "Copy Chat"),              # NEW
        ("ctrl+X", "export_chat", "Export Chat"),                  # NEW
        ("ctrl+h", "help_screen", "Help"),        # NEW
        ("f1", "toggle_sidebar", "Toggle Sidebar"),
        ("f5", "system_status", "System Status"),
        ("up", "history_up", "Previous Command"),
        ("down", "history_down", "Next Command"),
        ("ctrl+r", "search_history", "Search History"),
        ("escape", "focus_input", "Focus Input"),
    ]
    
    # Reactive variables for real-time updates
    current_agent = reactive("general")
    conversation_count = reactive(0)
    response_time = reactive("0.00s")
    voice_active = reactive(False)
    file_uploaded = reactive(False)
    ml_status = reactive("Active" if ML_SYSTEM_AVAILABLE else "Disabled")
    enterprise_mode = reactive(True)  # NEW
    
    def __init__(self):
        super().__init__()
        
        # Initialize ENHANCED NOVA system with Enterprise API Manager
        self.nova_system = NovaUltraSystem()

        self.conversation_history_for_copy = []

        self.shortcut_manager = SmartShortcutManager()
        self.hint_counter = 0
        
        # Command history and autocomplete
        self.command_history = []
        self.history_index = -1
        
        # ENHANCED COMMAND SUGGESTIONS with Enterprise Features
        self.command_suggestions = [
            "help me with coding",
            "analyze this code", 
            "debug my python script",
            "enterprise system status",
            "multi-candidate analysis",
            "intelligent model routing",
            "api keys rotation status",
            "export chat history",
            "manage plugins",
            "force multi-candidate response",
            "rate limit information",
            "career advice",
            "business strategy analysis", 
            "comprehensive market research",
            "system architecture design",
            "performance optimization",
            "scalability planning",
            "security assessment",
            "web search latest trends",
            "github repository analysis",
            "voice interaction mode",
            "file analysis and insights",
            "clear chat history",
            "system status overview",
            "voice commands",
            "search history",
            "upload file",
            "show metrics",
            "weather Mumbai",
            "weather Pune"
            "indian cricket match score latest"          
    "forecast Delhi",           # Weather forecast
    "calc 2 + 2 * 5",          # Calculator
    "emi 5000000 8.5 20",      # EMI calculator
    "currency 100 USD to INR",  # Currency conversion
    "convert 10 kg_to_lbs",     # Unit conversion
    "timer 25m",               # Timer plugin
    "pomodoro",                # Pomodoro timer
    "note Meeting tomorrow",    # Notes plugin
    "notes",                   # List notes
    "search_notes meeting",     # Search notes
    "code create a REST API",   # AI Code Assistant
    "debug this code",         # Code debugging
    "password 16",             # Password generator
    "security scan localhost", # Security tools
    "encrypt secret data",     # Data encryption
    "api status",              # API hub
    "github integration",      # GitHub tools
    "slack connect"          # Slack integration
        ]
    
    def compose(self) -> ComposeResult:
        """Compose the WORLD'S BEST ENTERPRISE interface"""
        yield Header(show_clock=True)
        
        with Horizontal():
            # LEFT SIDEBAR - ENHANCED with Enterprise Features
            with Container(classes="sidebar"):
                yield Label("ü§ñ NOVA AGENTS", classes="agents-title")
                yield Button("üîß CODING EXPERT", id="agent-coding", classes="-primary")
                yield Button("üíº CAREER COACH", id="agent-career", classes="-success")
                yield Button("üìà BUSINESS GURU", id="agent-business", classes="-primary")
                yield Button("üè• MEDICAL ADVISOR", id="agent-medical", classes="-error")
                yield Button("üíô EMOTIONAL SUPPORT", id="agent-emotional", classes="-success")
                yield Button("üöÄ TECH ARCHITECT", id="agent-technical", classes="-primary")
                
                # NEW: ENTERPRISE FEATURES SECTION
                yield Label("üè¢ ENTERPRISE", classes="enterprise-title")
                yield Button("üéØ MULTI-CANDIDATE", id="multi-candidate", classes="-enterprise")
                yield Button("üîë API KEYS STATUS", id="api-keys", classes="-enterprise")
                yield Button("üìä ENTERPRISE STATUS", id="enterprise-status", classes="-enterprise")
                yield Button("üîÑ RATE LIMITS", id="rate-limits", classes="-enterprise")
                
                yield Label("üéØ PREMIUM TOOLS", classes="features-title")
                yield Button("üé§ VOICE MODE", id="voice-mode", classes="-success")
                yield Button("üîç WEB SEARCH", id="web-search", classes="-primary")
                yield Button("üìÅ FILE UPLOAD", id="file-upload", classes="-warning")
                yield Button("üìä FILE ANALYSIS", id="file-analysis", classes="-warning")
                yield Button("üîó GITHUB ANALYZER", id="github-analysis", classes="-success")
                yield Button("üß† ML INSIGHTS", id="ml-insights", classes="-primary")
                yield Button("‚å®Ô∏è COMMAND PALETTE", id="command-palette", classes="-primary")
                
                # NEW: PLUGINS & EXPORT SECTION
                yield Label("üß© PLUGINS & EXPORT", classes="features-title")
                yield Button("‚ö° MANAGE PLUGINS", id="plugins", classes="-premium")
                yield Button("üì§ EXPORT CHAT", id="export-chat", classes="-premium")
                yield Button("üìã COPY CHAT", id="copy-chat", classes="-premium")
                yield Button("üîß PLUGIN SETTINGS", id="plugin-settings", classes="-warning")
            
            # MAIN CONTENT - Enhanced
            with Container(classes="main-content"):
                yield RichLog(id="conversation", markup=True, highlight=True, wrap=True)
                yield Input(
                    placeholder="üöÄ Ask NOVA anything... (Type your query and Press Enter)",
                    id="user-input",
                    suggester=SuggestFromList(self.command_suggestions, case_sensitive=False)
                )
            
            # RIGHT STATUS PANEL - Enhanced with Enterprise Metrics
            with Container(classes="status-panel"):
                yield Label("üìä ENTERPRISE METRICS", classes="status-title")
                yield Static("", id="system-status")
                yield Static("", id="enterprise-stats")  # NEW
                yield Button("üîÑ REFRESH", id="refresh-status", classes="-primary")
                yield Button("‚ùå CLEAR CHAT", id="clear-chat", classes="-error")
                yield Button("üìù HISTORY", id="show-history", classes="-warning")
                yield Button("üìä ANALYTICS", id="show-analytics", classes="-premium")  # NEW
        
        yield Footer()
    
    def on_mount(self) -> None:
     """Initialize NOVA-CLI ENTERPRISE interface with Smart Shortcuts"""
     conversation = self.query_one("#conversation", RichLog)

     # PRESERVE YOUR ORIGINAL ENTERPRISE STATUS LOGIC (EXACTLY AS YOU HAVE)
     try:
        if self.nova_system.api_manager and hasattr(self.nova_system.api_manager, 'get_enterprise_status'):
            enterprise_status = self.nova_system.api_manager.get_enterprise_status()
        else:
            # Fallback status
            enterprise_status = {
                'enterprise_features': {
                    'multi_candidate_responses': '‚ùå Basic Mode',
                    'intelligent_model_routing': '‚úÖ Active',
                    'multi_key_rotation': '‚ùå Not Available',
                    'rate_limiting': '‚úÖ Basic',
                    'smart_enhancement_detection': '‚úÖ Active'
                },
                'performance_metrics': {
                    'available_providers': len(getattr(self.nova_system.api_manager, 'available', [])),
                    'total_api_keys': len(getattr(self.nova_system.api_manager, 'available', [])),
                    'average_quality_rating': 8.0
                }
            }
     except Exception as e:
        print(f"Enterprise status error: {e}")
        enterprise_status = {
            'enterprise_features': {'multi_candidate_responses': '‚ùå Error'},
            'performance_metrics': {'available_providers': 0, 'total_api_keys': 0, 'average_quality_rating': 0}
        }

     # ENHANCED WELCOME MESSAGE (Your original + shortcuts discovery)
     welcome_msg = f"""[bold cyan]üöÄ NOVA CLI v3.0 - Ultimate AI Experience[/bold cyan]

[green]‚úÖ Top 1% Performance Architecture Active[/green]
[gold]‚úÖ Multi-Candidate Response System: {enterprise_status['enterprise_features']['multi_candidate_responses']}[/gold]
[gold]‚úÖ Intelligent Model Routing: {enterprise_status['enterprise_features']['intelligent_model_routing']}[/gold]
[gold]‚úÖ Multi-Key API Rotation: {enterprise_status['enterprise_features']['multi_key_rotation']}[/gold]
[gold]‚úÖ Enterprise Rate Limiting: {enterprise_status['enterprise_features']['rate_limiting']}[/gold]
[gold]‚úÖ Smart Enhancement Detection: {enterprise_status['enterprise_features']['smart_enhancement_detection']}[/gold]

[yellow]üè¢ Enterprise Infrastructure:[/yellow]
[blue]‚Ä¢ Available Providers: {enterprise_status['performance_metrics']['available_providers']} premium providers (Google, Groq, DeepSeek, Cerebras, Mistral, etc.)[/blue]
[blue]‚Ä¢ Total API Keys: {enterprise_status['performance_metrics']['total_api_keys']} keys with intelligent rotation[/blue]
[blue]‚Ä¢ Provider Quality: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10 average rating[/blue]
[blue]‚Ä¢ Zero Failure Guarantee: Multi-tier fallback system ensures 100% uptime[/blue]

[magenta]üß† Intelligent Features (Automatic):[/magenta]
[cyan]‚Ä¢ **Smart Agent Selection** - AI automatically picks best agent for your query (no manual selection needed!)[/cyan]
[cyan]‚Ä¢ **Multi-Candidate Responses** - Complex queries analyzed by multiple models, best response selected[/cyan]
[cyan]‚Ä¢ **ML Enhancement Detection** - AI decides when to apply full ML processing vs quick response[/cyan]
[cyan]‚Ä¢ **UltraHybridMemorySystem** - Industry's best conversation memory with context awareness[/cyan]
[cyan]‚Ä¢ **Intelligent Model Routing** - Automatically routes to optimal model (coding‚ÜíDeepSeek, speed‚ÜíGroq, etc.)[/cyan]
[cyan]‚Ä¢ **Smart Error Handling** - Automatic fallback with local LLM if all APIs fail[/cyan]

[yellow]üéØ Interactive Features (Try These!):[/yellow]
‚Ä¢ **üîß File Upload (Ctrl+U)** - Upload any file ‚Üí CLI minimizes ‚Üí select file ‚Üí returns with AI analysis ready!
‚Ä¢ **üé§ Voice Mode (Ctrl+V)** - Speak naturally ‚Üí AI processes voice ‚Üí get intelligent responses!  
‚Ä¢ **üìÅ GitHub Analyzer (Ctrl+G)** - Enter repo URL ‚Üí complete codebase analysis with QA engine!
‚Ä¢ **üåê Web Search (Ctrl+S)** - Real-time web search with AI-powered result analysis
‚Ä¢ **üí¨ Natural Chat** - Just type and get responses from world's best AI models automatically!

[yellow]üöÄ Available Agents (Auto-Selected):[/yellow]
‚Ä¢ üîß **Coding Expert** - Enterprise development with best practices (auto-triggered by code keywords)
‚Ä¢ üíº **Career Coach** - Professional growth & strategic planning (auto-triggered by career queries) 
‚Ä¢ üìà **Business Guru** - Market analysis & strategic consulting (auto-triggered by business terms)
‚Ä¢ üè• **Medical Advisor** - Health insights with professional guidance (auto-triggered by health queries)
‚Ä¢ üíô **Emotional Support** - Empathetic guidance & wellness (auto-triggered by emotional context)
‚Ä¢ üèóÔ∏è **Tech Architect** - System design & architecture (auto-triggered by technical queries)

[yellow]‚ö° Smart Shortcuts (Essential):[/yellow]
[green]MUST KNOW (Learn These First!):[/green]
‚Ä¢ [bold]Ctrl+P[/bold] - Command Palette (üéØ Access Everything Instantly!)
‚Ä¢ [bold]F1[/bold] - Complete Shortcuts Guide & Help System
‚Ä¢ [bold]Ctrl+U[/bold] - File Upload (Any Format - PDF, Code, Excel, Word, etc.)
‚Ä¢ [bold]Ctrl+V[/bold] - Voice Mode (Speak Naturally to AI)
‚Ä¢ [bold]Ctrl+G[/bold] - GitHub Repository Analysis

[blue]ADVANCED FEATURES:[/blue]
‚Ä¢ [bold]Ctrl+E[/bold] - Enterprise Status Dashboard | [bold]Ctrl+M[/bold] - Multi-Candidate Info
‚Ä¢ [bold]Ctrl+K[/bold] - API Keys Status | [bold]Ctrl+S[/bold] - Web Search
‚Ä¢ [bold]Ctrl+X[/bold] - Export Chat | [bold]Ctrl+O[/bold] - Copy Chat
‚Ä¢ [bold]Ctrl+L[/bold] - Plugin Management | [bold]Ctrl+C[/bold] - Clear Chat

[blue]DISCOVERY COMMANDS (Type These!):[/blue]
‚Ä¢ [bold]shortcuts[/bold] - Show all shortcuts | [bold]keys help[/bold] - Interactive tutorial
‚Ä¢ [bold]show keys[/bold] - Formatted shortcuts table | [bold]tips[/bold] - Pro user tips

[bold magenta]üéØ WHAT MAKES NOVA SPECIAL:[/bold magenta]
[bold green]ü§ñ ZERO MANUAL WORK - AI handles everything automatically![/bold green]
[bold green]üß† SMARTEST MEMORY - Remembers context across conversations![/bold green] 
[bold green]‚ö° FASTEST MODELS - Google Gemini 2.5, Groq 14K tokens/sec, DeepSeek coding![/bold green]
[bold green]üõ°Ô∏è NEVER FAILS - Multi-tier fallback ensures you always get responses![/bold green]
[bold green]üìÅ ANY FILE TYPE - Upload & analyze PDFs, code, Excel, Word, images![/bold green]
[bold green]üé§ NATURAL VOICE - Speak normally, get intelligent AI responses![/bold green]
[bold green]üåê LIVE WEB DATA - Real-time search with AI analysis![/bold green]

[bold yellow]üí° PRO TIPS FOR ULTIMATE EXPERIENCE:[/bold yellow]
[bold yellow]‚Ä¢ Just type naturally - AI automatically selects best agent & model![/bold yellow]
[bold yellow]‚Ä¢ Upload files with Ctrl+U - AI becomes expert on your documents![/bold yellow]
[bold yellow]‚Ä¢ Use voice with Ctrl+V - Perfect for long explanations![/bold yellow]
[bold yellow]‚Ä¢ GitHub analysis with Ctrl+G - Instant codebase expert![/bold yellow]
[bold yellow]‚Ä¢ Command Palette (Ctrl+P) = One-stop access to everything![/bold yellow]

[bold green]üåü Experience AI Like Never Before - No Competition! üåü[/bold green]"""

     conversation.write(welcome_msg)

     # Play enterprise startup sound
     self.nova_system.sound_system.play_sound("success")

     # Focus on input
     self.query_one("#user-input", Input).focus()

     # Update enterprise status
     self.update_enterprise_status()

     # NEW: Start the smart hint system
     self.setup_smart_hints()
    
    def setup_smart_hints(self):
     """Setup smart rotating hints system with proper timing (25-min intervals)"""
     # Show initial hint after 2 minutes (120 seconds) to let user get comfortable
     self.set_timer(120.0, self.show_smart_hint)
    
     # ‚úÖ FIXED: Setup recurring hints every 25 minutes (1500 seconds) 
     # Much less intrusive and won't break conversation flow
     self.set_interval(1500.0, self.show_smart_hint)

    def show_smart_hint(self):
     """Show smart rotating hint - now much less frequent and subtle"""
     conversation = self.query_one("#conversation", RichLog)
     hint = self.shortcut_manager.get_rotating_hint()
    
     # ‚úÖ FIXED: Make hints much more subtle
     # Only show prominent hints every 5th time (instead of every 3rd)
     if self.hint_counter % 5 == 0:
        conversation.write(f"[bold cyan]üí° TIP: {hint}[/bold cyan]")
     else:
        # Very subtle hints that don't interrupt conversation
        conversation.write(f"[dim white]üí° {hint}[/dim white]")
    
     self.hint_counter += 1

    def update_enterprise_status(self):
        """Enhanced system status with enterprise metrics"""
        # Get Enterprise API Manager status
        enterprise_status = self.nova_system.api_manager.get_enterprise_status()
        
        status_text = f"""ü§ñ Agent: {self.current_agent.title()}
üìä Conversations: {self.conversation_count}
‚è±Ô∏è Response: {self.response_time}
‚ö° Providers: {enterprise_status['performance_metrics']['available_providers']}
üîë API Keys: {enterprise_status['performance_metrics']['total_api_keys']} rotating
üéØ Multi-Candidate: ‚úÖ Active
üìÅ File: {'Uploaded' if self.file_uploaded else 'Ready'}
üß† ML: {self.ml_status}
üîä Sound: ‚úÖ Active
üìù History: {len(self.command_history)} commands
üóÇÔ∏è GitHub: {'‚úÖ ' + os.path.basename(self.nova_system.active_github_repo) if self.nova_system.active_github_repo else '‚ùå None'}"""

        
        self.query_one("#system-status", Static).update(status_text)
        
        # NEW: Enterprise stats panel
        enterprise_stats_text = f"""Enterprise: {enterprise_status['system_info']['status']}
Features: {enterprise_status['system_info']['enterprise_features_loaded']}
Quality: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10
Multi-Candidate: ‚úÖ Active
Smart Routing: ‚úÖ Active  
Rate Protection: ‚úÖ Active
Key Rotation: ‚úÖ Active
Context Memory: ‚úÖ Active"""

        self.query_one("#enterprise-stats", Static).update(enterprise_stats_text)
    
    # ========== ENHANCED INPUT HANDLING with Enterprise Features ==========
    
    @on(Input.Submitted, "#user-input")
    async def handle_user_input(self, event: Input.Submitted) -> None:
     """ENHANCED user input handling WITH SMART SHORTCUT DISCOVERY + ENTERPRISE FEATURES + PLUGIN INTEGRATION + CONVERSATION TRACKING"""
     user_input = event.value.strip()
    
     if not user_input:
        return
 
     # PRIORITY FIX: Repository Q&A MUST come BEFORE agent detection
     if (self.nova_system.is_repo_related_query(user_input) and 
        hasattr(self.nova_system, 'github_analyzer') and 
        self.nova_system.github_analyzer.has_active_repo()):
        
        conversation = self.query_one("#conversation")
        conversation.write("üîç SYSTEM: Repository Q&A started for: " + user_input)
        
        # Use repository-specific Q&A - BYPASS agent detection
        repo_response = await self.nova_system.github_analyzer.answer_repo_question(user_input)
        
        conversation.write("ü§ñ NOVA (Github_Qa):")
        conversation.write(repo_response)
        
        # Add to history and clear input
        self.conversation_count += 1
        self.command_history.append(user_input)
        event.input.value = ""
        return  # EXIT - Don't process through normal agent detection

     # Play input sound
     self.nova_system.sound_system.play_sound("click")

     # Clear input
     event.input.value = ""

     # Add to command history
     if user_input not in self.command_history:
        self.command_history.append(user_input)
        if len(self.command_history) > 100:
            self.command_history = self.command_history[-100:]

     # Reset history index
     self.history_index = -1

     # Get conversation log
     conversation = self.query_one("#conversation", RichLog)

     # Show user message with timestamp
     timestamp = datetime.now().strftime("%H:%M:%S")
     conversation.write(f"[dim][{timestamp}][/dim] [bold blue]üë§ You:[/bold blue] {user_input}")
    
     # ‚úÖ TRACK USER MESSAGE FOR COPY FUNCTIONALITY
     self.conversation_history_for_copy.append({
        "timestamp": timestamp,
        "type": "user",
        "content": user_input,
        "display": f"[{timestamp}] You: {user_input}"
     })

     # ========== üÜï NEW: SMART SHORTCUT DISCOVERY COMMANDS (HIGHEST PRIORITY) ==========
     user_input_lower = user_input.lower()
     if user_input_lower in ["shortcuts", "show shortcuts", "keys", "keybindings"]:
        self.show_shortcuts_guide()
        return
     elif user_input_lower in ["keys help", "shortcuts help", "help keys", "interactive help"]:
        self.show_interactive_shortcuts_tutorial()
        return
     elif user_input_lower in ["show keys", "bindings", "show bindings", "keybindings table"]:
        self.show_keybindings_table()
        return
     elif user_input_lower in ["hints", "show hints", "tips", "shortcut tips"]:
        self.show_shortcut_tips()
        return
     elif user_input_lower in ["help shortcuts", "shortcuts guide", "guide"]:
        self.show_shortcuts_guide()
        return
    
     # Enhanced processing indicator based on query complexity
     try:
        # Try to import but don't use for multi-candidate
        from new_claude import SmartEnhancementDetector
        enhancement_detected = SmartEnhancementDetector.needs_ml_enhancement(user_input)
    
        # Always use single response mode regardless of enhancement detection
        if enhancement_detected:
         conversation.write("[blue]üß† NOVA Enhanced Processing (Single Response Mode)...[/blue]")
        else:
         conversation.write("[green]üî• NOVA Core activated...[/green]")
        
     except ImportError:
       # Fallback if SmartEnhancementDetector not available
       conversation.write("[green]üî• NOVA Core activated...[/green]")
     except Exception as e:
       # Safe fallback if SmartEnhancementDetector causes errors
       print(f"SmartEnhancementDetector error: {e}")
       conversation.write("[green]üî• NOVA Core activated...[/green]")

     # ========== üöÄ NEW: PLUGIN COMMAND PROCESSING (HIGHEST PRIORITY) ==========
     if self.nova_system.plugin_manager and hasattr(self.nova_system.plugin_manager, 'is_plugin_command'):
        try:
            command_word = user_input.split()[0].lower()
            if self.nova_system.plugin_manager.is_plugin_command(command_word):
                conversation.write("[yellow]üß© Processing plugin command...[/yellow]")
                
                # Execute plugin command
                command = user_input.split()[0]
                args = " ".join(user_input.split()[1:]) if len(user_input.split()) > 1 else ""
                
                plugin_result = await self.nova_system.plugin_manager.execute_plugin_command(
                    command, args, {"user_id": self.nova_system.user_id}
                )
                
                if plugin_result.get("success"):
                    result_data = plugin_result["result"]
                    plugin_name = plugin_result["plugin_name"]
                    
                    # Show plugin response with enhanced formatting
                    plugin_header = self.nova_system.ultimate_emoji_system.create_smart_header(
                        "general", False, "plugin_execution", command
                    )
                    conversation.write(f"[bold green]{plugin_header}:[/bold green]")

                    enhanced_plugin_msg = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                        result_data.get("message", "Plugin executed successfully"), 
                        "general", command, "plugin_execution", False
                    )
                    conversation.write(enhanced_plugin_msg)
                    
                    # Track for copy functionality
                    plugin_timestamp = datetime.now().strftime("%H:%M:%S")
                    self.conversation_history_for_copy.append({
                        "timestamp": plugin_timestamp,
                        "type": "plugin",
                        "plugin": plugin_name,
                        "content": result_data.get("message", "Plugin executed successfully"),
                        "display": f"[{plugin_timestamp}] üß© {plugin_name}: {result_data.get('message', 'Plugin executed successfully')}"
                    })
                    
                    # Show plugin metadata
                    plugin_metadata = f"üß© Plugin: {plugin_name} | ‚è±Ô∏è Command: {command} | üéØ Type: {result_data.get('type', 'unknown')}"
                    conversation.write(f"[dim]{plugin_metadata}[/dim]")
                    
                    # Track metadata for copy
                    self.conversation_history_for_copy.append({
                        "timestamp": plugin_timestamp,
                        "type": "metadata",
                        "content": plugin_metadata,
                        "display": f" ‚îî‚îÄ {plugin_metadata}"
                    })
                    
                    conversation.write("")
                    self.nova_system.sound_system.play_sound("success")
                else:
                    error_msg = plugin_result.get("error", "Plugin execution failed")
                    conversation.write(f"[red]‚ùå Plugin Error: {error_msg}[/red]")
                    
                    # Track error for copy
                    self.conversation_history_for_copy.append({
                        "timestamp": datetime.now().strftime("%H:%M:%S"),
                        "type": "plugin_error",
                        "content": error_msg,
                        "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå PLUGIN ERROR: {error_msg}"
                    })
                    
                    self.nova_system.sound_system.play_sound("error")
                    
                # Update status and return (don't process as regular AI command)
                self.update_enterprise_status()
                return
                
        except Exception as e:
            conversation.write(f"[red]‚ùå Plugin system error: {str(e)}[/red]")
            
            # Track plugin system error for copy
            self.conversation_history_for_copy.append({
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "system_error",
                "content": f"Plugin system error: {str(e)}",
                "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå PLUGIN SYSTEM ERROR: {str(e)}"
            })
            
            self.nova_system.sound_system.play_sound("error")
            self.update_enterprise_status()
            return

     # ========== ENTERPRISE COMMANDS (HIGH PRIORITY) ==========
     if user_input.lower() in ["enterprise status", "enterprise system status"]:
        self.show_enterprise_status()
        return
     elif user_input.lower().startswith("multi-candidate analysis") or user_input.lower().startswith("force multi-candidate"):
        await self.force_multi_candidate_response(user_input)
        return
     elif user_input.lower() in ["api keys status", "api status", "keys status"]:
        self.show_api_keys_status()
        return
    
     # ========== SPECIAL SYSTEM COMMANDS ==========
     if user_input.lower().startswith("search "):
        query = user_input[7:]
        await self.handle_web_search(query)
        return
     elif user_input.lower() in ["upload", "upload file", "file upload"]:
        await self.handle_file_upload()
        return
     elif user_input.lower() in ["voice", "voice mode"]:
        await self.activate_voice_mode()
        return
     elif user_input.lower() in ["clear", "clear chat"]:
        self.clear_conversation()
        return
     elif user_input.lower() in ["help", "help me"]:
        self.action_help_screen()
        return
     elif user_input.lower() in ["repos", "repositories", "registry", "show repos"]:
         await self.show_repository_registry()
         return
     
     elif user_input.lower() in ["debug db", "check db", "validate db", "db status", "database status"]:
         self.nova_system.sound_system.play_sound("click")
         conversation = self.query_one("#conversation", RichLog)

         if self.nova_system.github_analyzer.has_active_repo():
              # Enhanced database validation with emoji system
              validation_header = self.nova_system.ultimate_emoji_system.create_smart_header(
                  "technical_architect", self.enterprise_mode, "database_validation", "database check"
              )
              conversation.write(f"[bold green]{validation_header}[/bold green]")
              conversation.write("[yellow]üîç Validating repository database connection...[/yellow]")

              # Perform validation
              is_valid = self.nova_system.github_analyzer.validate_database_connection()

              if is_valid:
                  success_msg = "Database connection validated successfully - Repository vector database is healthy and operational!"
                  enhanced_success = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                      success_msg, "technical_architect", "database validation", "validation_success", self.enterprise_mode
                  )
                  conversation.write(f"[bold green]‚úÖ {enhanced_success}[/bold green]")

                  # Show database stats if available
                  if hasattr(self.nova_system.github_analyzer, 'vector_db_path') and self.nova_system.github_analyzer.vector_db_path:
                      conversation.write(f"[blue]üìÑ Database Path:[/blue] {self.nova_system.github_analyzer.vector_db_path}")

                  conversation.write("[green]üéØ Repository is ready for intelligent queries![/green]")
                  self.nova_system.sound_system.play_sound("success")
              else:
                    error_msg = "Database validation failed - Unable to connect to the repository vector database. Please check the configuration."
                    enhanced_error = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                        error_msg, "technical_architect", "database validation failed", "validation_failure", self.enterprise_mode
                    )
                    conversation.write(f"[bold red]‚ùå {enhanced_error}[/bold red]")
                    conversation.write("[yellow]üí° Troubleshooting suggestions:[/yellow]")
                    conversation.write("[yellow] ‚Ä¢ Re-analyze the repository to rebuild the database[/yellow]")
                    conversation.write("[yellow] ‚Ä¢ Check if the repository URL is still accessible[/yellow]")
                    conversation.write("[yellow] ‚Ä¢ Verify ChromaDB installation and dependencies[/yellow]")
                    self.nova_system.sound_system.play_sound("error")

              # Track for copy functionality
              validation_timestamp = datetime.now().strftime("%H:%M:%S")
              self.conversation_history_for_copy.append({
                    "timestamp": validation_timestamp,
                    "type": "database_validation",
                    "content": f"Database validation: {'‚úÖ Success' if is_valid else '‚ùå Failed'}",
                    "display": f"[{validation_timestamp}] üîç DB VALIDATION: {'‚úÖ Success' if is_valid else '‚ùå Failed'}"
                })

         else:
              # No active repository message with emoji enhancement
              no_repo_msg = "No active repository found - Please analyze a GitHub repository first before validating database!"
              enhanced_no_repo = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                  no_repo_msg, "general", "no repository", "repository_required", self.enterprise_mode
              )

              conversation.write(f"[red]‚ùå {enhanced_no_repo}[/red]")
              conversation.write("[blue]üí° To analyze a repository, paste a GitHub URL in the chat![/blue]")
              self.nova_system.sound_system.play_sound("error")

              # Track no repo error
              no_repo_timestamp = datetime.now().strftime("%H:%M:%S")
              self.conversation_history_for_copy.append({
                    "timestamp": no_repo_timestamp,
                    "type": "validation_error",
                    "content": "Database validation failed: No active repository",
                    "display": f"[{no_repo_timestamp}] ‚ùå DB VALIDATION: No active repository"
                })           
         return
         
     # ========== PLUGIN MANAGEMENT COMMANDS ==========
     if user_input.lower().startswith("plugins"):
        await self.handle_plugin_command()
        return
     elif user_input.lower().startswith("export"):
        await self.handle_export_command(user_input)
        return
    
     # ========== üî• ENHANCED GITHUB REPOSITORY ANALYSIS WITH PROGRESS DISPLAY ==========
     elif user_input.lower().startswith("https://github.com/") or user_input.lower().startswith("github.com/"):
        try:
            repo_url = user_input if user_input.startswith("https://") else f"https://{user_input}"
            
            # ‚úÖ ENHANCED PROGRESS DISPLAY IN CHATBOX
            conversation.write(f"[bold cyan]üöÄ GitHub Repository Analysis Started[/bold cyan]")
            conversation.write(f"[blue]üîó Repository: {repo_url}[/blue]")
            conversation.write("[yellow]üìä Step 1/5: Initializing repository analysis...[/yellow]")
            
            # Track initialization
            self.conversation_history_for_copy.append({
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "system",
                "content": f"Repository analysis started: {repo_url}",
                "display": f"[{datetime.now().strftime('%H:%M:%S')}] üöÄ ANALYSIS STARTED: {repo_url}"
            })
            
            conversation.write("[yellow]üìä Step 2/5: Fetching repository data from GitHub...[/yellow]")
            conversation.write("[yellow]üìä Step 3/5: Processing files and creating vector database...[/yellow]")
            conversation.write("[yellow]üìä Step 4/5: Analyzing code structure and dependencies...[/yellow]")
            conversation.write("[yellow]üìä Step 5/5: Generating comprehensive analysis report...[/yellow]")
            
            # Show processing with enhanced visual feedback
            conversation.write("[bold blue]‚ö° Processing in progress... (This may take a moment for large repositories)[/bold blue]")
            
            # Execute the actual analysis
            analysis_result = await self.nova_system.github_analyzer.analyze_repository(repo_url)
            
            if analysis_result.get("success"):
                # ‚úÖ SUCCESS DISPLAY WITH DETAILED PROGRESS FEEDBACK
                conversation.write("[bold green]‚úÖ Repository Analysis Complete![/bold green]")
                
                self.nova_system.active_github_repo = repo_url
                self.nova_system.active_github_repo_data = analysis_result
                
                # üî• NEW: Set repository context in QA engine
                if hasattr(self.nova_system, 'qa_engine') and self.nova_system.qa_engine:
                    try:
                        vectordb_path = analysis_result.get('database_path', './chroma_repos')
                        self.nova_system.qa_engine.set_repository_context(
                            db_path=vectordb_path,
                            repo_url=repo_url
                        )
                        conversation.write("[bold green]‚úÖ QA Engine repository context set[/bold green]")
                    except Exception as e:
                        conversation.write(f"[yellow]‚ö†Ô∏è QA Engine context setup failed: {e}[/yellow]")
                
                self.nova_system.repo_context_memory = {
                    'last_analyzed_repo': repo_url,
                    'repo_files_detected': analysis_result.get('files_processed', 0),
                    'repo_languages': analysis_result.get('languages', []),
                    'repo_analysis_summary': f"Repository: {analysis_result['repo_name']}, Languages: {', '.join(analysis_result['languages'])}"
                }

                # Enhanced success message with emoji system
                github_header = self.nova_system.ultimate_emoji_system.create_smart_header(
                    "coding", False, "github_analysis", f"github analysis {repo_url}"
                )
                conversation.write(f"[bold green]{github_header}[/bold green]")

                # Display detailed results
                conversation.write(f"[bold green]üìÇ Repository Name:[/bold green] {analysis_result['repo_name']}")
                conversation.write(f"[bold green]üóÇÔ∏è Files Processed:[/bold green] {analysis_result['files_processed']}")
                conversation.write(f"[bold green]üíª Languages Detected:[/bold green] {', '.join(analysis_result['languages'])}")
                conversation.write(f"[bold green]üìÖ Analysis Date:[/bold green] {analysis_result.get('processed_at', 'Just now')}")
                
                if analysis_result.get('database_path'):
                    conversation.write(f"[bold green]üóÑÔ∏è Database Path:[/bold green] {analysis_result['database_path']}")
                
                # Enhanced response message
                enhanced_success_msg = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    f"Repository analysis completed successfully! Files: {analysis_result['files_processed']}, Languages: {', '.join(analysis_result['languages'])}", 
                    "coding", f"github analysis {repo_url}", "github_analysis", False
                )
                conversation.write(enhanced_success_msg)
                
                # Track success response
                self.conversation_history_for_copy.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "type": "ai",
                    "agent": "github_analyzer",
                    "content": f"Repository analysis completed: {analysis_result['repo_name']}, Files: {analysis_result['files_processed']}, Languages: {', '.join(analysis_result['languages'])}",
                    "enterprise": False,
                    "display": f"[{datetime.now().strftime('%H:%M:%S')}] üîó ANALYSIS SUCCESS: {analysis_result['repo_name']}"
                })
                
                # Show issues and suggestions if available
                if analysis_result.get('issues_found'):
                    conversation.write(f"[yellow]‚ö†Ô∏è Issues Found:[/yellow]")
                    for issue in analysis_result['issues_found']:
                        conversation.write(f"  ‚Ä¢ {issue}")
                
                if analysis_result.get('suggestions'):
                    conversation.write(f"[green]üí° Suggestions:[/green]")
                    for suggestion in analysis_result['suggestions']:
                        conversation.write(f"  ‚Ä¢ {suggestion}")

                # Smart mode activation message
                conversation.write("[bold cyan]üß† SMART MODE ACTIVATED![/bold cyan]")
                conversation.write("[green]‚ú® Repository is now loaded in memory! Ask ANY coding questions about this repo.[/green]")
                conversation.write("[green]üìù Examples: 'what files are there?', 'explain the main function', 'show me the architecture'[/green]")
                
                self.nova_system.sound_system.play_sound("success")
                
            else:
                # ‚úÖ ERROR DISPLAY WITH HELPFUL FEEDBACK
                error_msg = analysis_result.get('error', 'Unknown error occurred')
                conversation.write(f"[bold red]‚ùå Repository Analysis Failed[/bold red]")
                conversation.write(f"[red]üö´ Error: {error_msg}[/red]")
                conversation.write("[yellow]üí° Troubleshooting tips:[/yellow]")
                conversation.write("[yellow]  ‚Ä¢ Check if the repository URL is correct and accessible[/yellow]")
                conversation.write("[yellow]  ‚Ä¢ Ensure the repository is public or you have access[/yellow]")
                conversation.write("[yellow]  ‚Ä¢ Try again - temporary network issues may occur[/yellow]")
                
                # Track error
                self.conversation_history_for_copy.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "type": "error",
                    "content": f"Repository analysis failed: {error_msg}",
                    "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå ANALYSIS FAILED: {error_msg}"
                })
                
                self.nova_system.sound_system.play_sound("error")
                
        except Exception as e:
            # ‚úÖ EXCEPTION HANDLING WITH DETAILED ERROR INFO
            error_msg = f"Repository analysis exception: {str(e)}"
            conversation.write(f"[bold red]‚ùå Critical Analysis Error[/bold red]")
            conversation.write(f"[red]üö® Exception: {str(e)}[/red]")
            conversation.write("[yellow]üîß System recovery suggestions:[/yellow]")
            conversation.write("[yellow]  ‚Ä¢ Check your internet connection[/yellow]")
            conversation.write("[yellow]  ‚Ä¢ Verify repository permissions[/yellow]")
            conversation.write("[yellow]  ‚Ä¢ Report this error if it persists[/yellow]")
            
            # Track exception
            self.conversation_history_for_copy.append({
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "error",
                "content": error_msg,
                "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå ANALYSIS EXCEPTION: {error_msg}"
            })
            
            self.nova_system.sound_system.play_sound("error")
        return
     
     # üî• Repository Q&A Handling (COMPLETELY UPGRADED - CALLS QA ENGINE DIRECTLY)
     elif self.nova_system.is_repo_related_query(user_input):
        conversation.write(f"[bold cyan]üîç Repository Q&A Started[/bold cyan]")
        conversation.write(f"[blue]‚ùì Question: {user_input}[/blue]")
        
        try:
            # üî• CRITICAL FIX: Use QA engine directly for repository questions
            if hasattr(self.nova_system, 'qa_engine') and self.nova_system.qa_engine:
                conversation.write("[yellow]üî• Using QA Engine for repository question...[/yellow]")
                
                # Debug prints will appear here from your ask() method
                repo_response = await self.nova_system.github_analyzer.answer_repo_question(user_input)

                if isinstance(repo_response, dict):
                    response = repo_response.get('response', str(repo_response))
                    vector_search_used = repo_response.get('vector_search_used', False)
                    repo_context_used = repo_response.get('repo_context_used', False)
                else:
                    response = str(repo_response)
                    vector_search_used = False
                    repo_context_used = False
                
                # Enhanced response display
                github_header = self.nova_system.ultimate_emoji_system.create_smart_header(
                    "coding", False, "github_qa", f"github Q&A {user_input}"
                )
                conversation.write(f"[bold green]{github_header}[/bold green]")
                
                # Show debug info
                if vector_search_used:
                    conversation.write("[bold green]‚úÖ Vector search used - Repository-specific answer[/bold green]")
                elif repo_context_used:
                    conversation.write("[yellow]‚ö†Ô∏è Repository context used but no vector search[/yellow]")
                else:
                    conversation.write("[red]‚ùå No repository context used - Generic response[/red]")
                
                conversation.write(f"[green]{response}[/green]")
                
                # Track in conversation history
                self.conversation_history_for_copy.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "type": "ai",
                    "agent": "github_qa",
                    "content": response,
                    "enterprise": False,
                    "display": f"[{datetime.now().strftime('%H:%M:%S')}] üîç GITHUB Q&A: {user_input}"
                })
                
            # Fallback to GitHub analyzer
            elif hasattr(self.nova_system, 'github_analyzer') and self.nova_system.github_analyzer and self.nova_system.github_analyzer.has_active_repo():
                conversation.write("[yellow]üî• Using GitHub analyzer fallback...[/yellow]")
                response = await self.nova_system.github_analyzer.answer_repo_question(user_input)
                conversation.write(f"[green]{response}[/green]")
                
                # Track in conversation history
                self.conversation_history_for_copy.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "type": "ai",
                    "agent": "github_analyzer",
                    "content": response,
                    "enterprise": False,
                    "display": f"[{datetime.now().strftime('%H:%M:%S')}] üîç GITHUB ANALYZER: {user_input}"
                })
            
            else:
                error_msg = "No active repository or QA engine for repository questions"
                conversation.write(f"[bold red]‚ùå {error_msg}[/bold red]")
                conversation.write("[yellow]üí° Please analyze a GitHub repository first using a GitHub URL[/yellow]")
                
                # Track error
                self.conversation_history_for_copy.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "type": "error",
                    "content": error_msg,
                    "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå REPO Q&A ERROR: {error_msg}"
                })
                
        except Exception as e:
            error_msg = f"Repository Q&A failed: {str(e)}"
            conversation.write(f"[bold red]‚ùå {error_msg}[/bold red]")
            
            # Print debug traceback
            import traceback
            print(f"üî• REPO Q&A EXCEPTION: {e}")
            print(f"üî• TRACEBACK: {traceback.format_exc()}")
            
            # Track exception
            self.conversation_history_for_copy.append({
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "error", 
                "content": error_msg,
                "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå REPO Q&A EXCEPTION: {error_msg}"
            })
        
        return
        
     # ========== PROCESS WITH ENHANCED AI SYSTEM ==========
     start_time = time.time()
     try:
        # Get AI response using Enterprise API Manager
        response_data = await self.nova_system.get_response(user_input)

        # Handle rate limiting
        if response_data.get("rate_limited"):
            rate_limit_msg = response_data["response"]
            conversation.write(rate_limit_msg)
            
            # Track rate limit message
            self.conversation_history_for_copy.append({
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "system",
                "content": rate_limit_msg,
                "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ö†Ô∏è RATE LIMIT: {rate_limit_msg}"
            })
            
            self.nova_system.sound_system.play_sound("error")
            return

        # Play appropriate sound
        if response_data.get("error"):
            self.nova_system.sound_system.play_sound("error")
        else:
            self.nova_system.sound_system.play_sound("success")

        # Update UI with enhanced data
        response_time = time.time() - start_time
        self.response_time = f"{response_time:.2f}s"
        self.conversation_count = response_data.get("conversation_count", self.conversation_count)
        self.current_agent = response_data.get("agent_used", "general")

        # Show enhanced response
        agent_used = response_data.get("agent_used", "general")
        enterprise_features_used = response_data.get("enterprise_features_used", False)
        ai_response = response_data.get("response", "Processing your request...")

        agent_emoji = {
            "coding": "üîß", "career": "üíº", "business": "üìà",
            "medical": "üè•", "emotional": "üíô", "technical_architect": "üöÄ",
            "reasoning": "üß†", "creative": "üé®", "general": "ü§ñ"
        }.get(agent_used, "ü§ñ")

        # Enhanced response header
        ultimate_fancy_header = self.nova_system.ultimate_emoji_system.create_smart_header(
            agent_used, enterprise_features_used, "ai_processing", user_input
        )
        conversation.write(f"[bold green]{ultimate_fancy_header}[/bold green]")

        # ULTIMATE emoji-enhanced response
        ultimate_enhanced_response = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            ai_response, agent_used, user_input, "ai_processing", enterprise_features_used
        )
        conversation.write(ultimate_enhanced_response)

        # Track AI response for copy functionality
        ai_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": ai_timestamp,
            "type": "ai",
            "agent": agent_used,
            "content": ai_response,
            "enterprise": enterprise_features_used,
            "display": f"[{ai_timestamp}] {agent_emoji} NOVA ({agent_used.title()}): {ai_response}"
        })

        # Enhanced metadata with plugin info
        metadata_parts = []
        if enterprise_features_used:
            metadata_parts.append("üè¢ Enterprise")
        if response_data.get("file_context_used"):
            metadata_parts.append("üìÅ File Context")
        
        # Add plugin status to metadata
        if self.nova_system.plugin_manager:
            plugin_count = len(self.nova_system.plugin_manager.get_available_commands())
            metadata_parts.append(f"üß© {plugin_count} plugin commands")
            
        metadata_parts.extend([
            f"‚è±Ô∏è {self.response_time}",
            f"üéØ {agent_used}",
            f"üîß {response_data.get('available_providers', 0)} providers"
        ])

        metadata_text = f"{' | '.join(metadata_parts)}"
        conversation.write(f"[dim]{metadata_text}[/dim]")
        
        # Track metadata for copy
        self.conversation_history_for_copy.append({
            "timestamp": ai_timestamp,
            "type": "metadata",
            "content": metadata_text,
            "display": f" ‚îî‚îÄ {metadata_text}"
        })
        
        conversation.write("")

     except Exception as e:
        self.nova_system.sound_system.play_sound("error")
        error_msg = f"Enterprise Error: {str(e)}"
        conversation.write(f"[bold red]‚ùå {error_msg}[/bold red]")
        
        # Track error for copy
        self.conversation_history_for_copy.append({
            "timestamp": datetime.now().strftime("%H:%M:%S"),
            "type": "error",
            "content": error_msg,
            "display": f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå ERROR: {error_msg}"
        })

     # Update enhanced status
     self.update_enterprise_status()

     # ========== NEW ENTERPRISE BUTTON HANDLERS ==========

    @on(Button.Pressed, "#multi-candidate")
    async def on_multi_candidate(self):
        """Multi-candidate mode info"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        enterprise_status = self.nova_system.api_manager.get_enterprise_status()
        
        conversation.write(f"""[bold blue]üéØ Multi-Candidate System Information[/bold blue]

**üöÄ Advanced Multi-Candidate Features:**
‚Ä¢ Automatically detects complex queries requiring deep analysis
‚Ä¢ Generates multiple AI responses from different top-tier models
‚Ä¢ Uses advanced scoring to select the highest quality response
‚Ä¢ Combines responses from Groq Ultra, OpenRouter Premium, Together, NVIDIA , CHUTES , HuggingFace, and more
‚Ä¢ Ensures best possible answer for technical, business, and strategic questions
‚Ä¢ Reduces bias by leveraging diverse model perspectives


**‚ö° When Multi-Candidate Activates:**
‚Ä¢ Complex analysis requests (>15 words with analysis keywords)
‚Ä¢ Technical architecture and system design questions
‚Ä¢ Comprehensive business strategy queries
‚Ä¢ In-depth coding problems requiring multiple approaches
‚Ä¢ Detailed medical and health consultations

**üí° To Force Multi-Candidate Mode:**
Type: "multi-candidate analysis [your question]" or "force multi-candidate [your question]"

**üìä Current Status:**
‚Ä¢ Available Providers: {enterprise_status['performance_metrics']['available_providers']}
‚Ä¢ Total API Keys: {enterprise_status['performance_metrics']['total_api_keys']}
‚Ä¢ Quality Rating: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10
‚Ä¢ Multi-Candidate: {enterprise_status['enterprise_features']['multi_candidate_responses']}

Next complex query will automatically use multi-candidate processing!""")

    @on(Button.Pressed, "#api-keys")
    def on_api_keys_status(self):
        """API Keys status"""
        self.show_api_keys_status()

    @on(Button.Pressed, "#enterprise-status")
    def on_enterprise_status(self):
        """Enterprise status dashboard"""
        self.show_enterprise_status()

    @on(Button.Pressed, "#rate-limits")
    def on_rate_limits(self):
        """Rate limits information"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        rate_limiter = self.nova_system.api_manager.rate_limiter
        rate_check = rate_limiter.check_rate_limit(self.nova_system.user_id)
        
        conversation.write(f"""[bold blue]üîÑ Rate Limiting Information[/bold blue]

**üìä Current Usage:**
‚Ä¢ Requests this minute: {rate_check.get('usage', {}).get('requests_this_minute', 0)}/10
‚Ä¢ Requests this hour: {rate_check.get('usage', {}).get('requests_this_hour', 0)}/60
‚Ä¢ Status: {'‚úÖ Within Limits' if rate_check['allowed'] else '‚ö†Ô∏è Rate Limited'}

**üõ°Ô∏è Advanced Protection:**
‚Ä¢ Fair usage management active
‚Ä¢ Multi-key rotation prevents limits
‚Ä¢ Automatic provider switching
‚Ä¢ Request queuing and optimization

**üí° Rate Limit Features:**
‚Ä¢ Per-minute limits: 10 requests
‚Ä¢ Per-hour limits: 60 requests
‚Ä¢ Automatic key rotation when limited
‚Ä¢ Advanced users get higher limits
‚Ä¢ Real-time usage monitoring
‚Ä¢ Graceful degradation during high load
‚Ä¢ Priority access to premium models

Your usage is being managed optimally by the NOVA system!""")

    @on(Button.Pressed, "#plugins")
    async def on_plugins(self):
        """Plugin management"""
        await self.handle_plugin_command()

    @on(Button.Pressed, "#export-chat")
    async def on_export_chat(self):
        """Export chat functionality"""
        await self.handle_export_command("export markdown")

    @on(Button.Pressed, "#copy-chat")
    def on_copy_chat(self):
     """COMPLETELY FIXED: Copy actual conversation to clipboard"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     try:
        import pyperclip
        
        # ‚úÖ BUILD COMPLETE CONVERSATION TEXT WITH ACTUAL CONTENT
        conv_text = "NOVA CHAT - CONVERSATION EXPORT\n"
        conv_text += "=" * 60 + "\n\n"
        
        # Header information
        conv_text += f"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        conv_text += f"Session ID: {self.nova_system.current_session}\n"
        conv_text += f"Total Messages: {len([msg for msg in self.conversation_history_for_copy if msg['type'] in ['user', 'ai']])}\n"
        conv_text += f"Enterprise Mode: {'‚úÖ Active' if self.nova_system.enterprise_mode else '‚ùå Disabled'}\n"
        conv_text += f"Current Agent: {self.current_agent.title()}\n"
        conv_text += f"Available Providers: {len(getattr(self.nova_system.api_manager, 'available', []))}\n\n"
        
        conv_text += "CONVERSATION HISTORY:\n"
        conv_text += "-" * 40 + "\n\n"
        
        # ‚úÖ ADD ACTUAL CONVERSATION CONTENT
        if self.conversation_history_for_copy:
            for entry in self.conversation_history_for_copy:
                if entry['type'] == 'user':
                    conv_text += f"[{entry['timestamp']}] üë§ USER:\n"
                    conv_text += f"{entry['content']}\n\n"
                
                elif entry['type'] == 'ai':
                    agent_name = entry.get('agent', 'general').title()
                    enterprise_tag = " [ENTERPRISE]" if entry.get('enterprise') else ""
                    conv_text += f"[{entry['timestamp']}] ü§ñ NOVA ({agent_name}){enterprise_tag}:\n"
                    conv_text += f"{entry['content']}\n\n"
                
                elif entry['type'] == 'system':
                    conv_text += f"[{entry['timestamp']}] üîç SYSTEM:\n"
                    conv_text += f"{entry['content']}\n\n"
                
                elif entry['type'] == 'error':
                    conv_text += f"[{entry['timestamp']}] ‚ùå ERROR:\n"
                    conv_text += f"{entry['content']}\n\n"
                    
                elif entry['type'] == 'metadata':
                    # Include metadata as a note (optional)
                    conv_text += f"    ‚îî‚îÄ {entry['content']}\n\n"
        else:
            conv_text += "No conversation history found.\n"
            conv_text += "This might be because the conversation was cleared or just started.\n\n"
        
        # Footer
        conv_text += "-" * 40 + "\n"
        conv_text += f"End of Export | Total Characters: {len(conv_text)}\n"
        conv_text += f"Messages Copied: {len([msg for msg in self.conversation_history_for_copy if msg['type'] in ['user', 'ai']])}\n"
        conv_text += "Generated by NOVA Ultra Enterprise CLI v3.0\n"
        
        # ‚úÖ COPY THE COMPLETE CONVERSATION TO CLIPBOARD
        pyperclip.copy(conv_text)
        
        # Enhanced success message
        message_count = len([msg for msg in self.conversation_history_for_copy if msg['type'] in ['user', 'ai']])
        conversation.write(f"[green]‚úÖ Complete conversation copied to clipboard! ({len(conv_text)} characters)[/green]")
        conversation.write(f"[blue]üìä Copied {message_count} messages with full content[/blue]")
        conversation.write(f"[cyan]üíæ Includes: User messages, AI responses, system messages, and metadata[/cyan]")
        
        self.nova_system.sound_system.play_sound("success")
        
     except ImportError:
        conversation.write("[red]‚ùå pyperclip not installed. Install with: pip install pyperclip[/red]")
        conversation.write("[yellow]üí° Run: pip install pyperclip[/yellow]")
        self.nova_system.sound_system.play_sound("error")
     except Exception as e:
        conversation.write(f"[red]‚ùå Copy failed: {str(e)}[/red]")
        self.nova_system.sound_system.play_sound("error")

    @on(Button.Pressed, "#plugin-settings")
    def on_plugin_settings(self):
        """Plugin settings"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[blue]üîß Plugin settings panel coming soon![/blue]")

    @on(Button.Pressed, "#show-analytics")
    def on_show_analytics(self):
        """Show detailed analytics"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        enterprise_status = self.nova_system.api_manager.get_enterprise_status()
        
        analytics_text = f"""[bold cyan]üìä NOVA ULTRA - Enterprise Analytics Dashboard[/bold cyan]

[yellow]üè¢ System Performance:[/yellow]
‚Ä¢ Conversations: {self.conversation_count}
‚Ä¢ Available Providers: {enterprise_status['performance_metrics']['available_providers']} premium providers
‚Ä¢ Total API Keys: {enterprise_status['performance_metrics']['total_api_keys']} with rotation
‚Ä¢ Average Quality: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10
‚Ä¢ File Context: {'‚úÖ Active' if self.file_uploaded else '‚ùå Inactive'}
‚Ä¢ ML System: {self.ml_status}

[yellow]‚ö° Enterprise Features Status:[/yellow]
‚Ä¢ Multi-Candidate Responses: {enterprise_status['enterprise_features']['multi_candidate_responses']}
‚Ä¢ Intelligent Model Routing: {enterprise_status['enterprise_features']['intelligent_model_routing']}
‚Ä¢ Multi-Key Rotation: {enterprise_status['enterprise_features']['multi_key_rotation']}
‚Ä¢ Rate Limiting Protection: {enterprise_status['enterprise_features']['rate_limiting']}
‚Ä¢ Smart Enhancement Detection: {enterprise_status['enterprise_features']['smart_enhancement_detection']}

[yellow]üéØ Session Statistics:[/yellow]
‚Ä¢ Command History: {len(self.command_history)} commands
‚Ä¢ Current Agent: {self.current_agent.title()}
‚Ä¢ Response Time: {self.response_time}
‚Ä¢ Voice Usage: {'‚úÖ Used' if self.voice_active else '‚ùå Not Used'}

[green]üöÄ System Status: {enterprise_status['system_info']['status']}[/green]
[green]üìà Performance Tier: Top 1% Enterprise Grade[/green]"""
        
        conversation.write(analytics_text)

    # Keep all your existing button handlers (coding, career, business, etc.)
    # I'll include the key ones here:

    @on(Button.Pressed, "#agent-coding")
    async def on_coding_agent(self):
        """Switch to coding agent WITH SOUND"""
        await self.switch_agent("coding")

    @on(Button.Pressed, "#agent-career")
    async def on_career_agent(self):
        """Switch to career agent WITH SOUND"""
        await self.switch_agent("career")

    @on(Button.Pressed, "#agent-business")
    async def on_business_agent(self):
        """Switch to business agent WITH SOUND"""
        await self.switch_agent("business")

    @on(Button.Pressed, "#agent-medical")
    async def on_medical_agent(self):
        """Switch to medical agent WITH SOUND"""
        await self.switch_agent("medical")

    @on(Button.Pressed, "#agent-emotional")
    async def on_emotional_agent(self):
        """Switch to emotional agent WITH SOUND"""
        await self.switch_agent("emotional")

    @on(Button.Pressed, "#agent-technical")
    async def on_technical_agent(self):
        """Switch to technical architect WITH SOUND"""
        await self.switch_agent("technical")

    @on(Button.Pressed, "#voice-mode")
    async def on_voice_mode(self):
        """Activate voice mode WITH SOUND"""
        await self.activate_voice_mode()

    @on(Button.Pressed, "#web-search")
    async def on_web_search(self):
        """Prompt for web search WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[yellow]üîç Enter your search query with 'search ' prefix (e.g., 'search latest AI news')[/yellow]")

    @on(Button.Pressed, "#file-upload")
    async def on_file_upload(self):
        """Handle file upload WITH SOUND"""
        await self.handle_file_upload()

    @on(Button.Pressed, "#file-analysis")
    async def on_file_analysis(self):
        """Show current file analysis WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        if self.nova_system.current_file_context:
            conversation.write(f"[bold green]üìä Current File Context:[/bold green]\n{self.nova_system.current_file_context}")
        else:
            conversation.write("[yellow]üìÅ No file currently uploaded. Click 'FILE UPLOAD' to analyze a file.[/yellow]")

    @on(Button.Pressed, "#github-analysis")
    async def on_github_analysis(self):
        """GitHub analysis WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        if self.nova_system.github_analyzer.has_active_repo():
            conversation.write("[bold green]üìÇ Active Repository Available![/bold green]")
            conversation.write("[green]üí° You can ask questions about the active repository![/green]")
        else:
            conversation.write("[blue]üîó GitHub Analyzer: Enter a GitHub repository URL in the chat to analyze it![/blue]")
            conversation.write("[blue]üí° Example: https://github.com/username/repository[/blue]")

    @on(Button.Pressed, "#ml-insights")
    async def on_ml_insights(self):
        """ML insights WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        if self.nova_system.ml_manager:
            conversation.write(f"[bold green]üß† ML System Status: Active and Learning[/bold green]\n")
            conversation.write("‚Ä¢ Query Enhancement: ‚úÖ Active\n")
            conversation.write("‚Ä¢ Performance Optimization: ‚úÖ Active\n")
            conversation.write("‚Ä¢ Emotional Intelligence: ‚úÖ Active\n")
            conversation.write("‚Ä¢ Context Memory: ‚úÖ Active\n")
            conversation.write("‚Ä¢ Predictive Agent Selection: ‚úÖ Active\n")
        else:
            conversation.write("[yellow]üß† ML System: Not available in this configuration[/yellow]")

    @on(Button.Pressed, "#command-palette")
    def on_command_palette(self):
        """Show command palette WITH SOUND"""
        self.action_command_palette()

    @on(Button.Pressed, "#refresh-status")
    def on_refresh_status(self):
        """Refresh system status WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        self.update_enterprise_status()
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[green]üîÑ Enterprise status refreshed![/green]")

    @on(Button.Pressed, "#clear-chat")
    def on_clear_chat(self):
        """Clear chat WITH SOUND"""
        self.clear_conversation()

    @on(Button.Pressed, "#show-history")
    def on_show_history(self):
        """Show command history WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        if self.command_history:
            conversation.write("[bold blue]üìù Command History:[/bold blue]")
            for i, cmd in enumerate(self.command_history[-10:], 1):
                conversation.write(f"[dim]{i}.[/dim] {cmd}")
        else:
            conversation.write("[yellow]üìù No command history yet[/yellow]")

    # ========== NEW ENTERPRISE METHODS ==========

    def show_enterprise_status(self):
        """Display comprehensive enterprise status"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        enterprise_status = self.nova_system.api_manager.get_enterprise_status()
        
        status_text = f"""[bold cyan]üè¢ NOVA ULTRA - Enterprise Status Dashboard[/bold cyan]

[gold]System Information:[/gold]
‚Ä¢ Name: {enterprise_status['system_info']['name']}
‚Ä¢ Version: {enterprise_status['system_info']['version']}
‚Ä¢ Status: {enterprise_status['system_info']['status']}
‚Ä¢ Enterprise Features: {enterprise_status['system_info']['enterprise_features_loaded']}

[gold]üéØ Enterprise Features Status:[/gold]
‚Ä¢ Multi-Key Rotation: {enterprise_status['enterprise_features']['multi_key_rotation']}
‚Ä¢ Intelligent Model Routing: {enterprise_status['enterprise_features']['intelligent_model_routing']}
‚Ä¢ Rate Limiting: {enterprise_status['enterprise_features']['rate_limiting']}
‚Ä¢ Smart Enhancement Detection: {enterprise_status['enterprise_features']['smart_enhancement_detection']}
‚Ä¢ Multi-Candidate Responses: {enterprise_status['enterprise_features']['multi_candidate_responses']}

[gold]üìä Performance Metrics:[/gold]
‚Ä¢ Conversations: {self.conversation_count}
‚Ä¢ Available Providers: {enterprise_status['performance_metrics']['available_providers']}
‚Ä¢ Total API Keys: {enterprise_status['performance_metrics']['total_api_keys']}
‚Ä¢ Average Quality: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10
‚Ä¢ File Context: {'‚úÖ Active' if self.file_uploaded else '‚ùå Inactive'}
‚Ä¢ ML System: {self.ml_status}

[gold]‚ö° API Infrastructure:[/gold]
‚Ä¢ Top Providers: {', '.join(enterprise_status['performance_metrics'].get('top_providers', []))}
‚Ä¢ Quality Rating: {enterprise_status['performance_metrics']['average_quality_rating']:.1f}/10

[green]üöÄ All enterprise systems operational and performing at Top 1% level![/green]"""
        
        conversation.write(status_text)

    async def show_repository_registry(self):
     """üóÇÔ∏è Show all processed repositories"""
     try:
        if not REPOSITORY_REGISTRY_AVAILABLE:
            conversation.write("[red]‚ùå Repository registry system not available[/red]")
            return
            
        from ingest import list_processed_repositories
        
        registry = list_processed_repositories()
        repositories = registry.get("repositories", {})
        
        conversation = self.query_one("#conversation", RichLog)
        
        if repositories:
            header = self.nova_system.ultimate_emoji_system.create_smart_header(
                "general", self.enterprise_mode, "repository_registry", "repository management"
            )
            conversation.write(f"[bold green]{header}[/bold green]")
            
            enhanced_msg = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                f"Repository Registry - {len(repositories)} repositories processed with smart database management!",
                "general", "repository registry", "registry_display", self.enterprise_mode
            )
            conversation.write(f"[yellow]üìã {enhanced_msg}[/yellow]")
            
            for repo_url, repo_data in repositories.items():
                repo_name = repo_data.get('repo_name', 'Unknown')
                files_count = repo_data.get('files_count', 0)
                processed_at = repo_data.get('processed_at', 'Unknown')[:10]  # Date only
                db_path = repo_data.get('db_path', 'N/A')
                
                conversation.write(f"üóÇÔ∏è **{repo_name}** | Files: {files_count} | Date: {processed_at}")
                conversation.write(f"   üîó {repo_url}")
                conversation.write(f"   üìÅ {db_path}")
                conversation.write("")
            
            conversation.write(f"[green]üìä Total: {registry.get('total_repos', 0)} repositories | Last Active: {registry.get('last_active', 'None')[:50]}...[/green]")
        
        else:
            conversation.write("[yellow]üìã No repositories processed yet. Paste a GitHub URL to get started![/yellow]")
    
     except Exception as e:
        conversation = self.query_one("#conversation", RichLog)
        conversation.write(f"[red]‚ùå Repository registry error: {e}[/red]")

    def show_api_keys_status(self):
        """Show API Keys rotation status"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        # Get key manager status
        key_manager = self.nova_system.api_manager.key_manager
        
        status_text = "[bold cyan]üîë API Keys Rotation Status[/bold cyan]\n\n"
        
        for provider, keys in key_manager.provider_keys.items():
            status_text += f"**{provider}:**\n"
            for i, key_info in enumerate(keys):
                status = "üü¢ Active" if key_info['active'] else "üî¥ Inactive"
                rate_limited = "‚è∞ Rate Limited" if time.time() < key_info['rate_limited_until'] else "‚úÖ Available"
                status_text += f"  Key {i+1}: {status} | {rate_limited} | Used: {key_info['usage_count']} times\n"
            
            current_key_index = key_manager.current_key_index.get(provider, 0)
            status_text += f"  üéØ Currently using: Key {current_key_index + 1}\n\n"
        
        status_text += f"""**üîÑ Rotation Statistics:**
‚Ä¢ Total Keys: {sum(len(keys) for keys in key_manager.provider_keys.values())}
‚Ä¢ Active Providers: {len(key_manager.provider_keys)}
‚Ä¢ Automatic Rotation: ‚úÖ Enabled
‚Ä¢ Rate Limit Protection: ‚úÖ Active"""
        
        conversation.write(status_text)

    async def force_multi_candidate_response(self, original_query: str):
     """Force multi-candidate mode for any query - DISABLED FOR STABILITY"""
     conversation = self.query_one("#conversation", RichLog)
    
     # Extract the actual question
     if original_query.lower().startswith("multi-candidate analysis"):
        query = original_query[24:].strip()
     elif original_query.lower().startswith("force multi-candidate"):
        query = original_query[20:].strip()
     else:
        query = original_query
    
     if not query:
        conversation.write("[yellow]Multi-Candidate mode is disabled for system stability.[/yellow]")
        conversation.write("[blue]Using single response mode instead.[/blue]")
        return
    
     # ‚úÖ MULTI-CANDIDATE DISABLED - USE NORMAL PROCESSING
     conversation.write(f"[bold blue]üéØ Processing Query (Single Response Mode): {query}[/bold blue]")
     conversation.write("[blue]üß† NOVA Enhanced Processing...[/blue]")
    
     try:
        # ‚úÖ USE NORMAL get_response (NO MULTI-CANDIDATE FORCED)
        response_data = await self.nova_system.get_response(query)
        
        # Display results
        conversation.write("[bold green]üèÜ NOVA ENTERPRISE RESPONSE:[/bold green]")
        conversation.write(response_data.get("response", "Processing failed"))
        
        # Show standard metadata
        metadata = [
            "‚úÖ Single Response Mode",
            f"‚è±Ô∏è {response_data.get('response_time', 0):.2f}s",
            f"üîß {response_data.get('available_providers', 0)} providers"
        ]
        
        conversation.write(f"[dim]{' | '.join(metadata)}[/dim]")
        self.nova_system.sound_system.play_sound("success")
        
     except Exception as e:
        conversation.write(f"[bold red]‚ùå Processing error:[/bold red] {str(e)}")
        self.nova_system.sound_system.play_sound("error")

    async def handle_plugin_command(self):
     """üî• ULTIMATE Enhanced plugin management with FULL emoji system integration"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     try:
        plugins = self.nova_system.plugin_manager.get_plugin_list()
        available_commands = self.nova_system.plugin_manager.get_available_commands()
        
        # üéØ CREATE SMART EMOJI HEADER
        plugin_header = self.nova_system.ultimate_emoji_system.create_smart_header(
            "general", self.enterprise_mode, "plugin_management", "plugins"
        )
        conversation.write(f"[bold green]{plugin_header}[/bold green]")
        
        if plugins:
            # üåü ENHANCE PLUGIN LIST MESSAGE
            plugin_list_msg = f"Installed Plugins ({len(plugins)}) - Enterprise Plugin Ecosystem Ready!"
            enhanced_plugin_msg = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                plugin_list_msg, "general", "plugins", "plugin_listing", self.enterprise_mode
            )
            conversation.write(f"[yellow]üì¶ {enhanced_plugin_msg}[/yellow]")
            
            for plugin in plugins:
                status = "‚úÖ Active" if plugin.get("enabled") else "‚ùå Disabled"
                
                # üéØ ENHANCE PLUGIN STATUS MESSAGE
                plugin_status_msg = f"**{plugin['name']}** v{plugin['version']} - {status}"
                enhanced_status = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    plugin_status_msg, "general", "plugin status", "plugin_info", self.enterprise_mode
                )
                conversation.write(f"‚Ä¢ {enhanced_status}")
                
                conversation.write(f"  üìù {plugin['description']}")
                conversation.write(f"  üéØ Commands: {', '.join(plugin['commands'])}")
                conversation.write("")
            
            # üåü ENHANCE AVAILABLE COMMANDS MESSAGE  
            commands_msg = f"Available Commands ({len(available_commands)}) - Ready for Enterprise Usage!"
            enhanced_commands_msg = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                commands_msg, "general", "available commands", "command_listing", self.enterprise_mode
            )
            conversation.write(f"[cyan]‚ö° {enhanced_commands_msg}[/cyan]")
            
            # Group commands by plugin type
            command_groups = {
                "Weather": ["weather", "forecast", "alerts"],
                "Calculator": ["calc", "calculate", "emi", "convert", "currency"],  
                "Timer": ["timer", "remind", "pomodoro"],
                "Notes": ["note", "notes", "search_notes", "categories"],
                "AI Code": ["code", "debug", "optimize", "explain"],
                "Security": ["security", "password", "scan", "encrypt"],
                "API Hub": ["api", "connect", "github", "slack"]
            }
            
            for group_name, group_commands in command_groups.items():
                available_in_group = [cmd for cmd in group_commands if cmd in available_commands]
                if available_in_group:
                    # üéØ ENHANCE GROUP LISTING  
                    group_msg = f"**{group_name}:** {', '.join(available_in_group)}"
                    enhanced_group = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                        group_msg, "general", f"{group_name.lower()} commands", "command_group", self.enterprise_mode
                    )
                    conversation.write(f"üîπ {enhanced_group}")
            
            # üåü ENHANCE USAGE EXAMPLES MESSAGE
            usage_examples_msg = "Usage Examples - Professional Plugin Command Guide!"
            enhanced_usage = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                usage_examples_msg, "general", "usage examples", "plugin_help", self.enterprise_mode
            )
            conversation.write(f"[blue]üí° {enhanced_usage}[/blue]")
            
            conversation.write("‚Ä¢ `weather Mumbai` - Get real weather data")
            conversation.write("‚Ä¢ `calc 2 + 2 * 5` - Mathematical calculations")
            conversation.write("‚Ä¢ `emi 5000000 8.5 20` - Calculate EMI")
            conversation.write("‚Ä¢ `timer 25m` - Set a 25-minute timer")
            conversation.write("‚Ä¢ `note Important meeting` - Save a quick note")
            conversation.write("‚Ä¢ `password 16` - Generate secure password")
            conversation.write("‚Ä¢ `code create REST API` - AI code generation")
            
        else:
            # üéØ ENHANCE NO PLUGINS MESSAGE
            no_plugins_msg = "No plugins installed - Plugin system is ready for enterprise extensions!"
            enhanced_no_plugins = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                no_plugins_msg, "general", "no plugins", "plugin_ready", self.enterprise_mode
            )
            conversation.write(f"[yellow]üß© {enhanced_no_plugins}[/yellow]")
            
            ready_msg = "Plugin system is ready for extensions - Enterprise ecosystem awaits!"
            enhanced_ready = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                ready_msg, "general", "plugin ready", "system_ready", self.enterprise_mode
            )
            conversation.write(f"[blue]üí° {enhanced_ready}[/blue]")
            
        # üéØ ADD COMPLETION MESSAGE WITH EMOJI ENHANCEMENT
        completion_msg = f"Plugin management completed - {len(plugins)} plugins analyzed, {len(available_commands)} commands available!"
        enhanced_completion = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            completion_msg, "general", "plugin management completion", "management_success", self.enterprise_mode
        )
        conversation.write(f"[green]‚úÖ {enhanced_completion}[/green]")
        
        # ‚úÖ TRACK FOR COPY FUNCTIONALITY
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": timestamp,
            "type": "plugin_management",
            "content": f"Plugin management: {len(plugins)} plugins, {len(available_commands)} commands",
            "display": f"[{timestamp}] üß© PLUGIN SYSTEM: {len(plugins)} plugins managed, {len(available_commands)} commands available"
        })
            
     except Exception as e:
        # üéØ ENHANCE ERROR MESSAGE WITH EMOJI SYSTEM
        error_msg = f"Plugin system error: {str(e)} - Enterprise plugin management encountered an issue"
        enhanced_error = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            error_msg, "general", "plugin error", "system_error", self.enterprise_mode
        )
        conversation.write(f"[red]‚ùå {enhanced_error}[/red]")
        
        # ‚úÖ TRACK ERROR FOR COPY
        error_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": error_timestamp,
            "type": "plugin_error",
            "content": error_msg,
            "display": f"[{error_timestamp}] ‚ùå PLUGIN ERROR: {error_msg}"
        })

    async def handle_export_command(self, command: str):
        """Handle export commands"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        parts = command.split()
        fmt = parts[1] if len(parts) > 1 else "markdown"
        
        try:
            if hasattr(self.nova_system.export_system, "export_conversation_history"):
                result = await self.nova_system.export_system.export_conversation_history(
                    self.nova_system.user_id, fmt
                )
                if result and result.get("success"):
                    conversation.write(f"[green]‚úÖ Exported: {result['file_path']}[/green]")
                    self.nova_system.sound_system.play_sound("success")
                else:
                    conversation.write(f"[red]Export failed: {result.get('error', 'Unknown')}[/red]")
                    self.nova_system.sound_system.play_sound("error")
            else:
                conversation.write("[red]Export system not available or method missing.[/red]")
                self.nova_system.sound_system.play_sound("error")
        except Exception as e:
            conversation.write(f"[red]Export error: {e}[/red]")
            self.nova_system.sound_system.play_sound("error")

    # ========== NEW ENTERPRISE ACTION HANDLERS ==========

    def action_enterprise_status(self):
        """Show enterprise status (keyboard shortcut)"""
        self.show_enterprise_status()

    def action_multi_candidate(self):
        """Multi-candidate information (keyboard shortcut)"""
        asyncio.create_task(self.on_multi_candidate())

    def action_api_keys_status(self):
        """API keys status (keyboard shortcut)"""
        self.show_api_keys_status()

    def action_export_chat(self):
        """Export chat (keyboard shortcut)"""
        asyncio.create_task(self.handle_export_command("export markdown"))

    def action_copy_chat(self):
        """Copy chat (keyboard shortcut)"""
        self.on_copy_chat()

    def action_plugin_manager(self):
        """Plugin manager (keyboard shortcut)"""
        asyncio.create_task(self.handle_plugin_command())

    # Keep all existing methods from your original class
    # (switch_agent, activate_voice_mode, handle_file_upload, etc.)
    
    async def switch_agent(self, agent_name: str):
        """Switch to specific agent WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        self.current_agent = agent_name
        conversation = self.query_one("#conversation", RichLog)
        
        agent_names = {
            "coding": "üîß Coding Expert",
            "career": "üíº Career Coach", 
            "business": "üìà Business Guru",
            "medical": "üè• Medical Advisor",
            "emotional": "üíô Emotional Support",
            "technical": "üöÄ Tech Architect"
        }
        
        agent_display = agent_names.get(agent_name, "ü§ñ General")
        conversation.write(f"[bold green]üîÑ Switched to {agent_display}[/bold green]")
        self.update_enterprise_status()

    # Keep all your existing utility methods (activate_voice_mode, handle_file_upload, etc.)
    # I'm including the key ones here:

    async def activate_voice_mode(self):
     """üî• ULTIMATE Enhanced voice mode with SINGLE PIPELINE"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     # üéØ CREATE SMART EMOJI HEADER FOR VOICE MODE
     voice_header = self.nova_system.ultimate_emoji_system.create_smart_header(
        "general", self.enterprise_mode, "voice_mode", "voice activation"
     )
     conversation.write(f"[bold green]{voice_header}[/bold green]")
    
     # üåü ENHANCE VOICE ACTIVATION MESSAGE
     activation_msg = "Voice Mode Activated - Enterprise voice processing listening for professional input!"
     enhanced_activation = self.nova_system.ultimate_emoji_system.enhance_response_smart(
        activation_msg, "general", "voice activation", "voice_ready", self.enterprise_mode
     )
     conversation.write(f"[bold blue]üé§ {enhanced_activation}[/bold blue]")
    
     self.voice_active = True
     self.update_enterprise_status()
    
     # ‚úÖ CRITICAL FIX: Ensure voice engine is actually ON before processing
     voice_was_on = self.nova_system.voice_system.voice_mode
     if not voice_was_on:
        print("üîß Turning ON voice engine...")
        self.nova_system.voice_system.toggle_voice_mode()
    
     try:
        # ‚úÖ ADDED: Debug voice status before processing
        voice_status = self.nova_system.voice_system.get_voice_status()
        print(f"üîç Voice status before processing: {voice_status}")
        
        voice_result = await self.nova_system.process_voice_input()
        
        if voice_result.get("error"):
            # üéØ ENHANCE VOICE ERROR MESSAGE WITH EMOJI SYSTEM
            error_msg = f"Voice Error: {voice_result['error']} - Enterprise voice processing encountered an issue"
            enhanced_error = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                error_msg, "general", "voice error", "voice_failure", self.enterprise_mode
            )
            conversation.write(f"[bold red]‚ùå {enhanced_error}[/bold red]")
            
            # ‚úÖ TRACK ERROR FOR COPY
            error_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": error_timestamp,
                "type": "voice_error", 
                "content": error_msg,
                "display": f"[{error_timestamp}] ‚ùå VOICE ERROR: {error_msg}"
            })
            
            self.nova_system.sound_system.play_sound("error")
            
        elif voice_result.get("success"):  # ‚úÖ FIXED: Check for success properly
            voice_input = voice_result.get("voice_input", "")
            ai_response_data = voice_result.get("ai_response", {})
            
            # üåü ENHANCE VOICE INPUT RECOGNITION MESSAGE
            input_msg = f"You said: '{voice_input}' - Voice recognition successful with enterprise accuracy!"
            enhanced_input = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                input_msg, "general", "voice input", "voice_recognition", self.enterprise_mode
            )
            conversation.write(f"[bold blue]üé§ {enhanced_input}[/bold blue]")
            
            # ‚úÖ TRACK VOICE INPUT FOR COPY
            input_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": input_timestamp,
                "type": "voice_input",
                "content": voice_input,
                "display": f"[{input_timestamp}] üé§ VOICE INPUT: {voice_input}"
            })
            
            if ai_response_data.get("response"):
                agent_used = ai_response_data.get("agent_used", "general")
                agent_emoji = {
                    "coding": "üîß", "career": "üíº", "business": "üìà",
                    "medical": "üè•", "emotional": "üíô", "technical_architect": "üöÄ",
                    "general": "ü§ñ"
                }.get(agent_used, "ü§ñ")
                
                # üéØ ENHANCE AI VOICE RESPONSE WITH EMOJI SYSTEM
                voice_response = ai_response_data['response']
                enhanced_voice_response = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    voice_response, agent_used, voice_input, "voice_response", self.enterprise_mode
                )
                
                # üåü CREATE ENHANCED AGENT HEADER FOR VOICE RESPONSE
                agent_voice_header = self.nova_system.ultimate_emoji_system.create_smart_header(
                    agent_used, self.enterprise_mode, "voice_response", voice_input
                )
                
                # ‚úÖ SINGLE PIPELINE: Chatbox is source of truth
                conversation.write(f"[bold green]{agent_voice_header}:[/bold green]")
                conversation.write(enhanced_voice_response)
                
                # ‚úÖ TRACK AI VOICE RESPONSE FOR COPY
                response_timestamp = datetime.now().strftime("%H:%M:%S")
                self.conversation_history_for_copy.append({
                    "timestamp": response_timestamp,
                    "type": "voice_ai_response",
                    "agent": agent_used,
                    "content": voice_response,
                    "enterprise": self.enterprise_mode,
                    "display": f"[{response_timestamp}] {agent_emoji} NOVA VOICE ({agent_used.title()}): {voice_response}"
                })
                
                # ‚úÖ CRITICAL FIX: TTS AFTER chatbox display (Single Pipeline)
                if voice_result.get("tts_needed"):
                    try:
                        conversation.write("[dim]üó£Ô∏è Speaking response...[/dim]")
                        await self.nova_system.voice_system.speak(voice_response)
                        conversation.write("[dim green]‚úÖ Voice response completed[/dim green]")
                    except Exception as tts_error:
                        conversation.write(f"[dim red]‚ö†Ô∏è TTS error: {tts_error}[/dim red]")
                
                # üéØ ADD VOICE COMPLETION MESSAGE
                completion_msg = f"Voice interaction completed - Speech processed with {agent_used} agent using enterprise voice AI!"
                enhanced_completion = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    completion_msg, "general", "voice completion", "voice_success", self.enterprise_mode
                )
                conversation.write(f"[cyan]üéØ {enhanced_completion}[/cyan]")
                
                self.nova_system.sound_system.play_sound("success")
            else:
                # üåü ENHANCE NO RESPONSE MESSAGE
                no_response_msg = "Voice input processed but no AI response generated - Please try speaking more clearly"
                enhanced_no_response = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    no_response_msg, "general", "no voice response", "voice_incomplete", self.enterprise_mode
                )
                conversation.write(f"[yellow]‚ö†Ô∏è {enhanced_no_response}[/yellow]")
                self.nova_system.sound_system.play_sound("error")
        else:
            # ‚úÖ ADDED: Handle case where voice_result exists but has no clear success/error
            conversation.write("[yellow]‚ö†Ô∏è Voice processing completed but result unclear - Please try again[/yellow]")
            self.nova_system.sound_system.play_sound("error")
                
     except Exception as e:
        # üéØ ENHANCE EXCEPTION MESSAGE WITH EMOJI SYSTEM
        exception_msg = f"Voice processing failed: {str(e)} - Enterprise voice system encountered critical error"
        enhanced_exception = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            exception_msg, "general", "voice processing failed", "voice_system_error", self.enterprise_mode
        )
        conversation.write(f"[bold red]‚ùå {enhanced_exception}[/bold red]")
        
        # ‚úÖ TRACK EXCEPTION FOR COPY
        exception_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": exception_timestamp,
            "type": "voice_exception",
            "content": exception_msg,
            "display": f"[{exception_timestamp}] ‚ùå VOICE EXCEPTION: {exception_msg}"
        })
        
        self.nova_system.sound_system.play_sound("error")
        
     finally:
        # ‚úÖ CRITICAL FIX: Properly clean up voice mode
        self.voice_active = False
        
        # Only turn off voice mode if we turned it on
        if not voice_was_on and self.nova_system.voice_system.voice_mode:
            print("üîß Turning OFF voice engine...")
            self.nova_system.voice_system.toggle_voice_mode()
        
        # üåü ENHANCE VOICE DEACTIVATION MESSAGE
        deactivation_msg = "Voice Mode Deactivated - Enterprise voice processing session completed successfully!"
        enhanced_deactivation = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            deactivation_msg, "general", "voice deactivated", "voice_complete", self.enterprise_mode
        )
        conversation.write(f"[blue]üîá {enhanced_deactivation}[/blue]")
        
        # ‚úÖ TRACK DEACTIVATION FOR COPY
        deactivation_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": deactivation_timestamp,
            "type": "voice_deactivation", 
            "content": "Voice mode deactivated",
            "display": f"[{deactivation_timestamp}] üîá VOICE MODE: Deactivated"
        })
        
        self.update_enterprise_status()
        print("‚úÖ Voice mode cleanup completed")

    async def handle_file_upload(self):
     """üî• ULTIMATE Enhanced file upload with FULL emoji system integration"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     # üéØ CREATE SMART EMOJI HEADER FOR FILE UPLOAD
     file_header = self.nova_system.ultimate_emoji_system.create_smart_header(
        "general", self.enterprise_mode, "file_upload", "file upload"
     )
     conversation.write(f"[bold green]{file_header}[/bold green]")
    
     # üåü ENHANCE FILE DIALOG MESSAGE
     dialog_msg = "Opening file dialog - Enterprise file analysis system ready!"
     enhanced_dialog = self.nova_system.ultimate_emoji_system.enhance_response_smart(
        dialog_msg, "general", "file dialog", "file_selection", self.enterprise_mode
     )
     conversation.write(f"[yellow]üìÅ {enhanced_dialog}[/yellow]")
    
     try:
        result = await self.nova_system.upload_and_analyze_file()
        
        if result.get("error"):
            # üéØ ENHANCE ERROR MESSAGE WITH EMOJI SYSTEM
            error_msg = f"Upload Error: {result['error']} - File analysis system encountered an issue"
            enhanced_error = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                error_msg, "general", "upload error", "file_error", self.enterprise_mode
            )
            conversation.write(f"[bold red]‚ùå {enhanced_error}[/bold red]")
            
            # ‚úÖ TRACK ERROR FOR COPY
            error_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": error_timestamp,
                "type": "file_error",
                "content": error_msg,
                "display": f"[{error_timestamp}] ‚ùå FILE ERROR: {error_msg}"
            })
            
            self.nova_system.sound_system.play_sound("error")
        else:
            file_analysis = result.get("file_analysis", {})
            
            # üåü ENHANCE SUCCESS MESSAGE
            success_msg = f"File Uploaded Successfully! Professional file analysis completed for enterprise usage."
            enhanced_success = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                success_msg, "general", "file upload success", "file_success", self.enterprise_mode
            )
            conversation.write(f"[bold green]‚úÖ {enhanced_success}[/bold green]")
            
            # üéØ ENHANCE FILE INFO MESSAGES
            file_name_msg = f"File: {file_analysis.get('file_name', 'Unknown')} - Ready for enterprise analysis"
            enhanced_file_name = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                file_name_msg, "general", "file name", "file_info", self.enterprise_mode
            )
            conversation.write(f"[blue]üìÑ {enhanced_file_name}[/blue]")
            
            size_msg = f"Size: {file_analysis.get('file_size', 0)} bytes - Optimal for processing"
            enhanced_size = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                size_msg, "general", "file size", "file_metrics", self.enterprise_mode
            )
            conversation.write(f"[blue]üìè {enhanced_size}[/blue]")
            
            type_msg = f"Type: {file_analysis.get('file_type', 'Unknown')} - Professional format detected"
            enhanced_type = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                type_msg, "general", "file type", "file_format", self.enterprise_mode
            )
            conversation.write(f"[blue]üìù {enhanced_type}[/blue]")
            
            if file_analysis.get('lines'):
                lines_msg = f"Lines: {file_analysis['lines']} - Code structure analyzed"
                enhanced_lines = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    lines_msg, "general", "file lines", "code_analysis", self.enterprise_mode
                )
                conversation.write(f"[blue]üìä {enhanced_lines}[/blue]")
            
            # üåü ENHANCE CONTENT PREVIEW MESSAGE
            preview_msg = "Content Preview - Enterprise file content analysis ready!"
            enhanced_preview = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                preview_msg, "general", "content preview", "file_content", self.enterprise_mode
            )
            conversation.write(f"[blue]üìã {enhanced_preview}[/blue]")
            conversation.write(f"{file_analysis.get('content', '')[:300]}...")
            
            # üéØ ENHANCE FILE CONTEXT ACTIVATION MESSAGE
            context_msg = "File context is now active - Ask intelligent questions about your file with enterprise AI analysis!"
            enhanced_context = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                context_msg, "general", "file context active", "context_ready", self.enterprise_mode
            )
            conversation.write(f"[green]üí° {enhanced_context}[/green]")
            
            # üåü ADD FILE COMPLETION MESSAGE
            completion_msg = f"File analysis completed - {file_analysis.get('file_name', 'File')} ready for enterprise AI queries!"
            enhanced_completion = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                completion_msg, "general", "file upload completion", "upload_success", self.enterprise_mode
            )
            conversation.write(f"[cyan]üéØ {enhanced_completion}[/cyan]")
            
            # ‚úÖ TRACK SUCCESS FOR COPY FUNCTIONALITY
            success_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": success_timestamp,
                "type": "file_upload",
                "content": f"File uploaded: {file_analysis.get('file_name', 'Unknown')} ({file_analysis.get('file_size', 0)} bytes)",
                "display": f"[{success_timestamp}] üìÅ FILE UPLOAD: {file_analysis.get('file_name', 'Unknown')} successfully analyzed"
            })
            
            self.file_uploaded = True
            self.nova_system.sound_system.play_sound("success")
            self.update_enterprise_status()
            
     except Exception as e:
        # üéØ ENHANCE EXCEPTION MESSAGE WITH EMOJI SYSTEM
        exception_msg = f"File upload failed: {str(e)} - Enterprise file processing system encountered critical error"
        enhanced_exception = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            exception_msg, "general", "upload failed", "system_failure", self.enterprise_mode
        )
        conversation.write(f"[bold red]‚ùå {enhanced_exception}[/bold red]")
        
        # ‚úÖ TRACK EXCEPTION FOR COPY
        exception_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": exception_timestamp,
            "type": "file_exception",
            "content": exception_msg,
            "display": f"[{exception_timestamp}] ‚ùå FILE EXCEPTION: {exception_msg}"
        })
        
        self.nova_system.sound_system.play_sound("error")

    async def handle_web_search(self, query: str):
     """üî• ULTIMATE Enhanced web search with FULL emoji system integration"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     # üéØ CREATE SMART EMOJI HEADER FOR WEB SEARCH
     search_header = self.nova_system.ultimate_emoji_system.create_smart_header(
        "general", self.enterprise_mode, "web_search", f"search {query}"
     )
     conversation.write(f"[bold green]{search_header}[/bold green]")
    
     # üåü ENHANCE SEARCH INITIATION MESSAGE
     search_msg = f"Searching for: {query} - Enterprise web search engine processing your query!"
     enhanced_search = self.nova_system.ultimate_emoji_system.enhance_response_smart(
        search_msg, "general", f"search {query}", "search_initiated", self.enterprise_mode
     )
     conversation.write(f"[yellow]üîç {enhanced_search}[/yellow]")
    
     # ‚úÖ TRACK SEARCH INITIATION FOR COPY
     search_timestamp = datetime.now().strftime("%H:%M:%S")
     self.conversation_history_for_copy.append({
        "timestamp": search_timestamp,
        "type": "web_search_start",
        "content": f"Web search initiated: {query}",
        "display": f"[{search_timestamp}] üîç WEB SEARCH: Started searching for '{query}'"
     })
    
     try:
        search_result = await self.nova_system.search_web(query)
        
        if search_result.get("error"):
            # üéØ ENHANCE SEARCH ERROR MESSAGE WITH EMOJI SYSTEM
            error_msg = f"Search Error: {search_result['error']} - Enterprise web search encountered an issue"
            enhanced_error = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                error_msg, "general", "search error", "search_failure", self.enterprise_mode
            )
            conversation.write(f"[bold red]‚ùå {enhanced_error}[/bold red]")
            
            # ‚úÖ TRACK ERROR FOR COPY
            error_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": error_timestamp,
                "type": "search_error",
                "content": error_msg,
                "display": f"[{error_timestamp}] ‚ùå SEARCH ERROR: {error_msg}"
            })
            
            self.nova_system.sound_system.play_sound("error")
        else:
            # üåü ENHANCE SEARCH SUCCESS MESSAGE
            results_count = search_result.get("results_count", 0)
            success_msg = f"Web search completed successfully - Found {results_count} enterprise-grade results for your query!"
            enhanced_success = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                success_msg, "general", f"search success {query}", "search_success", self.enterprise_mode
            )
            conversation.write(f"[bold green]‚úÖ {enhanced_success}[/bold green]")
            
            # üéØ ENHANCE SEARCH RESULTS PRESENTATION
            formatted_response = search_result.get("formatted_response", "No results found")
            if formatted_response != "No results found":
                # Add enterprise enhancement to the search results
                results_header_msg = f"Enterprise Web Search Results - Professional information curated for your query!"
                enhanced_results_header = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    results_header_msg, "general", "search results", "results_presentation", self.enterprise_mode
                )
                conversation.write(f"[cyan]üìä {enhanced_results_header}[/cyan]")
                conversation.write("")
                
                # Display the formatted response with enhancement
                enhanced_formatted_response = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    "Search results processed and formatted for enterprise analysis",
                    "general", f"results for {query}", "results_ready", self.enterprise_mode
                )
                conversation.write(formatted_response)
                conversation.write("")
                conversation.write(f"[blue]üí° {enhanced_formatted_response}[/blue]")
            else:
                # üåü ENHANCE NO RESULTS MESSAGE
                no_results_msg = f"No results found for '{query}' - Try different search terms or check spelling"
                enhanced_no_results = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                    no_results_msg, "general", "no results", "search_empty", self.enterprise_mode
                )
                conversation.write(f"[yellow]‚ö†Ô∏è {enhanced_no_results}[/yellow]")
            
            # üéØ ADD SEARCH COMPLETION MESSAGE
            completion_msg = f"Web search session completed - Query '{query}' processed with enterprise search intelligence!"
            enhanced_completion = self.nova_system.ultimate_emoji_system.enhance_response_smart(
                completion_msg, "general", "search completion", "search_complete", self.enterprise_mode
            )
            conversation.write(f"[green]üéØ {enhanced_completion}[/green]")
            
            # ‚úÖ TRACK SUCCESS FOR COPY FUNCTIONALITY
            success_timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history_for_copy.append({
                "timestamp": success_timestamp,
                "type": "web_search_success",
                "content": f"Web search completed: '{query}' - {results_count} results found",
                "display": f"[{success_timestamp}] üåê SEARCH SUCCESS: Found {results_count} results for '{query}'"
            })
            
            self.nova_system.sound_system.play_sound("success")
                
     except Exception as e:
        # üéØ ENHANCE EXCEPTION MESSAGE WITH EMOJI SYSTEM
        exception_msg = f"Search failed: {str(e)} - Enterprise web search system encountered critical error"
        enhanced_exception = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            exception_msg, "general", "search failed", "search_system_error", self.enterprise_mode
        )
        conversation.write(f"[bold red]‚ùå {enhanced_exception}[/bold red]")
        
        # üåü ADD SEARCH TROUBLESHOOTING MESSAGE
        troubleshoot_msg = "Search system troubleshooting - Please check network connection and try again with enterprise support"
        enhanced_troubleshoot = self.nova_system.ultimate_emoji_system.enhance_response_smart(
            troubleshoot_msg, "general", "search troubleshoot", "search_help", self.enterprise_mode
        )
        conversation.write(f"[blue]üí° {enhanced_troubleshoot}[/blue]")
        
        # ‚úÖ TRACK EXCEPTION FOR COPY
        exception_timestamp = datetime.now().strftime("%H:%M:%S")
        self.conversation_history_for_copy.append({
            "timestamp": exception_timestamp,
            "type": "search_exception",
            "content": exception_msg,
            "display": f"[{exception_timestamp}] ‚ùå SEARCH EXCEPTION: {exception_msg}"
        })
        
        self.nova_system.sound_system.play_sound("error")

    def clear_conversation(self):
     """Clear the conversation log AND copy history WITH SOUND"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
     conversation.clear()
     conversation.write("[bold green]üßπ Chat cleared! Ready for a fresh enterprise conversation.[/bold green]")
    
     # ‚úÖ CLEAR COPY HISTORY TOO (MOST IMPORTANT ADDITION)
     self.conversation_history_for_copy.clear()
    
     # Reset file context
     self.nova_system.current_file_context = None
     self.file_uploaded = False
     
     # Update status
     self.update_enterprise_status()
    
     # ‚úÖ Show confirmation of what was cleared
     conversation.write("[cyan]üíæ Conversation history and copy cache cleared[/cyan]")

    # Keep all existing action methods (action_help_screen, etc.)
    def action_command_palette(self):
        """Show command palette (RESTORED) WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        self.push_screen(CommandPalette(), self.handle_command_result)

    def handle_command_result(self, command_id):
        """Handle command palette result (RESTORED) WITH SOUND"""
        if not command_id:
            return
        
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        
        if command_id.startswith("agent-"):
            agent_name = command_id.replace("agent-", "")
            asyncio.create_task(self.switch_agent(agent_name))
        elif command_id == "voice-mode":
            asyncio.create_task(self.activate_voice_mode())
        elif command_id == "web-search":
            conversation.write("[yellow]üîç Enter search query in the input box with 'search ' prefix[/yellow]")
        elif command_id == "upload-file":
            asyncio.create_task(self.handle_file_upload())
        elif command_id == "plugins":
            asyncio.create_task(self.handle_plugin_command())
        elif command_id == "export-chat":
            asyncio.create_task(self.handle_export_command("export markdown"))
        elif command_id == "clear-chat":
            self.clear_conversation()
        elif command_id == "show-status":
            self.show_enterprise_status()
        elif command_id == "help":
            self.action_help_screen()

    def action_help_screen(self):
     """Enhanced help screen with smart shortcuts discovery integration"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
    
     # Get the smart shortcuts tutorial from shortcut manager
     help_text = self.shortcut_manager.get_shortcut_tutorial()
     conversation.write(help_text)
     
     conversation.write("")
     conversation.write("[bold cyan]üè¢ NOVA CLI - Additional Help & Commands[/bold cyan]")
    
     conversation.write("")
     conversation.write("[yellow]üè¢ Enterprise Commands:[/yellow]")
     conversation.write("‚Ä¢ [bold]enterprise status[/bold] - Comprehensive system overview")
     conversation.write("‚Ä¢ [bold]multi-candidate analysis [question][/bold] - Force enhanced mode")
     conversation.write("‚Ä¢ [bold]api keys status[/bold] - Check API key rotation")
     conversation.write("‚Ä¢ [bold]export markdown[/bold] - Export conversation")
     conversation.write("‚Ä¢ [bold]plugins[/bold] - Manage plugin system")
    
     conversation.write("")
     conversation.write("[yellow]üéØ Quick Start Guide:[/yellow]")
     conversation.write("‚Ä¢ Type your question and press Enter")
     conversation.write("‚Ä¢ Use agent buttons to switch AI personalities")
     conversation.write("‚Ä¢ Upload files for contextual analysis")
     conversation.write("‚Ä¢ Voice mode for hands-free interaction")
    
     conversation.write("")
     conversation.write("[yellow]üí° Pro Tips:[/yellow]")
     conversation.write("‚Ä¢ Complex queries automatically trigger multi-candidate responses")
     conversation.write("‚Ä¢ File context persists across conversations")
     conversation.write("‚Ä¢ GitHub URLs are automatically analyzed")
     conversation.write("‚Ä¢ Voice responses are optimized for clarity")
     conversation.write("‚Ä¢ Advanced features provide top 1% performance")
    
     conversation.write("")
     conversation.write("[bold yellow]üéØ Want more help? Try these discovery commands:[/bold yellow]")
     conversation.write("‚Ä¢ [bold]shortcuts[/bold] - Quick shortcuts reference")
     conversation.write("‚Ä¢ [bold]keys help[/bold] - Interactive shortcuts guide")  
     conversation.write("‚Ä¢ [bold]show keys[/bold] - Formatted keybindings table")
     conversation.write("‚Ä¢ [bold]tips[/bold] - Shortcut tips and pro tricks")
     conversation.write("‚Ä¢ [bold]plugins[/bold] - Plugin system management")
    
     conversation.write("")
     conversation.write("[green]üöÄ Ready to experience the ultimate AI CLI with smart shortcuts! üåü[/green]")
     # Keep all existing action methods unchanged
     # (action_clear_conversation, action_voice_mode, etc.)

    def show_shortcuts_guide(self):
     """Show complete shortcuts guide"""
     self.nova_system.sound_system.play_sound("success")
     conversation = self.query_one("#conversation", RichLog)
     shortcuts_guide = self.shortcut_manager.get_shortcut_tutorial()
     conversation.write(shortcuts_guide)

    def show_interactive_shortcuts_tutorial(self):
     """Show interactive shortcuts tutorial"""
     self.nova_system.sound_system.play_sound("success")
     conversation = self.query_one("#conversation", RichLog)
     conversation.write("[bold green]üéØ Loading Interactive Shortcuts Tutorial...[/bold green]")
     tutorial = self.shortcut_manager.get_shortcut_tutorial()
     conversation.write(tutorial)
     conversation.write("")
     conversation.write("[yellow]üí° Try pressing Ctrl+P right now to see the Command Palette in action![/yellow]")

    def show_keybindings_table(self):
     """Show formatted keybindings table"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
     table = self.shortcut_manager.get_keybindings_table()
     conversation.write(table)

    def show_shortcut_tips(self):
     """Show shortcut tips and tricks"""
     self.nova_system.sound_system.play_sound("click")
     conversation = self.query_one("#conversation", RichLog)
     tips = """[bold cyan]üî• NOVA CLI - Shortcut Tips & Tricks[/bold cyan]

[yellow]üéØ DISCOVERY TIPS:[/yellow]
‚Ä¢ Watch the rotating hints - they reveal hidden shortcuts
‚Ä¢ F1 is always your shortcut discovery friend
‚Ä¢ Command Palette (Ctrl+P) shows all available actions
‚Ä¢ Most shortcuts work from anywhere in the app

[yellow]‚ö° POWER USER TRICKS:[/yellow]
‚Ä¢ Ctrl+P then type what you want (fuzzy search works!)
‚Ä¢ Use Up/Down arrows to recall previous commands
‚Ä¢ Enterprise shortcuts (Ctrl+E, Ctrl+M, Ctrl+K) unlock advanced features
‚Ä¢ Voice mode (Ctrl+V) works hands-free

[yellow]üß© HIDDEN GEMS:[/yellow]
‚Ä¢ Type 'shortcuts' for instant help (no need to remember F1)
‚Ä¢ Escape key always brings focus back to input
‚Ä¢ F5 refreshes system status instantly
‚Ä¢ Plugin commands work directly (weather, calc, etc.)

[blue]üí° MEMORIZATION HACK:[/blue]
‚Ä¢ Ctrl+P (Palette) - Most important, remember this one!
‚Ä¢ Ctrl+E (Enterprise) - For advanced status
‚Ä¢ Ctrl+O (Output/Copy) - For copying chat
‚Ä¢ F1 (Function/Help) - Universal help key

[green]üöÄ You're now a shortcuts discovery expert![/green]"""
     conversation.write(tips)

    def action_clear_conversation(self):
        """Clear conversation WITH SOUND"""
        self.clear_conversation()

    def action_voice_mode(self):
        """Voice mode action WITH SOUND"""
        asyncio.create_task(self.activate_voice_mode())

    def action_search_web(self):
        """Web search action WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[yellow]üîç Enter your search query with 'search ' prefix[/yellow]")

    def action_upload_file(self):
        """File upload action WITH SOUND"""
        asyncio.create_task(self.handle_file_upload())

    def action_system_status(self):
        """Enhanced system status WITH SOUND"""
        self.show_enterprise_status()

    def action_github_analyzer(self):
        """GitHub analyzer action WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[blue]üîó GitHub Analyzer: Enter a GitHub repository URL in the chat to analyze it![/blue]")

    def action_focus_input(self):
        """Focus on input WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        self.query_one("#user-input", Input).focus()

    def action_history_up(self):
        """Navigate command history up WITH SOUND"""
        if self.command_history and self.history_index < len(self.command_history) - 1:
            self.nova_system.sound_system.play_sound("click")
            self.history_index += 1
            cmd = self.command_history[-(self.history_index + 1)]
            input_widget = self.query_one("#user-input", Input)
            input_widget.value = cmd
            input_widget.cursor_position = len(cmd)

    def action_history_down(self):
        """Navigate command history down WITH SOUND"""
        if self.command_history and self.history_index > -1:
            self.nova_system.sound_system.play_sound("click")
            if self.history_index == 0:
                self.history_index = -1
                input_widget = self.query_one("#user-input", Input)
                input_widget.value = ""
            else:
                self.history_index -= 1
                cmd = self.command_history[-(self.history_index + 1)]
                input_widget = self.query_one("#user-input", Input)
                input_widget.value = cmd
                input_widget.cursor_position = len(cmd)

    def action_search_history(self):
        """Search command history WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[yellow]üìù Use Up/Down arrows to navigate command history[/yellow]")

    def action_toggle_sidebar(self):
        """Toggle sidebar visibility WITH SOUND"""
        self.nova_system.sound_system.play_sound("click")
        conversation = self.query_one("#conversation", RichLog)
        conversation.write("[blue]üîß Sidebar toggle coming soon![/blue]")

def main():
    """Clean startup - Animation first, no logs"""
    
    try:
        # STEP 1: RESTORE STDOUT FOR ANIMATION ONLY
        restore_stdout_for_animation()
        
        # STEP 2: SHOW ANIMATION IMMEDIATELY
        if FUTURISTIC_STARTUP_AVAILABLE:
            ultra_futuristic_startup()
            
            # Show AI personality
            personality = random_startup_personality()
            from colorama import Fore, Style
            print(f"{Fore.CYAN}{personality['name']} - {personality['greeting']}{Style.RESET_ALL}")
            print(f"{personality['tagline']}")
            print("\nüöÄ Starting NOVA CLI interface...")
        else:
            print("üöÄ Starting NOVA CLI...")
            print("‚ö° Loading features...")
        
        # STEP 3: SUPPRESS LOGS AGAIN FOR CLI INITIALIZATION
        suppress_logs_after_animation()
        
        # STEP 4: INITIALIZE CLI (SILENTLY)
        if not TEXTUAL_AVAILABLE:
            restore_stdout_for_animation()
            print("Textual UI not available. Please install textual: pip install textual")
            return
        
        app = NOVA_CLI()
        
        # STEP 5: RESTORE STDOUT AND RUN CLI
        restore_stdout_for_animation()
        app.run()
        
    except KeyboardInterrupt:
        restore_stdout_for_animation()
        if FUTURISTIC_STARTUP_AVAILABLE:
            from colorama import Fore, Style
            print(f"{Fore.CYAN}üåü NOVA AI returning to quantum sleep mode...{Style.RESET_ALL}")
            print(f"{Fore.MAGENTA}‚ú® Until we meet again in the digital cosmos! {Style.RESET_ALL}")
        else:
            print("\nüëã NOVA CLI terminated by user")
    except Exception as e:
        restore_stdout_for_animation()
        print(f"‚ùå Critical error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()