{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bf1a34",
   "metadata": {},
   "source": [
    "# Coupling Layers\n",
    "\n",
    "Coupling layers form the basis of many normalizing flows.\n",
    "Bijections, most commonly acting on scalar values, are applied to an *active* subset of variables while using the other *passive* ones to set the parameters of the transformations.\n",
    "Bijx provides multiple flexible frameworks for building coupling layers including both traditional affine coupling and possibly more complex architectures.\n",
    "\n",
    "To summarize again, the idea behind coupling layers is to:\n",
    "\n",
    "1. **Split** the input into **active** and **passive** components\n",
    "2. **Condition** the transformation on the passive components\n",
    "3. **Transform** only the active components using parameters computed from passive ones\n",
    "4. Preserve **invertibility** by keeping passive components unchanged\n",
    "\n",
    "Schematically, for input $x = (x_a, x_p)$ where $x_a$ are active and $x_p$ are passive:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_p &= x_p \\\\\n",
    "y_a &= T(x_a; \\theta(x_p))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $T$ is an invertible transformation and $\\theta(\\cdot)$ is a neural network computing parameters.\n",
    "Most commonly $T$ are scalar bijections that transform each array component of $x_a$ independently.\n",
    "This importantly gives a triangular Jacobian structure allowing for efficient density-change computation.\n",
    "\n",
    "Bijx provides a general toolkit for the above abstract approach, which lets one specify an arbitrary transformation $T$ as a neural network, extracts the information about parameters that need to be computed, and handles batch dimensions if present.\n",
    "This will be explained below, and supports arbitrary choices of $T$ (including non-scalar bijections).\n",
    "\n",
    "This approach can also be used for scalar bijections, although in those cases a direct approach is also straightforward.\n",
    "For example, for the typical case of [real NVP](https://arxiv.org/abs/1605.08803) the transformation $T$ is a simple element-wise affine linear transformation:\n",
    "\n",
    "$$\n",
    "y_a^i = e^{s_i(x_p)} \\cdot x_a^i + t_i(x_p) \\,,\n",
    "$$\n",
    "\n",
    "where $s_i$ and $t_i$ are any neural networks yielding outputs of the same shape as $x_a$, and the product is taken element-wise (often more explicitly denoted $e^{s_i} \\odot x_a$ as \"Hadamard product\").\n",
    "\n",
    "\n",
    "There are basically three levels of generality, as reflected in the next sections (after the first one on [masking](#masking)):\n",
    "- [Simple broadcasting](#simple-broadcasting): Use standard broadcasting between parameters and input arrays, manually selected coupling layers to match bijection parameters.\n",
    "- [Automatic parameter extraction](#automatic-parameter-extraction): Automatic extraction of parameters from a *template* bijection.\n",
    "- [Automatic vectorization](#automatic-vectorization): Automatic vectorization over batch dimensions if fundamental bijection does not support broadcasting.\n",
    "\n",
    "In principle the last, most powerful abstraction can be used in all cases.\n",
    "\n",
    "```{eval-rst}\n",
    "\n",
    ".. currentmodule:: bijx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d01577",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "For the splitting into active and passive degrees of freedom we have to define some kind of *masking* pattern.\n",
    "A general kind of binary mask is implemented as {class}`BinaryMask`, which supports two kinds of splitting:\n",
    "\n",
    "- Indexing, that is splitting $x = (x_a, x_p)$ explicitly.\n",
    "- Multiplicative, that is setting $x_a = \\text{mask} \\odot x$ and $x_p = (1 - \\text{mask}) \\odot x$.\n",
    "\n",
    "The latter is especially convenient if we want to preserve the shape and locality information of the input, such as in convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bijx\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# the simplest way is to initialize from boolean mask\n",
    "mask = bijx.BinaryMask.from_boolean_mask(jnp.array([True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2, (2, 1), (2, 1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.eye(2)\n",
    "y = mask.split(x)\n",
    "\n",
    "type(y), len(y), y[0].shape, y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08806e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ True,  True],\n",
       "       [ True,  True]], dtype=bool)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.merge(*y) == x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89211dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking by multiplication\n",
    "mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6da2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invert mask with mask.flip() or ~mask\n",
    "~mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae24a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ True, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False,  True]], dtype=bool)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common checkerboard mask\n",
    "mask = bijx.checker_mask(shape=(3, 3), parity=True)\n",
    "\n",
    "# can recover underlying boolean mask\n",
    "mask.boolean_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d72ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 2, 4, 6, 8], dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that indexing always flattens the event shape\n",
    "x = jnp.arange(3 * 3).reshape(3, 3)\n",
    "x[mask.indices()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cb34f",
   "metadata": {},
   "source": [
    "To be compatible with jax.jit tracing, all shapes need to be known at compile time.\n",
    "In particular, that means we cannot index with the boolean mask directly.\n",
    "That is why {class}`bijx.BinaryMask` internally stores the indices of the \"active\" and \"passive\", or \"primary\" and \"secondary\" components, which are used by `.split(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c6b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Const( # 10 (80 B)\n",
       "  value=(array([0, 0, 1, 2, 2]), array([0, 2, 1, 0, 2]))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.primary_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6faa2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ellipsis, array([0, 1, 1, 2]), array([1, 0, 2, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the indexing tuple that includes a batch ellipsis can also be explicitly obtained\n",
    "mask.indices(primary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f209e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ellipsis, array([0, 1, 1, 2]), array([1, 0, 2, 1]), slice(None, None, None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and for convenience \"channel\" dimensions can be added\n",
    "mask.indices(primary=False, extra_channel_dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff7f2a",
   "metadata": {},
   "source": [
    "## Simple broadcasting\n",
    "\n",
    "All scalar bijections that are subclasses of {class}`ScalarBijection` can automatically broadcast over parameter and input shape batch axes. \n",
    "This is in particular the case for {class}`AffineLinear` which implements the simple affine linear transformation $y = s \\cdot x + t$.\n",
    "\n",
    "It is then straightforward to implement a simple coupling layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nnx\n",
    "from flax import nnx\n",
    "rngs = nnx.Rngs(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8010b",
   "metadata": {},
   "source": [
    "Because the forward and reverse methods would look basically the same, we can use {class}`bijx.ApplyBijection` as base class which implements forward and reverse into a single `apply` method that takes `reverse` as additional boolean keyword argument.\n",
    "This could obviously also easily be implemented manually, but it saves a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae544d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCouplingLayer(bijx.ApplyBijection):\n",
    "    def __init__(self, mask: bijx.BinaryMask):\n",
    "        self.mask = mask\n",
    "        self.net = bijx.nn.nets.MLP(\n",
    "            in_features=mask.count_primary,\n",
    "            out_features=2 * mask.count_primary,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def apply(self, x, log_density, reverse=False, **kwargs):\n",
    "        active, passive = self.mask.split(x)\n",
    "\n",
    "        params = self.net(passive)\n",
    "        # reshape to (..., event_size, 2)\n",
    "        params.reshape(params.shape[:-1] + (-1, 2))\n",
    "        s, t = jnp.split(params, 2, axis=-1)\n",
    "\n",
    "        # Note that active and passive have shape (..., 1).\n",
    "        # This is the same shape as s and t because split\n",
    "        # does not remove the final axis.\n",
    "        # Therefore, broadcasting will work as expected.\n",
    "\n",
    "        bijection = bijx.AffineLinear(s, t, transform_scale=jnp.exp)\n",
    "\n",
    "        method = bijection.reverse if reverse else bijection.forward\n",
    "        active, log_density = method(active, log_density)\n",
    "\n",
    "        x = self.mask.merge(active, passive)\n",
    "        return x, log_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5, 2), (5,))\n"
     ]
    }
   ],
   "source": [
    "bijx.utils.load_shapes_magic()\n",
    "\n",
    "flow = AffineCouplingLayer(bijx.checker_mask((2,), True))\n",
    "\n",
    "# apply layer with batch=(5,) and event_shape=(2,)\n",
    "%shapes flow.forward(jnp.ones((5, 2)), jnp.zeros((5,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e8be3",
   "metadata": {},
   "source": [
    "These layers, with varying parity/masking patterns can then be repeated to build more complex flows.\n",
    "\n",
    "An analogous construciton with multiplicative masking looks similar, except that more care needs to be taken that the log-likelihood change is not included for the (to be ignored) masked array entries. Below is a sketch how this can be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones((10, 3))\n",
    "log_density = jnp.zeros(10,)\n",
    "\n",
    "mask = bijx.checker_mask((3,), True)\n",
    "\n",
    "# the parameters will now typically have the same shape as x,\n",
    "# including the passive entries\n",
    "s, t = jnp.ones((3,)), jnp.ones((3,))\n",
    "bij = bijx.AffineLinear(s, t)\n",
    "\n",
    "# we need to get the log-density change for each entry so we can mask out passive\n",
    "y, ld_change = bij.forward(\n",
    "    x,\n",
    "    # append the event shape\n",
    "    jnp.zeros(log_density.shape + (3,)),\n",
    ")\n",
    "\n",
    "# only sum over active entries\n",
    "log_density += jnp.sum(ld_change, where=mask.boolean_mask, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ee880",
   "metadata": {},
   "source": [
    "The above pattern can be applied to any scalar bijection, not just the affine linear one used here.\n",
    "Note, however, that we needed to explicitly insert in the network definition the number of parameters needed for the transformation (2 in the above).\n",
    "\n",
    "A method to automatically extract the number of necessary parameters is provided by {class}`bijx.ModuleReconstructor`, which then also extends to arbitrary coupling layers as explained in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0db6d9",
   "metadata": {},
   "source": [
    "## Automatic parameter extraction\n",
    "\n",
    "For more sophisticated coupling layers beyond affine transformations, bijx provides {class}`ModuleReconstructor` which facilitates parameter sharing and \"reconstructing\" a bijection given a set of parameters (in one of various representations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96371061",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = bijx.MonotoneRQSpline(10, (3,), rngs=rngs)\n",
    "\n",
    "template = bijx.bijections.coupling.ModuleReconstructor(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a767a",
   "metadata": {},
   "source": [
    "The template extracts the parameter information from the provided module, and allows various different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(87)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can provide single 1d array of parameters of this size\n",
    "template.params_total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9e9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heights': ShapedArray(float32[3,10]),\n",
       " 'slopes': ShapedArray(float32[3,9]),\n",
       " 'widths': ShapedArray(float32[3,10])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also provide dictionary which matches this structure\n",
    "template.params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ShapedArray(float32[3,10]),\n",
       " ShapedArray(float32[3,9]),\n",
       " ShapedArray(float32[3,10])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or \"leaves\" of the corresponding pytree\n",
    "template.params_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cbe78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dtype('float32'), dtype('float32'), dtype('float32')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some other convenience attributes also available\n",
    "template.params_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 10), (3, 9), (3, 10)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some other convenience attributes also available\n",
    "template.params_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c3868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heights': (3, 10), 'slopes': (3, 9), 'widths': (3, 10)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some other convenience attributes also available\n",
    "template.params_shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b01a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caveat: need to be careful if parameters are complex (not fully supported)\n",
    "template.has_complex_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 3), (10,))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# dummy array of parameters (would usually be/depend on the output of some NN)\n",
    "params_array = jnp.zeros((template.params_total_size,))\n",
    "\n",
    "# example inputs\n",
    "x = jnp.ones((10, 3)) / 2\n",
    "lp = jnp.zeros((10,))\n",
    "\n",
    "# reconstruct flow from contiguous array of parameters\n",
    "flow = template.from_params(params_array)\n",
    "\n",
    "%shapes flow.forward(x, lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd63387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also reconstruct from other representations\n",
    "\n",
    "flow = template.from_params({\n",
    "    key: jnp.zeros(shape)\n",
    "    for key, shape in template.params_shape_dict.items()\n",
    "})\n",
    "\n",
    "flow = template.from_params([\n",
    "    jnp.zeros(shape)\n",
    "    for shape in template.params_shapes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130eedeb",
   "metadata": {},
   "source": [
    "## Automatic vectorization\n",
    "\n",
    "The implementation of splines in bijx, just as the other scalar bijections, support broadcasting over batch indices that are added to the parameters if they match the event shape. In fact, this is indistinguishable from having a parameter shape that matches the event shape (from the perspective of scalar bijections, those are effectively the same thing).\n",
    "\n",
    "However, general bijections may not support this replacement of internal parameters with a batch of parameters that carry additional indices (this is true, fundamentally, of the spline flows -- however, they already internally apply `jax.vmap` automatically to handle this).\n",
    "In this case, an additional argument `autovmap=True` can be passed to `from_params`, which returns an object that behaves almost like the original module/bijection, except that function calls are automatically `jax.vmap`'ed over the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 3), (10,))\n"
     ]
    }
   ],
   "source": [
    "# first, demonstration that the spline flow works with a batch of parameters\n",
    "params_array = jnp.zeros((10, template.params_total_size))\n",
    "\n",
    "# example inputs\n",
    "x = jnp.ones((10, 3)) / 2\n",
    "lp = jnp.zeros((10,))\n",
    "\n",
    "# reconstruct flow from now batched contiguous array of parameters\n",
    "flow = template.from_params(params_array)\n",
    "\n",
    "%shapes flow.forward(x, lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a008d11",
   "metadata": {},
   "source": [
    "For demonstration purposes, let us define a version of spline flow that does not support broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a968f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoBatchSpline(bijx.Bijection):\n",
    "\n",
    "    def __init__(self, knots, *, rngs):\n",
    "        self.knots = knots\n",
    "\n",
    "        # in contrast to the above, here also FIX event_shape to be scalar, i.e. ()\n",
    "        self.spline = bijx.MonotoneRQSpline(knots, (), rngs=rngs)\n",
    "\n",
    "    def forward(self, x, log_density):\n",
    "        assert self.spline.widths.value.shape == (self.knots,)\n",
    "        return self.spline.forward(x, log_density)\n",
    "\n",
    "    def reverse(self, x, log_density):\n",
    "        assert self.spline.widths.value.shape == (self.knots - 1,)\n",
    "        return self.spline.reverse(x, log_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.5008062, dtype=float32), Array(-0.00292323, dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = NoBatchSpline(10, rngs=rngs)\n",
    "\n",
    "# works on scalar values\n",
    "flow.forward(0.5, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa642bae",
   "metadata": {},
   "source": [
    "It also still works on batched inputs.\n",
    "The problem (by construction, but representing a realistic scenario) are batched *parameters*, although the final solution would also handle bijections that do not support any kind of batch dimension (for the inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 3), (10,))\n"
     ]
    }
   ],
   "source": [
    "%shapes flow.forward(jnp.ones((10, 3)), jnp.zeros((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = bijx.ModuleReconstructor(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected assertion error about wrong parameter shape\n"
     ]
    }
   ],
   "source": [
    "params_array = jnp.zeros((10, template.params_total_size))\n",
    "\n",
    "x = jnp.ones((10, 3)) / 2\n",
    "lp = jnp.zeros((10,))\n",
    "\n",
    "flow = template.from_params(params_array)\n",
    "\n",
    "try:\n",
    "    flow.forward(x, lp)\n",
    "except AssertionError:\n",
    "    print('Expected assertion error about wrong parameter shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa10b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we now have an additional \"3\" because the flow uses scalar event shape, i.e. ()\n",
    "params_array = jnp.zeros((10, 3, template.params_total_size))\n",
    "\n",
    "x = jnp.ones((10, 3)) / 2\n",
    "lp = jnp.zeros((10,))\n",
    "\n",
    "# now acts almost like the original flow, except it actually wraps around it\n",
    "# and automatically applies vmap to function calls\n",
    "flow = template.from_params(params_array, autovmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98abe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bijx.bijections.coupling.AutoVmapReconstructor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters can be accessed (now has batch index)\n",
    "flow.spline.widths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9092e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmap got inconsistent sizes for array axes to be mapped:\n",
      "  * most axes (2 of them) had size 3, e.g. axis 0 of argument all_args of type float32[3,29];\n",
      "  * one axis had size 10: axis 0 of args[1][1] of type float32[10]\n"
     ]
    }
   ],
   "source": [
    "# This now almost works, but there is another complication:\n",
    "try:\n",
    "    flow.forward(x, lp, input_ranks=(0, 0))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c8d28",
   "metadata": {},
   "source": [
    "We have to be careful to have the log-density match the shape of x.\n",
    "The above doesn't work because the log-density implies the event shape of `x` should be scalar, while the combined input shapes of `x` and `lp` imply `x` is a vector.\n",
    "We can avoid that by inputting a log-density that matches the shape of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 3), (10, 3))\n"
     ]
    }
   ],
   "source": [
    "# The input_ransk default to (0, 0), but can be modified if the fundamental bijection is not scalar.\n",
    "%shapes flow.forward(x, jnp.zeros_like(x), input_ranks=(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078764b",
   "metadata": {},
   "source": [
    "### General coupling layer\n",
    "\n",
    "Even more convenience, which also takes care of the annoyance above that we had to extend the log-density to match the shape of `x` (even though it should be obvious that we want to broadcast and sum the log-density over the event shape of `x`) is provided by {class}`GeneralCouplingLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82de483",
   "metadata": {},
   "outputs": [],
   "source": [
    "knots = 11\n",
    "spline_template = bijx.ModuleReconstructor(\n",
    "    # again, define bijection to act on scalars, event_shapes=()\n",
    "    bijx.MonotoneRQSpline(knots, (), rngs=nnx.Rngs(0))\n",
    ")\n",
    "\n",
    "# funciton to construct a single coupling layer\n",
    "def spline_coupling_layer(mask, width, depth, rngs):\n",
    "\n",
    "        # mask contains information about active/passive features\n",
    "        count_active, count_passive = mask.counts\n",
    "        # template knows how many parameters are needed\n",
    "        param_count = spline_template.params_total_size\n",
    "\n",
    "        # define network that maps frozen features to parameters\n",
    "        resnet = bijx.nn.nets.ResNet(\n",
    "            count_passive, count_active *param_count, width, depth,\n",
    "            final_kernel_init=nnx.initializers.normal(),\n",
    "            final_bias_init=nnx.initializers.zeros,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        # the parameters in the end must have shape (..., count_active, param_count)\n",
    "        def reshape_params(p):\n",
    "            return rearrange(p, '... (t b) -> ... t b', t=count_active)\n",
    "\n",
    "        param_net = nnx.Sequential(\n",
    "            resnet,\n",
    "            reshape_params,\n",
    "        )\n",
    "\n",
    "        return bijx.GeneralCouplingLayer(\n",
    "            param_net,\n",
    "            mask,\n",
    "            spline_template,\n",
    "            # defaults to 0 (scalar bijections)\n",
    "            bijection_event_rank=0,\n",
    "            # also supports multiplicative masking\n",
    "            split=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeda1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = bijx.BinaryMask.from_boolean_mask(jnp.array([True, False]))\n",
    "\n",
    "layers = []\n",
    "\n",
    "# chain 5 coupling layers\n",
    "for _ in range(5):\n",
    "    layers.append(spline_coupling_layer(mask, 32, 2, rngs=rngs))\n",
    "    # invert mask after each layer\n",
    "    mask = ~mask\n",
    "\n",
    "flow = bijx.Chain(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfb8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 2), (10,))\n"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((10, 2))\n",
    "lp = jnp.zeros((10,))\n",
    "\n",
    "%shapes flow.forward(x, lp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
