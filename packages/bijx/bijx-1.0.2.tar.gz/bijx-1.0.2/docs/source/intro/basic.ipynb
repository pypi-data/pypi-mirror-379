{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1b72ec",
   "metadata": {},
   "source": [
    "# Basic Bijections\n",
    "\n",
    "```{eval-rst}\n",
    ".. currentmodule:: bijx\n",
    "```\n",
    "\n",
    "Besides continuous flows, powerful transformations can be built out of closed-form one-dimensional (scalar) bijections.\n",
    "In particular, the next page discusses how they can be composed in coupling layers to form higher-dimensional bijections.\n",
    "The following sections review implemented scalar bijections, as well as [meta layers](#meta-layers) that only change the representation of the input such as shapes, and do not change their value or density.\n",
    "\n",
    "For convenience, most scalar bijections inherit from {class}`ScalarBijection` which splits the forward/reverse input transformation and the log-jacobian computation into separate methods.\n",
    "\n",
    "```python\n",
    "class ScalarBijection(Bijection):\n",
    "    def fwd(self, x, **kwargs):        # Forward transformation x → y\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def rev(self, y, **kwargs):        # Reverse transformation y → x\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def log_jac(self, x, y, **kwargs): # Log |∂y/∂x|\n",
    "        # both x and y are available; can use whichever is most convenient\n",
    "        raise NotImplementedError()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e7eaf",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "All scalar transformations operate on the entries of an array individually.\n",
    "For convenience, classes that inherit from ScalarBijection (which are all presented here except for spline flows) allow the passed `log_density` to have a different shape than the input `x`.\n",
    "If this is the case, it is used to infer the event shape, and the log-jacobian is summed over these axes.\n",
    "Thus, the scalar bijections naturally extend to element-wise higher dimensional transformations.\n",
    "\n",
    "This broadcasting behavior extends to the parameters of the bijections themselves.\n",
    "By default, the scalar transformations here have scalar parameters or none at all.\n",
    "However, it is also possible to specify parameters that match the event shape of the input.\n",
    "Parameters and inputs are broadcast according to the usual numpy broadcasting behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb1598",
   "metadata": {},
   "source": [
    "## Parameter Specification\n",
    "\n",
    "1. **Shape Tuple**: `(D,)` or `()` creates new `nnx.Param` with default initialization\n",
    "2. **Array Value**: e.g. `jnp.array([1.0, 2.0])`, will by default be wrapped with `nnx.Param`\n",
    "3. **Variable Instance**: e.g. `nnx.Param(value)` or `bijx.Const(value)` for explicit control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcdc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scaling( # Const: 2 (8 B)\n",
       "  scale=TransformedParameter(param=Const( # 2 (8 B)\n",
       "    value=Array(shape=(2,), dtype=dtype('float32'))\n",
       "  ), transform=None)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bijx\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Method 1: Shape-based initialization\n",
    "bijx.Scaling((2,), rngs=rngs)  # Creates trainable parameters\n",
    "\n",
    "# Method 2: Value-based initialization\n",
    "bijx.Scaling(jnp.array([1.0, 2.0]))  # Uses provided values\n",
    "\n",
    "# Method 3: Variable-based (non-trainable constant)\n",
    "bijx.Scaling(bijx.Const(jnp.array([1.0, 2.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb3139",
   "metadata": {},
   "source": [
    "Because some parameters are restricted to certain ranges (e.g. the scaling), the parametrized scalar bijections in addition have optional arguments of the kind `transform_parameter` which can be any callable that transform the parameter value before it is used.\n",
    "{class}`Scaling` by default does not apply a transformation, `transform_scale=None` (only zero is unsafe; note however that {class}`AffineLinear` does apply `jnp.exp` as in real NVP).\n",
    "For stable training, this could be changed to softplus, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb9a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scaling( # Param: 1 (4 B)\n",
       "  scale=TransformedParameter(param=Param( # 1 (4 B)\n",
       "    value=Array(1., dtype=float32)\n",
       "  ), transform=<PjitFunction of <function softplus at 0x1157d0ae0>>)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bijx.Scaling(transform_scale=nnx.softplus, rngs=rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b03908",
   "metadata": {},
   "source": [
    "## Scalar Bijections\n",
    "\n",
    "### Linear and Affine Transformations\n",
    "\n",
    "Basic building blocks for normalizing flows:\n",
    "\n",
    "- {class}`AffineLinear`: $[-\\infty, \\infty] → [-\\infty, \\infty]$ via $\\text{scale} \\cdot x + \\text{shift}$ with learnable parameters\n",
    "- {class}`Scaling`: $[-\\infty, \\infty] → [-\\infty, \\infty]$ via $\\text{scale} \\cdot x$\n",
    "- {class}`Shift`: $[-\\infty, \\infty] → [-\\infty, \\infty]$ via $x + \\text{shift}$\n",
    "\n",
    "These provide the most basic learnable transformations with simple Jacobians.\n",
    "\n",
    "### Bounded/Unbounded Transforms\n",
    "\n",
    "**Bounded range transforms** that map unbounded inputs to bounded intervals:\n",
    "\n",
    "- {class}`Sigmoid`: $[-\\infty, \\infty] → [0, 1]$ via $σ(x) = 1/(1 + e^{-x})$\n",
    "- {class}`Tanh`: $[-\\infty, \\infty] → [-1, 1]$ via $\\tanh(x)$\n",
    "- {class}`GaussianCDF`: $[-\\infty, \\infty] → [0, 1]$ via $Φ((x-\\mu)/\\sigma)$ with learnable location and scale\n",
    "\n",
    "**Unbounding transforms** for mapping bounded to unbounded:\n",
    "\n",
    "- {class}`Tan`: $[0, 1] → [-\\infty, \\infty]$ via $\\tan(\\pi(x - 0.5))$\n",
    "\n",
    "Note that each bijection can be inverted via `.invert()`.\n",
    "\n",
    "### Positive Domain Transforms\n",
    "\n",
    "Transforms for mapping to positive reals:\n",
    "\n",
    "- {class}`Exponential`: $[-\\infty, \\infty] → [0, \\infty]$ via $e^x$ (simple but can be numerically unstable)\n",
    "- {class}`SoftPlus`: $[-\\infty, \\infty] → [0, \\infty]$ via $\\log(1 + e^x)$ (numerically stable alternative)\n",
    "- {class}`Power`: $[0, \\infty] → [0, \\infty]$ via $x^p$ with learnable exponent $p > 0$\n",
    "\n",
    "### Other\n",
    "\n",
    "- {class}`Sinh`: $[-\\infty, \\infty] → [-\\infty, \\infty]$ via $\\sinh(x)$\n",
    "- {class}`BetaStretch`: $[0, 1] → [0, 1]$ via $\\frac{x^{\\alpha}}{x^{\\alpha} + (1-x)^{\\alpha}}$\n",
    "\n",
    "The latter provides smooth stretching of the unit interval with a learnable parameter $\\alpha$ controlling the degree and direction of stretching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6d7aa",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The following exhibits different ways of using scalar bijections.\n",
    "\n",
    "- Parameters can be specified as shapes or values (arrays or nnx.Variables).\n",
    "- Broadcasting between parameters and inputs follows numpy rules, except that inferred event shape is summed over in log-density.\n",
    "- Parameters can be scalar or match (a subset of) the event shape of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d901685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2., dtype=float32), Array(0., dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bijx\n",
    "import numpy as jnp\n",
    "import jax\n",
    "from flax import nnx\n",
    "\n",
    "# random number geenrator\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "layer = bijx.AffineLinear(\n",
    "    # can specify parameters as shapes\n",
    "    scale=(),\n",
    "    # can also give specific values\n",
    "    # must be arrays -- then wrapped in nnx.Param -- or any nnx.Variable\n",
    "    shift=jnp.array(1.0),\n",
    "    # then need to provide rngs for initialization\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "# shifts input, leaves density unchanged\n",
    "layer.forward(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 7), (10, 7))\n"
     ]
    }
   ],
   "source": [
    "# load %shapes magic, which tree-maps jnp.shape before showing output\n",
    "bijx.utils.load_shapes_magic()\n",
    "\n",
    "# both have batch shape (10, 7), x is interpreted to be scalar\n",
    "x = jnp.zeros((10, 7))\n",
    "log_density = jnp.zeros((10, 7))\n",
    "\n",
    "# usual numpy broadcasting behavior\n",
    "%shapes layer.forward(x, log_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 7), (10,))\n"
     ]
    }
   ],
   "source": [
    "# x is inferred to be a vector, batch shape is (10,)\n",
    "x = jnp.zeros((10, 7))\n",
    "log_density = jnp.zeros((10,))\n",
    "\n",
    "# transformation sums density change over event axes\n",
    "%shapes layer.forward(x, log_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536accd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.4424558 , -2.0356805 ,  0.20554423, -0.3535502 , -0.76197404,\n",
       "       -1.1785518 , -1.1482196 ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize to carry parameters of shape (7,)\n",
    "layer = bijx.AffineLinear(\n",
    "    scale=(7,),\n",
    "    shift=jax.random.normal(rngs.next(), (7,)),\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "# parameters and event-shape of x are broadcast together\n",
    "x, log_density = layer.forward(x, log_density)\n",
    "x[0]  # first entry in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba7460",
   "metadata": {},
   "source": [
    "## Rational Quadratic Splines\n",
    "\n",
    "{class}`MonotoneRQSpline`: Implements monotonic rational quadratic splines following [Durkan et al. (2019)](https://arxiv.org/abs/1906.04032).\n",
    "This class behaves similar to scalar bijections above, in the sense that the transformation is applied per element.\n",
    "However, the parameters here are always randomly initialized depending on the event shape, because their array shapes also depend on the number of knots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd6833",
   "metadata": {},
   "source": [
    "## Meta layers\n",
    "\n",
    "Meta layers are bijections that rearrange data without changing its density.\n",
    "This results in a log-Jacobian determinant of zero.\n",
    "They are useful when creating complex architectures as chains of bijections that have differing assumptions (e.g. vectorial vs image-like shape, feature channels, etc.).\n",
    "\n",
    "- {class}`Reshape`: Reshapes the event dimensions of an array.\n",
    "- {class}`ExpandDims`: Adds a new dimension to an array.\n",
    "- {class}`SqueezeDims`: Removes a singleton dimension from an array.\n",
    "- {class}`Partial`: Wraps another bijection to fix a set of its keyword arguments, creating a specialized version.\n",
    "\n",
    "It is also straightforward to create custom layers given two functions, `forward(x)` and `reverse(x)` that only depend on the input using {class}`MetaLayer` as `MetaLayer(forward, reverse)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3429be",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = bijx.MetaLayer(\n",
    "    # forward\n",
    "    lambda x: jnp.split(x, 2, axis=-1),\n",
    "    # reverse\n",
    "    lambda y: jnp.concatenate(y, axis=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0350e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((3, 4))\n",
    "\n",
    "y, log_density = split.forward(x, jnp.zeros((3,)))\n",
    "print(*y, sep=\"\\n\")\n",
    "print(jnp.all(log_density == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01108106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets back to original shape, x\n",
    "split.reverse(y, jnp.zeros((3,)))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
