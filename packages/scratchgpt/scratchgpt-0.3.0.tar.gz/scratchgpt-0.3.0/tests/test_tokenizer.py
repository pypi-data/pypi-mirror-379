from scratchgpt.tokenizer.char_tokenizer import CharTokenizer, Utf8Tokenizer

test_data = [
    "ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÐºÐ°Ðº Ð´ÐµÐ»Ð°? ðŸ˜Š",
    "Ð¯ Ð»ÑŽÐ±Ð»ÑŽ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ ÐºÐ½Ð¸Ð³Ð¸.",
    "ÐœÐ¾ÑÐºÐ²Ð° - ÑÑ‚Ð¾ ÐºÑ€Ð°ÑÐ¸Ð²Ñ‹Ð¹ Ð³Ð¾Ñ€Ð¾Ð´.",
    "ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹",
    "ë‚˜ëŠ” ë‹¹ì‹ ì„ ë§Œë‚˜ì„œ í–‰ë³µí•´ìš” ðŸ˜Š",
    "ì„œìš¸ì€ ì•„ë¦„ë‹¤ìš´ ë„ì‹œìž…ë‹ˆë‹¤.",
    "Ciao, come stai? ðŸ˜Š",
    "Amo leggere libri.",
    "Roma Ã¨ una cittÃ  bellissima.",
    "Hello, how are you? ðŸ‘‹",
    "I love to read books.",
    "New York City is a bustling metropolis ðŸ—½ï¸",
]


def test_basic_char_tokenizer() -> None:
    test_corpus = "".join(test_data)
    tokenizer = CharTokenizer(test_corpus)

    for test_sample in test_data:
        encoded = tokenizer.encode(test_sample)
        decoded = tokenizer.decode(encoded)

        assert test_sample == decoded, "Oh no, this thing failed"


def test_utf8_tokenizer() -> None:
    tokenizer = Utf8Tokenizer()

    for test_sample in test_data:
        encoded = tokenizer.encode(test_sample)
        decoded = tokenizer.decode(encoded)

        assert test_sample == decoded, "Oh no, Utf8Tokenizer failed"

    print(f"{tokenizer.vocab_size=}")
