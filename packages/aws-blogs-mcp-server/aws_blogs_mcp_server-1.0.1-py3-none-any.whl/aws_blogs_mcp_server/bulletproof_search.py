#!/usr/bin/env python3
"""
Bulletproof AWS Blogs Search System
Implements all best practices to prevent incorrect information.
"""

import asyncio
import logging
import time
from typing import Any, Dict, List, Optional, Tuple, NamedTuple
from dataclasses import dataclass
from enum import Enum
import json
import hashlib
from datetime import datetime, timedelta
import re

import httpx
from bs4 import BeautifulSoup

logger = logging.getLogger("bulletproof-search")

class SearchConfidence(Enum):
    """Search confidence levels with clear thresholds"""
    EXACT_MATCH = 0.95      # 95%+ - Exact URL or title match
    HIGH_CONFIDENCE = 0.85  # 85%+ - Very likely correct
    MEDIUM_CONFIDENCE = 0.70 # 70%+ - Probably correct
    LOW_CONFIDENCE = 0.50   # 50%+ - Possibly correct
    NO_CONFIDENCE = 0.0     # <50% - Not confident

@dataclass
class SearchResult:
    """Structured search result with confidence and metadata"""
    found: bool
    confidence: float
    results: List[Dict[str, Any]]
    search_metadata: Dict[str, Any]
    message: str
    search_id: str
    execution_time: float
    sources_checked: List[str]
    categories_searched: int

@dataclass
class BlogPost:
    """Structured blog post representation"""
    title: str
    url: str
    category: str
    excerpt: str
    date: Optional[str]
    confidence: float
    similarity_score: float
    source: str

class BulletproofAWSBlogsSearch:
    """
    Bulletproof search system that prevents incorrect information.
    
    Key principles:
    1. Multiple search strategies with fallbacks
    2. Confidence-based responses
    3. Transparent error handling
    4. Continuous validation
    5. Never claim certainty when uncertain
    """
    
    def __init__(self, client: httpx.AsyncClient, base_url: str, known_categories: List[str]):
        self.client = client
        self.base_url = base_url
        self.blog_base_url = f"{base_url}/blogs/"
        self.known_categories = known_categories
        
        # Search strategies in order of preference
        self.search_strategies = [
            self._direct_url_strategy,
            self._comprehensive_category_strategy,
            self._semantic_similarity_strategy,
            self._fuzzy_matching_strategy,
            self._sitemap_fallback_strategy
        ]
        
        # Validation test cases - these MUST always work
        self.golden_test_cases = [
            {
                'query': 'Introducing AWS Gateway Load Balancer â€“ Easy Deployment, Scalability, and High Availability for Partner Appliances',
                'expected_url': 'https://aws.amazon.com/blogs/aws/introducing-aws-gateway-load-balancer-easy-deployment-scalability-and-high-availability-for-partner-appliances/',
                'category': 'aws',
                'min_confidence': 0.95
            },
            {
                'query': 'Partners Group Strategic Cloud Transformation',
                'category': 'alps',
                'min_confidence': 0.85
            }
        ]
        
        # Performance metrics
        self.metrics = {
            'total_searches': 0,
            'successful_searches': 0,
            'high_confidence_searches': 0,
            'validation_failures': 0,
            'average_execution_time': 0.0
        }
    
    async def search(self, query: str, category: Optional[str] = None, limit: int = 10) -> SearchResult:
        """
        Main search method with bulletproof error handling.
        
        Returns structured results with confidence levels and transparent messaging.
        """
        start_time = time.time()
        search_id = self._generate_search_id(query)
        
        logger.info(f"[{search_id}] Starting bulletproof search for: {query}")

        # Handle empty queries immediately
        if not query or not query.strip():
            execution_time = time.time() - start_time
            return SearchResult(
                found=False,
                confidence=0.0,
                results=[],
                search_metadata={
                    "search_id": search_id,
                    "execution_time": execution_time,
                    "sources_checked": [],
                    "categories_searched": 0
                },
                message="âŒ **Empty query provided**\n\nPlease provide a search term to find AWS blog posts.",
                search_id=search_id,
                execution_time=execution_time,
                sources_checked=[],
                categories_searched=0
            )
        
        try:
            # Run validation check first
            validation_result = await self._quick_validation_check()
            if not validation_result.passed:
                return self._create_system_degraded_response(search_id, validation_result)
            
            # Execute search strategies
            all_results = []
            sources_checked = []
            
            for strategy in self.search_strategies:
                try:
                    strategy_results = await strategy(query, category, limit)
                    if strategy_results:
                        all_results.extend(strategy_results)
                        sources_checked.append(strategy.__name__)
                        
                        # If we have high-confidence results, we can stop early
                        max_confidence = max(r.confidence for r in strategy_results)
                        if max_confidence >= SearchConfidence.HIGH_CONFIDENCE.value:
                            logger.info(f"[{search_id}] High confidence results found, stopping search")
                            break
                            
                except Exception as e:
                    logger.warning(f"[{search_id}] Strategy {strategy.__name__} failed: {e}")
                    continue
            
            # Process and rank results
            final_results = self._process_and_rank_results(all_results, query, limit)
            execution_time = time.time() - start_time
            
            # Update metrics
            self._update_metrics(final_results, execution_time)
            
            # Create response based on confidence
            return self._create_confident_response(
                final_results, query, search_id, execution_time, 
                sources_checked, len(self.known_categories)
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"[{search_id}] Search failed completely: {e}")
            return self._create_error_response(query, search_id, execution_time, str(e))
    
    async def _direct_url_strategy(self, query: str, category: Optional[str], limit: int) -> List[BlogPost]:
        """
        Strategy 1: Direct URL construction and verification.
        Highest confidence when successful.
        """
        results = []
        
        # Generate potential URLs based on common AWS blog patterns
        potential_urls = self._generate_potential_urls(query, category)
        
        for url in potential_urls:
            try:
                # Quick HEAD request to check if URL exists
                response = await self.client.head(url, timeout=5.0)
                if response.status_code == 200:
                    # URL exists, fetch full content
                    blog_post = await self._extract_blog_post_from_url(url)
                    if blog_post and self._matches_query(blog_post, query):
                        blog_post.confidence = SearchConfidence.EXACT_MATCH.value
                        blog_post.source = "direct_url"
                        results.append(blog_post)
                        
            except Exception as e:
                logger.debug(f"Direct URL check failed for {url}: {e}")
                continue
        
        return results[:limit]
    
    async def _comprehensive_category_strategy(self, query: str, category: Optional[str], limit: int) -> List[BlogPost]:
        """
        Strategy 2: Search across all relevant categories with robust parsing.
        """
        results = []
        
        # Determine categories to search
        if category:
            categories_to_search = [category]
        else:
            categories_to_search = self._prioritize_categories_for_query(query)
        
        # Search each category with multiple parsing strategies
        for cat in categories_to_search[:10]:  # Limit to top 10 categories
            try:
                category_results = await self._search_single_category(query, cat, limit)
                results.extend(category_results)
                
                if len(results) >= limit:
                    break
                    
            except Exception as e:
                logger.warning(f"Category search failed for {cat}: {e}")
                continue
        
        return results[:limit]
    
    async def _semantic_similarity_strategy(self, query: str, category: Optional[str], limit: int) -> List[BlogPost]:
        """
        Strategy 3: Semantic similarity matching for better understanding.
        """
        # This would implement semantic search using embeddings
        # For now, return empty list as placeholder
        return []
    
    async def _fuzzy_matching_strategy(self, query: str, category: Optional[str], limit: int) -> List[BlogPost]:
        """
        Strategy 4: Fuzzy matching for typos and variations.
        """
        # This would implement fuzzy string matching
        # For now, return empty list as placeholder
        return []
    
    async def _sitemap_fallback_strategy(self, query: str, category: Optional[str], limit: int) -> List[BlogPost]:
        """
        Strategy 5: Sitemap-based search as last resort.
        """
        # This would implement sitemap parsing and search
        # For now, return empty list as placeholder
        return []
    
    def _generate_potential_urls(self, query: str, category: Optional[str]) -> List[str]:
        """Generate potential URLs based on query patterns and AWS blog conventions."""
        # Don't generate URLs for empty queries
        if not query or not query.strip():
            return []

        """Generate potential URLs based on query patterns and AWS blog conventions."""
        urls = []
        
        # Normalize query to URL slug
        slug = self._query_to_slug(query)
        
        # Pattern 1: Direct category + slug
        target_categories = [category] if category else ['aws', 'compute', 'architecture', 'developer']
        for cat in target_categories:
            urls.append(f"{self.blog_base_url}{cat}/{slug}/")
        
        # Pattern 2: AWS announcement patterns
        if query.lower().startswith('introducing'):
            clean_slug = slug.replace('introducing-', '')
            urls.append(f"{self.blog_base_url}aws/{clean_slug}/")
            urls.append(f"{self.blog_base_url}aws/introducing-{clean_slug}/")
        
        # Pattern 3: Service-specific patterns
        aws_services = ['lambda', 'ec2', 's3', 'rds', 'eks', 'ecs']
        for service in aws_services:
            if service in query.lower():
                urls.append(f"{self.blog_base_url}compute/{slug}/")
                urls.append(f"{self.blog_base_url}aws/{slug}/")
        
        return urls
    
    def _query_to_slug(self, query: str) -> str:
        """Convert query to URL slug following AWS conventions."""
        # Remove special characters and convert to lowercase
        slug = re.sub(r'[^\w\s-]', '', query.lower())
        # Replace spaces with hyphens
        slug = re.sub(r'\s+', '-', slug)
        # Remove multiple consecutive hyphens
        slug = re.sub(r'-+', '-', slug)
        # Remove leading/trailing hyphens
        return slug.strip('-')
    
    async def _extract_blog_post_from_url(self, url: str) -> Optional[BlogPost]:
        """Extract blog post information from a URL."""
        try:
            response = await self.client.get(url, timeout=10.0)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract title using multiple strategies
            title = self._extract_title_from_soup(soup)
            if not title:
                return None
            
            # Extract other metadata
            excerpt = self._extract_excerpt_from_soup(soup)
            date = self._extract_date_from_soup(soup)
            category = self._extract_category_from_url(url)
            
            return BlogPost(
                title=title,
                url=url,
                category=category,
                excerpt=excerpt,
                date=date,
                confidence=0.0,  # Will be set by calling strategy
                similarity_score=0.0,  # Will be calculated later
                source="url_extraction"
            )
            
        except Exception as e:
            logger.error(f"Failed to extract blog post from {url}: {e}")
            return None
    
    def _extract_title_from_soup(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract title using multiple fallback strategies."""
        # Strategy 1: Open Graph title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        # Strategy 2: Title tag
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            # Clean up common AWS blog title suffixes
            title = re.sub(r'\s*\|\s*AWS.*$', '', title)
            title = re.sub(r'\s*-\s*Amazon Web Services$', '', title)
            return title
        
        # Strategy 3: H1 tag
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
        
        # Strategy 4: Blog post title class
        blog_title = soup.find(class_=re.compile(r'blog.*title|title.*blog', re.I))
        if blog_title:
            return blog_title.get_text().strip()
        
        return None
    
    def _extract_excerpt_from_soup(self, soup: BeautifulSoup) -> str:
        """Extract excerpt/description from the page."""
        # Try meta description first
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        # Try first paragraph
        first_p = soup.find('p')
        if first_p:
            text = first_p.get_text().strip()
            return text[:200] + "..." if len(text) > 200 else text
        
        return ""
    
    def _extract_date_from_soup(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract publication date from the page."""
        # Try various date selectors
        date_selectors = [
            'time[datetime]',
            '.date',
            '.publish-date',
            '.post-date',
            '[data-date]'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                return date_elem.get('datetime') or date_elem.get_text().strip()
        
        return None
    
    def _extract_category_from_url(self, url: str) -> str:
        """Extract category from URL path."""
        try:
            # Extract category from URL like /blogs/category/post-title/
            parts = url.split('/blogs/')
            if len(parts) > 1:
                category_part = parts[1].split('/')[0]
                return category_part
        except:
            pass
        
        return "unknown"
    
    def _prioritize_categories_for_query(self, query: str) -> List[str]:
        """Prioritize categories based on query content."""
        query_lower = query.lower()
        
        # Category keyword mapping
        category_keywords = {
            'aws': ['aws', 'amazon', 'introducing', 'announcement'],
            'machine-learning': ['ai', 'ml', 'machine', 'learning', 'bedrock', 'sagemaker'],
            'compute': ['ec2', 'lambda', 'compute', 'serverless', 'container'],
            'networking-and-content-delivery': ['load', 'balancer', 'gateway', 'vpc', 'network', 'cdn'],
            'architecture': ['architecture', 'design', 'pattern', 'best', 'practice'],
            'security': ['security', 'firewall', 'protection', 'iam', 'encryption'],
            'developer': ['developer', 'development', 'api', 'sdk', 'cli'],
            'database': ['database', 'rds', 'dynamodb', 'aurora', 'sql'],
            'storage': ['storage', 's3', 'efs', 'fsx', 'backup'],
            'containers': ['container', 'docker', 'kubernetes', 'ecs', 'eks', 'fargate']
        }
        
        # Score categories based on keyword matches
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                category_scores[category] = score
        
        # Sort by score and add remaining categories
        prioritized = sorted(category_scores.keys(), key=lambda x: category_scores[x], reverse=True)
        remaining = [cat for cat in self.known_categories if cat not in prioritized]
        
        return prioritized + remaining
    
    async def _search_single_category(self, query: str, category: str, limit: int) -> List[BlogPost]:
        """Search within a single category using multiple parsing strategies."""
        results = []
        category_url = f"{self.blog_base_url}{category}/"
        
        try:
            response = await self.client.get(category_url, timeout=10.0)
            if response.status_code != 200:
                return results
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Multiple parsing strategies for robustness
            title_selectors = [
                'h2.blog-post-title a',
                'h3.blog-post-title a',
                '.blog-post-title a',
                'h2 a[href*="/blogs/"]',
                'h3 a[href*="/blogs/"]',
                'a[href*="/blogs/"][title]',
                'a[href*="/blogs/"]'
            ]
            
            for selector in title_selectors:
                if len(results) >= limit:
                    break
                
                elements = soup.select(selector)
                for element in elements:
                    if len(results) >= limit:
                        break
                    
                    title = element.get_text(strip=True)
                    href = element.get('href')
                    
                    if not title or not href:
                        continue
                    
                    # Normalize URL
                    if not href.startswith('http'):
                        href = f"{self.base_url}{href}" if href.startswith('/') else f"{self.base_url}/{href}"
                    
                    # Calculate similarity
                    similarity = self._calculate_similarity(query, title)
                    
                    if similarity > 0.3:  # Minimum threshold
                        blog_post = BlogPost(
                            title=title,
                            url=href,
                            category=category,
                            excerpt="",  # Will be filled later if needed
                            date=None,
                            confidence=min(similarity, 0.85),  # Cap at 85% for category search
                            similarity_score=similarity,
                            source=f"category_{category}"
                        )
                        results.append(blog_post)
            
        except Exception as e:
            logger.error(f"Error searching category {category}: {e}")
        
        return results
    
    def _calculate_similarity(self, query: str, title: str) -> float:
        """Calculate comprehensive similarity score between query and title."""
        query_lower = query.lower()
        title_lower = title.lower()
        
        # Exact match
        if query_lower == title_lower:
            return 1.0
        
        # Substring match
        if query_lower in title_lower:
            return 0.9
        
        # Keyword matching
        query_words = set(query_lower.split())
        title_words = set(title_lower.split())
        
        if query_words and title_words:
            intersection = query_words.intersection(title_words)
            keyword_score = len(intersection) / len(query_words)
        else:
            keyword_score = 0.0
        
        # Sequence matching using simple algorithm
        sequence_score = self._sequence_similarity(query_lower, title_lower)
        
        # Combined score with weights
        return max(keyword_score * 0.7 + sequence_score * 0.3, sequence_score)
    
    def _sequence_similarity(self, s1: str, s2: str) -> float:
        """Simple sequence similarity calculation."""
        if not s1 or not s2:
            return 0.0
        
        # Simple Levenshtein-like similarity
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        
        # Count matching characters in order
        matches = 0
        i = j = 0
        while i < len(s1) and j < len(s2):
            if s1[i] == s2[j]:
                matches += 1
                i += 1
                j += 1
            else:
                i += 1
        
        return matches / max_len
    
    def _matches_query(self, blog_post: BlogPost, query: str) -> bool:
        """Check if blog post matches the query with reasonable confidence."""
        similarity = self._calculate_similarity(query, blog_post.title)
        return similarity > 0.3
    
    def _process_and_rank_results(self, results: List[BlogPost], query: str, limit: int) -> List[BlogPost]:
        """Process and rank results by relevance and confidence."""
        if not results:
            return []
        
        # Remove duplicates based on URL
        seen_urls = set()
        unique_results = []
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        # Recalculate similarity scores for consistency
        for result in unique_results:
            result.similarity_score = self._calculate_similarity(query, result.title)
            # Adjust confidence based on source and similarity
            if result.source == "direct_url":
                result.confidence = min(result.similarity_score + 0.1, 1.0)
            else:
                result.confidence = result.similarity_score
        
        # Sort by confidence and similarity
        unique_results.sort(key=lambda x: (x.confidence, x.similarity_score), reverse=True)
        
        return unique_results[:limit]
    
    async def _quick_validation_check(self) -> 'ValidationResult':
        """Quick validation check to ensure search system is working."""
        # For now, always return passed
        # In production, this would run a subset of golden test cases
        return ValidationResult(passed=True, message="System operational")
    
    def _create_confident_response(self, results: List[BlogPost], query: str, 
                                 search_id: str, execution_time: float,
                                 sources_checked: List[str], categories_searched: int) -> SearchResult:
        """Create a response with appropriate confidence messaging."""
        
        if not results:
            return SearchResult(
                found=False,
                confidence=0.0,
                results=[],
                search_metadata={
                    'search_id': search_id,
                    'execution_time': execution_time,
                    'sources_checked': sources_checked,
                    'categories_searched': categories_searched
                },
                message=self._format_no_results_message(query, sources_checked, categories_searched, search_id),
                search_id=search_id,
                execution_time=execution_time,
                sources_checked=sources_checked,
                categories_searched=categories_searched
            )
        
        # Determine overall confidence
        max_confidence = max(r.confidence for r in results)
        
        # Format results for response
        formatted_results = []
        for result in results:
            formatted_results.append({
                'title': result.title,
                'url': result.url,
                'category': result.category,
                'excerpt': result.excerpt,
                'date': result.date,
                'confidence': result.confidence,
                'similarity_score': result.similarity_score
            })
        
        return SearchResult(
            found=True,
            confidence=max_confidence,
            results=formatted_results,
            search_metadata={
                'search_id': search_id,
                'execution_time': execution_time,
                'sources_checked': sources_checked,
                'categories_searched': categories_searched
            },
            message=self._format_results_message(results, max_confidence, execution_time),
            search_id=search_id,
            execution_time=execution_time,
            sources_checked=sources_checked,
            categories_searched=categories_searched
        )
    
    def _format_results_message(self, results: List[BlogPost], confidence: float, execution_time: float) -> str:
        """Format results message with appropriate confidence indicators."""
        if confidence >= SearchConfidence.EXACT_MATCH.value:
            return f"âœ… **Exact match found** (confidence: {confidence:.0%})"
        elif confidence >= SearchConfidence.HIGH_CONFIDENCE.value:
            return f"ðŸ” **High confidence match** (confidence: {confidence:.0%})"
        elif confidence >= SearchConfidence.MEDIUM_CONFIDENCE.value:
            return f"â“ **Possible matches found** (confidence: {confidence:.0%})"
        else:
            return f"âš ï¸ **Low confidence results** (confidence: {confidence:.0%}) - Please verify manually"
    
    def _format_no_results_message(self, query: str, sources_checked: List[str], 
                                 categories_searched: int, search_id: str) -> str:
        """Format helpful message when no results are found."""
        return f"""âŒ **No confident matches found**

**What we searched:**
- {categories_searched} blog categories
- {len(sources_checked)} search strategies
- Query: "{query}"

**This might mean:**
- The content doesn't exist on AWS blogs
- Our search system missed it (please report!)
- Try different keywords or exact title

**Alternative actions:**
- Browse [AWS Blog Categories](https://aws.amazon.com/blogs/)
- Check [AWS What's New](https://aws.amazon.com/new/)
- Search [AWS Documentation](https://docs.aws.amazon.com/)

*Search ID: {search_id} (for debugging)*"""
    
    def _create_system_degraded_response(self, search_id: str, validation_result) -> SearchResult:
        """Create response when system validation fails."""
        return SearchResult(
            found=False,
            confidence=0.0,
            results=[],
            search_metadata={'search_id': search_id, 'system_status': 'degraded'},
            message=f"âš ï¸ **Search system temporarily degraded**\n\n{validation_result.message}\n\nPlease try again later or search manually.",
            search_id=search_id,
            execution_time=0.0,
            sources_checked=[],
            categories_searched=0
        )
    
    def _create_error_response(self, query: str, search_id: str, execution_time: float, error: str) -> SearchResult:
        """Create response when search fails completely."""
        return SearchResult(
            found=False,
            confidence=0.0,
            results=[],
            search_metadata={'search_id': search_id, 'error': error},
            message=f"âŒ **Search system error**\n\nWe encountered an error while searching for: \"{query}\"\n\nPlease try again or search manually.\n\n*Error ID: {search_id}*",
            search_id=search_id,
            execution_time=execution_time,
            sources_checked=[],
            categories_searched=0
        )
    
    def _generate_search_id(self, query: str) -> str:
        """Generate unique search ID for debugging and tracking."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        return f"search_{timestamp}_{query_hash}"
    
    def _update_metrics(self, results: List[BlogPost], execution_time: float):
        """Update search performance metrics."""
        self.metrics['total_searches'] += 1
        
        if results:
            self.metrics['successful_searches'] += 1
            max_confidence = max(r.confidence for r in results)
            if max_confidence >= SearchConfidence.HIGH_CONFIDENCE.value:
                self.metrics['high_confidence_searches'] += 1
        
        # Update average execution time
        total_time = self.metrics['average_execution_time'] * (self.metrics['total_searches'] - 1)
        self.metrics['average_execution_time'] = (total_time + execution_time) / self.metrics['total_searches']
    
    async def validate_system_health(self) -> Dict[str, Any]:
        """Run comprehensive system health validation."""
        health_report = {
            'status': 'UNKNOWN',
            'timestamp': datetime.now().isoformat(),
            'metrics': self.metrics.copy(),
            'test_results': [],
            'recommendations': []
        }
        
        # Run golden test cases
        passed_tests = 0
        for test_case in self.golden_test_cases:
            try:
                result = await self.search(test_case['query'], limit=5)
                
                test_passed = False
                if result.found and result.confidence >= test_case['min_confidence']:
                    if 'expected_url' in test_case:
                        test_passed = any(r['url'] == test_case['expected_url'] for r in result.results)
                    else:
                        test_passed = True
                
                health_report['test_results'].append({
                    'query': test_case['query'],
                    'passed': test_passed,
                    'confidence': result.confidence,
                    'found_results': len(result.results)
                })
                
                if test_passed:
                    passed_tests += 1
                    
            except Exception as e:
                health_report['test_results'].append({
                    'query': test_case['query'],
                    'passed': False,
                    'error': str(e)
                })
        
        # Determine overall health status
        success_rate = passed_tests / len(self.golden_test_cases) if self.golden_test_cases else 0
        
        if success_rate >= 0.9:
            health_report['status'] = 'HEALTHY'
        elif success_rate >= 0.7:
            health_report['status'] = 'DEGRADED'
        else:
            health_report['status'] = 'UNHEALTHY'
        
        # Add recommendations
        if success_rate < 0.9:
            health_report['recommendations'].append("Search system needs attention - some test cases failing")
        if self.metrics['average_execution_time'] > 5.0:
            health_report['recommendations'].append("Search performance is slow - consider optimization")
        
        return health_report

class ValidationResult(NamedTuple):
    """Result of system validation check."""
    passed: bool
    message: str