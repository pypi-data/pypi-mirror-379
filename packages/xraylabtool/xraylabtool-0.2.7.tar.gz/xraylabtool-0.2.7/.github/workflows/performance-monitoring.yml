name: Performance Monitoring & Optimization

on:
  schedule:
    # Run daily at 2 AM UTC to monitor CI performance
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      analyze_last_runs:
        description: 'Number of recent runs to analyze'
        default: '20'
        required: false

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONUNBUFFERED: 1
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  analyze-performance:
    name: üìä Analyze CI Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        fetch-depth: 1

    - name: Set up Python 3.12
      uses: actions/setup-python@v6
      with:
        python-version: '3.12'

    - name: Install analysis tools
      run: |
        python -m pip install --upgrade pip --quiet
        pip install requests matplotlib pandas --quiet

    - name: Analyze workflow performance
      id: analyze
      run: |
        python << 'EOF'
        import json
        import requests
        import os
        from datetime import datetime, timedelta

        # GitHub API setup
        headers = {
            'Authorization': f'token {os.environ["GITHUB_TOKEN"]}',
            'Accept': 'application/vnd.github.v3+json'
        }

        repo = os.environ.get('GITHUB_REPOSITORY')
        base_url = f'https://api.github.com/repos/{repo}'

        # Get recent workflow runs
        runs_to_analyze = int('${{ github.event.inputs.analyze_last_runs }}' or '20')

        print(f"üîç Analyzing last {runs_to_analyze} workflow runs...")

        # Fetch workflow runs
        response = requests.get(
            f'{base_url}/actions/runs',
            headers=headers,
            params={'per_page': runs_to_analyze, 'status': 'completed'}
        )

        if response.status_code != 200:
            print(f"‚ùå Failed to fetch runs: {response.status_code}")
            exit(1)

        runs = response.json()['workflow_runs']

        # Analyze performance metrics
        performance_data = {
            'total_runs': len(runs),
            'successful_runs': 0,
            'failed_runs': 0,
            'avg_duration_minutes': 0,
            'slowest_workflow': None,
            'fastest_workflow': None,
            'failure_rate': 0
        }

        durations = []
        slowest_duration = 0
        fastest_duration = float('inf')

        for run in runs:
            if run['conclusion'] == 'success':
                performance_data['successful_runs'] += 1
            elif run['conclusion'] in ['failure', 'cancelled', 'timed_out']:
                performance_data['failed_runs'] += 1

            # Calculate duration
            start_time = datetime.fromisoformat(run['created_at'].replace('Z', '+00:00'))
            end_time = datetime.fromisoformat(run['updated_at'].replace('Z', '+00:00'))
            duration = (end_time - start_time).total_seconds() / 60

            durations.append(duration)

            if duration > slowest_duration:
                slowest_duration = duration
                performance_data['slowest_workflow'] = {
                    'name': run['name'],
                    'duration': duration,
                    'url': run['html_url']
                }

            if duration < fastest_duration:
                fastest_duration = duration
                performance_data['fastest_workflow'] = {
                    'name': run['name'],
                    'duration': duration,
                    'url': run['html_url']
                }

        if durations:
            performance_data['avg_duration_minutes'] = sum(durations) / len(durations)

        if performance_data['total_runs'] > 0:
            performance_data['failure_rate'] = (performance_data['failed_runs'] / performance_data['total_runs']) * 100

        # Generate report
        print("\nüìä PERFORMANCE ANALYSIS REPORT")
        print("=" * 50)
        print(f"üî¢ Total Runs Analyzed: {performance_data['total_runs']}")
        print(f"‚úÖ Successful Runs: {performance_data['successful_runs']}")
        print(f"‚ùå Failed Runs: {performance_data['failed_runs']}")
        print(f"üìà Success Rate: {(performance_data['successful_runs']/performance_data['total_runs']*100):.1f}%")
        print(f"‚è±Ô∏è  Average Duration: {performance_data['avg_duration_minutes']:.1f} minutes")

        if performance_data['slowest_workflow']:
            print(f"üêå Slowest Run: {performance_data['slowest_workflow']['name']} ({performance_data['slowest_workflow']['duration']:.1f}m)")

        if performance_data['fastest_workflow']:
            print(f"üöÄ Fastest Run: {performance_data['fastest_workflow']['name']} ({performance_data['fastest_workflow']['duration']:.1f}m)")

        # Performance recommendations
        print("\nüí° OPTIMIZATION RECOMMENDATIONS")
        print("=" * 50)

        if performance_data['avg_duration_minutes'] > 15:
            print("‚ö†Ô∏è  Average duration is high (>15 min). Consider:")
            print("   - Reducing test matrix size")
            print("   - Optimizing dependency caching")
            print("   - Using fail-fast strategies")

        if performance_data['failure_rate'] > 10:
            print("‚ö†Ô∏è  High failure rate detected. Consider:")
            print("   - Reviewing flaky tests")
            print("   - Adding retry mechanisms")
            print("   - Improving error handling")

        if performance_data['avg_duration_minutes'] < 8:
            print("‚úÖ Excellent performance! CI runs are fast and efficient.")

        # Export data for artifacts
        with open('performance_metrics.json', 'w') as f:
            json.dump(performance_data, f, indent=2, default=str)

        print(f"\nüìÅ Performance data saved to performance_metrics.json")
        EOF

    - name: Check for performance regressions
      run: |
        python << 'EOF'
        import json
        import os

        if os.path.exists('performance_metrics.json'):
            with open('performance_metrics.json', 'r') as f:
                data = json.load(f)

            # Define performance thresholds
            MAX_AVG_DURATION = 12  # minutes
            MAX_FAILURE_RATE = 15  # percent
            MIN_SUCCESS_RATE = 85  # percent

            issues = []

            if data['avg_duration_minutes'] > MAX_AVG_DURATION:
                issues.append(f"‚ö†Ô∏è Average duration ({data['avg_duration_minutes']:.1f}m) exceeds threshold ({MAX_AVG_DURATION}m)")

            if data['failure_rate'] > MAX_FAILURE_RATE:
                issues.append(f"‚ö†Ô∏è Failure rate ({data['failure_rate']:.1f}%) exceeds threshold ({MAX_FAILURE_RATE}%)")

            success_rate = (data['successful_runs'] / data['total_runs']) * 100 if data['total_runs'] > 0 else 0
            if success_rate < MIN_SUCCESS_RATE:
                issues.append(f"‚ö†Ô∏è Success rate ({success_rate:.1f}%) below threshold ({MIN_SUCCESS_RATE}%)")

            if issues:
                print("üö® PERFORMANCE REGRESSION DETECTED")
                print("=" * 40)
                for issue in issues:
                    print(issue)
                print("\nüìã Consider running workflow optimization tools")
            else:
                print("‚úÖ No performance regressions detected")
                print("üéØ CI performance is within acceptable thresholds")
        EOF

    - name: Upload performance metrics
      uses: actions/upload-artifact@v4
      with:
        name: performance-metrics-${{ github.run_number }}
        path: performance_metrics.json
        retention-days: 30

  cache-cleanup:
    name: üßπ Cache Optimization
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
    - name: Analyze cache usage
      run: |
        echo "üîç Analyzing GitHub Actions cache usage..."
        echo "This would analyze cache hit rates and storage usage"
        echo "Recommendations for cache optimization would be generated here"

        # Note: GitHub doesn't provide direct cache analytics via API yet
        # This is a placeholder for future cache optimization features

        echo "üí° Cache optimization tips:"
        echo "- Use specific cache keys with version hashes"
        echo "- Include tool versions in cache keys"
        echo "- Set appropriate cache restore fallbacks"
        echo "- Monitor cache hit rates manually"

  workflow-health:
    name: üè• Workflow Health Check
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
    - name: Checkout workflows
      uses: actions/checkout@v5
      with:
        fetch-depth: 1

    - name: Validate workflow syntax
      run: |
        echo "üîç Validating GitHub Actions workflow syntax..."

        python << 'EOF'
        import yaml
        import os
        import glob

        workflow_dir = '.github/workflows'
        workflow_files = glob.glob(f'{workflow_dir}/*.yml') + glob.glob(f'{workflow_dir}/*.yaml')

        print(f"Found {len(workflow_files)} workflow files")

        issues = []
        warnings = []

        for file_path in workflow_files:
            try:
                with open(file_path, 'r') as f:
                    workflow = yaml.safe_load(f)

                print(f"‚úÖ {os.path.basename(file_path)} - Valid syntax")

                # Check for common optimization opportunities
                if 'jobs' in workflow:
                    for job_name, job in workflow['jobs'].items():
                        # Check for missing timeouts
                        if 'timeout-minutes' not in job:
                            warnings.append(f"{file_path}:{job_name} - Missing timeout")

                        # Check for inefficient checkout
                        if 'steps' in job:
                            for step in job['steps']:
                                if isinstance(step, dict) and step.get('uses', '').startswith('actions/checkout'):
                                    if 'with' not in step or 'fetch-depth' not in step.get('with', {}):
                                        warnings.append(f"{file_path}:{job_name} - Consider setting fetch-depth")

            except yaml.YAMLError as e:
                issues.append(f"‚ùå {file_path} - YAML syntax error: {e}")
            except Exception as e:
                issues.append(f"‚ùå {file_path} - Error: {e}")

        if issues:
            print("\nüö® ISSUES FOUND:")
            for issue in issues:
                print(f"  {issue}")

        if warnings:
            print("\n‚ö†Ô∏è OPTIMIZATION OPPORTUNITIES:")
            for warning in warnings:
                print(f"  {warning}")

        if not issues and not warnings:
            print("\nüéâ All workflows are optimally configured!")

        EOF

    - name: Check action versions
      run: |
        echo "üîç Checking for outdated GitHub Actions..."

        # Check for potentially outdated action versions
        grep -r "uses:" .github/workflows/ | grep -E "@v[0-9]" | sort | uniq | head -20 || true

        echo ""
        echo "üí° Consider using latest stable versions:"
        echo "  - actions/checkout@v5 (latest)"
        echo "  - actions/setup-python@v6 (latest)"
        echo "  - actions/cache@v4 (latest stable)"
        echo "  - actions/upload-artifact@v4 (latest stable)"
